{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some imports\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "# % matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('figure', figsize=(12, 5))\n",
    "\n",
    "# Settings for the visualizations\n",
    "#import seaborn as sns\n",
    "#sns.set_style(\"whitegrid\")\n",
    "#sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "# Others\n",
    "import cv2\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from skimage.io import imread\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import util.Display_images as display\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(file_path, ds_path):\n",
    "    path = os.path.realpath(os.path.join(ds_path, file_path))\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "\n",
    "def preprocess_img(dataset):\n",
    "    data = []\n",
    "    for img in dataset:\n",
    "        img = img / 255.\n",
    "        data.append(img)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ckpt_dir_encoder = \"./ckpt/img_encoder\"\n",
    "ckpt_dir_predict = \"./ckpt/predict\"\n",
    "ckpt_dir_predict_retrained = \"./ckpt/pred_ret\"\n",
    "# Prepare a directory to store all the checkpoints.\n",
    "if not os.path.exists(ckpt_dir_encoder):\n",
    "    os.makedirs(ckpt_dir_encoder)\n",
    "if not os.path.exists(ckpt_dir_predict):\n",
    "    os.makedirs(ckpt_dir_predict)\n",
    "if not os.path.exists(ckpt_dir_predict_retrained):\n",
    "    os.makedirs(ckpt_dir_predict_retrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = './../mlub2021-session10-dataset/'\n",
    "X_train_ds = 'X_train.p'\n",
    "y_train_ds = 'y_train.p'\n",
    "X_test_ds = 'X_test.p'\n",
    "X_unlabeled_ds = 'X_unlabeled.p'\n",
    "\n",
    "X_train_full = load_data(X_train_ds, dataset_path)\n",
    "X_unlabeled = load_data(X_unlabeled_ds, dataset_path)\n",
    "X_test_full = load_data(X_test_ds, dataset_path)\n",
    "y_train_full = load_data(y_train_ds, dataset_path)\n",
    "y_train_full = np.array([y[0] for y in y_train_full])\n",
    "\n",
    "# y_train_full = y_train_full.astype(\"float32\")\n",
    "X_train_full = preprocess_img(X_train_full)\n",
    "X_unlabeled = preprocess_img(X_unlabeled)\n",
    "X_test_full = preprocess_img(X_test_full)\n",
    "X_train_unlb = X_unlabeled[5000:]\n",
    "X_valid_unlb = X_unlabeled[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes no: 100\n",
      "Vf classes no:  100\n"
     ]
    }
   ],
   "source": [
    "n_classes = int(np.max(y_train_full) + 1)\n",
    "print(\"Classes no:\", n_classes)\n",
    "print(\"Vf classes no: \", len(np.unique(y_train_full)))\n",
    "y_train_one_hot = tf.one_hot(y_train_full, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAElCAYAAAA4KCPqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjuElEQVR4nO3dfWxk53Xf8e+ZGb4OyeVS+yLLeotlyWkkV4Kruq6VbWIEaRoXgWSoKFrZslPXECrBsOGiBtTAQh01bVD/kaBIVacC/JK4tRo3lWoFgu0CToJISZ1aTbK2aFurF2stWdJyd7lLLt85M6d/zG7ApbjnXHI50p2L30cYQJzn8rl37lw+8+wz55xr7o6IiJRL7Y0+ABEReS0NziIiJaTBWUSkhDQ4i4iUkAZnEZES0uAsIlJCGpxFREpIg7OIyDaZ2UfN7EkzWzWzLybbfsLMXjWzeTP7vJkNFdmHBmcRke17Gfg14PPRRmb2C8C9wM8BVwFvAX61yA40OIuIbJO7P+zu/ws4mWz6IeBz7j7t7qeAfwv8cpF9NKLGb/1gLcztbrfW0x3sRnq4mV1Ue6FtCvURt9cs/6yrJZ+HtQIfl9luzPJzbsTbFHgpQH7O0h4KnPe8j+z1di56H0Wuj7SLItvU4tdSq70+5zzbYqieXyBX7a9f/MFepF94T9NPzra3/Xv/7zur08DKhqcedPcHd3AI1wNf3fDzYeCgmV3i7uHAHg7OIiL97MRsmz//xuXb/r2BNz234u4378IhjAFzG34+9//jJLNuDc4iUmFO23fhX047twBMbPj53P+fyX5Ra84iUlkOdPBtP3bRNHDjhp9vBI5lSxqgwVlEKq6zg/8yZtYws2GgDtTNbNjMtlqJ+F3gn5vZT5nZJPAp4ItFjluDs4jI9n0KWKYbJveBs///KTO70swWzOxKAHf/OvAZ4I+AHwFHgX9TZAdacxaRynKcdg9uKOLunwY+fYHmsU3b/gbwG9vdhwZnEam0XV5Dft2Eg3NnfSn8Ze8UeNFJ4K5ZPe0ii7mtFwinrBUJIM6PJNlHfhzpqy0So5xsUyQkNz0dhWKls30UiKdNtqkVeDF1i9cI60Xiz3cllj5pT2KYuxvFMblF+rDknSkSw55tMrwL8davBwfaVRycRUT6XSVnziIi/cyhJ2vOrwcNziJSaW9oCspF0OAsIpXluNacRURKx6Hdn2OzBmcRqa5u+nZ/0uAsIhVmtHehtO0bQYOziFSWA0XSMcooHJzfNBnf6mp5rZXuoOXxp5bX8s+HNNmhSIJAEjRf7LN1F5JQ0kL5u1D0f1cK9l/8DQyycw67k/xRT96XeoEvhPLjSLsguz4KJZBk12mhJKWLf1+y3Qz20WRUM2cRkZLpZghqcBYRKZ1O8q/3stLgLCKVpZmziEgJOUa7T8vW9+dRi4hUnGbOIlJpWnMWESmZyq45XzIaBzsuN/IXvdyK++gUiNusJUG5ViSONVvAKRKovgvxxY0k1rVQIfTk5gJFjqOWHcduFJcv9Ddx8TcOyC6hWoE/zvy1XPwfuBUp+l9LbsdQ4O8ljbcu8FKyKptZvH55GG3vm4M9j2bOIlJZ3doaGpxFREqnkssaIiL9zF3LGiIipdTRzFlEpFy60Rr9OXPuz6MWESmku6yx3Uehns2mzOwRM1s0s6NmdscFtps0s98xs5mzj08X6V8zZxGprB5HazwArAEHgZuAx8zssLtPb9ruN4FR4GrgAPBNMzvq7l+IOtfMWUQqre227UfGzJrA7cB97r7g7k8AjwJ3brH5LwGfcfcld38B+Bzw4Wwf4czZ1lbCX64XuDnXUFJMv1OgAPlAMr+vF4iqz4qp1xsFEgSSyPtixfYvPuki+36jUOJGksywK8dRoAtPtsrau/vZhQSRi2zfzlYXZRdSkYvksWSb7EJOzuviIgof7TOzJzf8/KC7P7jh5+uAlrsf2fDcYeBnLtCfbfr/G7ID0LKGiFRaZ2ehdCfc/eagfQyY3/TcHDC+xbZfB+41sw/RXQL5MN1ljpCWNUSkss5Fa2z3UcACMLHpuQngzBbbfgxYBp4Bvgo8BLyU7UCDs4hUlrP99eYia87AEaBhZtdueO5GYPOXgbj7rLu/390vdffr6Y67/zfbgZY1RKTSehGt4e6LZvYwcL+ZfYRutMatwLs3b2tm1wCnzz7+PnAXF16b/muaOYuI7Mw9wAgwQ3ep4m53nzazQ2a2sGG7vwV8l+6Sx68D798i3O41NHMWkcpyp2e1Ndx9Frhti+cfp/uF4bmfvwJ8Zbv9a3AWkQqzatbWyAqMNwrE9ZJs40nheIDBpKh/kePI4jIbBT6m0m0KxGzvxnXiWSX0XZAV4wfSgNlCh5nNagp8OZPG5BZ4LWkfBaO24z4uuovdCaUukmTxuhxI7zm9mzn3mmbOIlJp/Vr4SIOziFSWY7rBq4hIGWnmLCJSMs6O07ffcBqcRaTCTPcQFBEpG82cRURKSjNnEZGScbeKzpyTD5xagQSSgWQbq6ddpMkftQLVw9Pi4LX8zgFu8TZZQX8AJ37BWeJP0W1yyTkrVJE9uflAgW/JO8luvJ2/L41GgYsoPY5dSOxJ3pZCe8huglCsl2Qf+SaeJtT0z7xOSSgiIiXTvYegljVERErGNHMWESmbbrSGZs4iIqWjDEERkZLp59oa/fmRIiJScZo5i0il9eIegq+HuNh+LR676wXinK2RnJgC5y2L7SwS9mtJ/GityHEk+9mN+OMi4bbZbooU48+2KVKgPnvz1lfbaQ/f+tM/D9tfPPpC2sdNN709bH/b33hb2kdjaDDdJpOdsWKh1OlFVqCPJDY8ideHAtdHoXjrN345oXubqjf+OHZCM2cRqbR+XXPW4CwildX9QrCCyxoiIv1OhY9EREqmn5NQ+nO+LyJSSHdZY7uPQj2bTZnZI2a2aGZHzeyOC2w3ZGa/bWbHzGzWzP7AzN6c9a/BWUQqrYNt+1HQA8AacBB4P/BZM7t+i+0+Dvxd4G8ClwGngN/KOtfgLCKVdS6UbruPjJk1gduB+9x9wd2fAB4F7txi858AvuHux9x9Bfg9YKtB/DwanEWk0na4rLHPzJ7c8LhrU7fXAS13P7LhucNsPeh+DrjFzC4zs1G6s+yvZccdfiHY9oH4t/Mcg6S0PFAk2SHZpuAnXdheKEEgeTFF+vAkQaBYIkvvi+13vECR+1p8fcy8cjzt40+++UTY/uzT3037+N5f/UXY/ov/8L1pH9cmiSoTU3vTPoaaI2F7kX8ut1sXX0zfaskNLpJ2yK+wIkkou3NTiItzEbU1Trj7zUH7GDC/6bk5YHyLbZ8BXgR+THfU/C7w0ewANHMWkUrr0ZrzAjCx6bkJ4MwW2z4ADAGXAE3gYQrMnDU4i0hlnQul2+6jgCNAw8yu3fDcjcD0FtveBHzR3WfdfZXul4HvNLN90Q40OItIpfUilM7dF+nOgO83s6aZ3QLcCnxpi82/DXzQzPaY2QBwD/Cyu5+I9qHBWUSqawez5m2sUd8DjAAzwEPA3e4+bWaHzGxhw3b/Clihu/Z8HHgv8L6sc2UIikhl9fIGr+4+C9y2xfOP0/3C8NzPJ+lGaGyLZs4iIiWkmbOIVFq/1tYIB+ellSQmt0Cgcz2JqawlRfC7nSQnt0jR/2Q/VuDfEPWkIr/VCgR+1+JzWqulkeFp/GiR+NL0fOSHwVPf+0HY/tgj30z7WF+Nz8dP33Io7cNbS2H7s9/7ftrHK6+8HLZfdvXVaR/veNffDttX1/NrfeFMfA3V6/mFOjwSv3lDw/mcrNNpxe3rq2kfE+Nj6Ta91s+FjzRzFpFK0+AsIlIy/Xz3bQ3OIlJpvYrW6DUNziJSXa5lDRGR0tEXgiIiJaXBWUSkZPSFoIhISXkVB+f1zmL82wWqN9U9Hv9reTl+rB0H5hdJ/siSTKxAcL/XBy9qHwC15MYBRa6jWpYMUySxJ0tC6eQH8n++9Zdh+59++3Dax/jIaNh++sxW5XHP9463XRG2X3bJnrSPp557PmxfWouTMgCuuu4nw3av5XOheiO+gcHq2nraRyv5e2l38uuj1VoL22sFLtSJrcrOvwEUrSEiUjKuaA0RkXKq5LKGiEh/0xeCIiKlpJmziEjJ9HMSiorti4iUkGbOIlJd3o3Y6Efh4Fxrx7GdRV60d+KYS6/HcZ1QoDA8ccF2AJK43eSeAN1tkkL5hQ6D+LUUuflAuoZWJN46uUHB2moeO768GF8f7U5+IAuLcdH22ZljaR+szIXNE4femfeR3F1gcCC/TmtJfPH4nom0jywEeXAgzwtYWYljoVeTdoCBwfj6aBS5G0NJKM5ZRKRkHH0hKCJSQgqlExEppX5dc1a0hohUmrtt+1GEmU2Z2SNmtmhmR83sjgts9zUzW9jwWDOz72b9a+YsIpXl3tM15weANeAgcBPwmJkddvfp84/Bf3Hjz2b2x8AfZp1rcBaRSuvFmrOZNYHbgRvcfQF4wsweBe4E7g1+72rgEPDL2T40OItIpe1wzXmfmT254ecH3f3BDT9fB7Tc/ciG5w4DP5P0+0HgcXd/ITsADc4iUmk7XNY44e43B+1jwPym5+aArIr1B4FfK3IA4eDcWU2C1S1/0ZYM/612HhBPUlx+YGAo7cKznIqkCD5Ae3UpbvcCr6URB+97cnMCAE8SauqN/H0ZGxsO219+MU/+mDl2MmwvkqgwWIu3GRiJjxPg5Nzmv5HzvTgTHyfA1de8NWy/4qrL0z72JMc6VOCGDmdW46ScdivPdBobjf8eVpJ9ALSSov6evG9d8Y0UXg9O8S/4tmkB2JxVNAFc8O4QZvbTwKXA7xfZgaI1RKTSfAePAo4ADTO7dsNzNwLTF9ge4EPAw2fXqFNa1hCR6upRtIa7L5rZw8D9ZvYRutEatwLv3mp7MxsB/jHwvqL70MxZRKqtR1Nn4B5gBJgBHgLudvdpMztkZptnx7cBp4E/Ktq5Zs4iUmm9inN291m6g+7m5x+n+4XhxuceojuAF6bBWUQqTenbIiKyazRzFpHKqmzJ0NZ6HOvYSuJtARZXF+P2lbW0j4GhwXiDLJgasHYclzmcxB8D7JmIC66PjObHsZ4UsV9v5TGoS0mB+uHhvDD88EB8rAtzebTP6tJy2D42msco7xlrhu3NwSymH668Yn/Y/p6f+9m0jyuuvDJsL3IzhvZ6/N7On7pgCOxfO52c04WF/PrI4uA7BeLx19dXwnaz/Bo7eGAy3abnHKji4Cwi0u/6dc1Zg7OIVJsGZxGRsulZ+nbPaXAWkWrTzFlEpGR6W2y/pzQ4i0i1aeYsIlJGmjmLiJRPFWfOy2txIPpSHg/Pybm4j9n5uB2gkySZtzt5AXJrxZ9DtVYemH/lFZNh+1uujpMhANqtOFHh5PHTaR/LyYmf3Lu5BvhWWmHr1N49aQ9vv+Enw/bxiZfTPi7dvzdsXziV93HdW+NC+Af35a/FO/H7v7SW3a0B1pJC+MvZzSuAtXY8y1s6kydtrazE+xkZSZK6gIHBkbC9Q5Fi+yVRxcFZRKSvKUNQRKSclCEoIlJGGpxFREpIyxoiIuVjfTpzVrF9EZES0sxZRKprezdsLZVwcJ45NR/+8vxK/qoXV+PYz8WVPFh6dTWOyW0XOPmexI/SifcBsPKjOCb71GJ8vgAmmkNh+8KZpbSPThJyu1zghKy/HJ/3iT1jYTvASDOOhR3M67FTI34xx4/PpH1Md+LY371Tl6Z9DI/Gr7c5nsdKrybF9k+dzovtDw3FNx84PTuX9rGyGr+3rVa8D4DB4XjeNpjEQZeHac1ZRKSUqjhzFhHpe306OOsLQRGpNt/BowAzmzKzR8xs0cyOmtkdwbbvMLM/MbMFMztmZh/P+tfMWUSqq7fp2w8Aa8BB4CbgMTM77O7TGzcys33A14FPAL8PDAJxQRg0OItIxfUiztnMmsDtwA3uvgA8YWaPAncC927a/F8C33D3/3b251Xg+9k+tKwhItXWm2WN64CWux/Z8Nxh4Pottn0XMGtmf2ZmM2b2B2Z2ZbYDDc4iIq+1z8ye3PC4a1P7GLA5dnYOGN+ir8uBDwEfB64Efgg8lB2AljVEpNJ2uKxxwt1vDtoXgM2F0yeArYLZl4FH3P3bAGb2q8AJM9vj7hcMXA8H5+lnTkbNLCXFxSFPEKkVWKzvJNX28xLm4B4fq5G/luUzSTH1Vv5axofihIki/5RZX0uSco7niSxDw3EyzLNHj4TtAM//8Jmw/YXnn037aK0th+2D9byo+8lT8eudnftm2sfk5GTYfsMNb0/7uPRNbwrbB+t5Vs7KUnw+hgby+VRjIL6Kao38Op09dSpsN2bTPrr/8i+B3nwheARomNm17n7uD+FGYHqLbb/D+YslhT4utKwhItW1k/XmIhnH7ovAw8D9ZtY0s1uAW4EvbbH5F4D3mdlNZjYA3Ac8Ec2aQYOziFRdj+KcgXuAEWCG7hry3e4+bWaHzGzhr3fv/ofArwCPnd32rcAFY6LP0ZqziFRar0qGuvsscNsWzz9O9wvDjc99FvjsdvrX4Cwi1dan6dsanEWk2jQ4i4iUi7nuhCIiIrsonDmfXox/uW352J7dltw8j2NlII5TXCcvlE87LoQ+VMtjIYdqyT801vJY6eS+AViBkMzstK+18vPx9FNxHPMLPz6a9jGYxEqv1eJ2gDWLz9lagfnDwebmXIDzOflJ/eELL4TtJ06cSPu46qqrwvZrrrkm7aMxEJ+zrB2gXov/ppZX4lh7gHZy44DWet5HaajYvohICfXpsoYGZxGptH5dc9bgLCLVpsFZRKRk+jhaQ4OziFSbBmcRkRLS4CwiUj79uqyhJBQRkRIKZ861RhxoXre8zH2WhOKdfPK+vroato/U8uSPPc1m2D46kCfD7JsYCdtrBRJZFteSczqYF2RfWImzg/7qe3+Z9vHMc8+F7eN7L0v7mJhKiss396Z97D9wMGxfXc2THa69fF/YfmBsMO3j+efiGwecPpknoRw/9krYnt1YAODAwUvD9kv2x+cLYGBoON6gEyeYQH5jgOGB/DotjT6dOWtZQ0SqS9EaIiIlpcFZRKSENDiLiJSLoWUNEZFy0uAsIlIyffyFoOKcRURKKJw5jzXiou3jI/lH0uSeybB9/kweK/3D5+P40bE9cbF1gMnhOI7ZW3k87crSqbC9XiBWemVlIWx/8flX0z6+M/102H5mOY4LBzhw8PKwvdk8kPaxshTHlw+NTKZ9YGNhs9fy62NhOb4O9wzncb2jo3EcfGctP6fDyU0hRgbz62N+fjZsX1qKrx+A5ngcX16rJ3HQQDtJHaj107SuT2fOWtYQkWrT4CwiUj5acxYRKSPfwaMAM5sys0fMbNHMjprZHRfY7tNmtm5mCxseb8n618xZRKprG4PtDjwArAEHgZuAx8zssLtPb7Ht77n7B7bTuWbOIlJp5tt/pH2aNYHbgfvcfcHdnwAeBe7crePW4Cwi1bazZY19Zvbkhsddm3q9Dmi5+5ENzx0Grr/AUfySmc2a2bSZ3V3ksLWsISKVtsMvBE+4+81B+xgwv+m5OWB8i22/AjwIHAP+DvA/zey0uz8UHYBmziJSbb35QnAB2JxgMQGcec3u3b/n7i+7e9vd/wz4j8A/ynYQzpzr63Ek+uhoXnC7thInEbTOLKV9jA7GRe7rtbyY+ulkP41G/jm10IqTck6+NJP28YOnvhO2v/TK8bSPxvBk2D4+FRdsB6g3psL2Tid/b93j87G2ll/lK4txH40CNx9orWfHkd8EIcuqaI6Npl3saQ6F7YMFkpSGRuMEkeQSBGDu1MmwvTEQ/z0BjDb3hO3eL8HDvftC8AjQMLNr3f3cnRpuBLb6MnCro0ovSs2cRaSybIePjLsvAg8D95tZ08xuAW4FvvSaYzC71cz2Wtc7gY8BX832ocFZRKqtR3HOwD3ACDADPATc7e7TZnbIzDbm2f8T4Fm6Sx6/C/wHd/+drHN9ISgildarDEF3nwVu2+L5x+l+YXju53+6k/41OItItfXJ8vhmGpxFpNo0OIuIlIyK7YuIyG4KZ84jI3H88CvHT6c7WFiIi4M3Gnkc68BAvM3aal6AvOVxwfWFAkXMj5+MY5Cffe7ZtI/TpzcnFZ2vOZEXuR8a3xe2e4E4Vgbi99aG45hdgMHh+B9e7Xb+2V9L4ounmnlg04GJ+PXWOknleODqt8RFwlaWT6d9NJNY+eGB/HystuKi/p12Pg0cGY3Px5n5xbSPhbljYftQcnOCUunTmbOWNUSk0vp1WUODs4hUmwZnEZHy0cxZRKRseltsv6c0OItItWlwFhEpF0PLGiIi5aTBWUSkfMz7c3QOB+eVhbnwl5eSdoCJ5ljYPjqaFzFfb2cJJHnB/pd+/FLY/t3pp9I+5s7ECSRWz4upj07sDdvHJ+MEE4Bm0sfkvjyRpTmx+SYO51tfj885QCsptt9u5wkk7Xbcx9BQnqR05RXxzQVOHjuR9jE8HCdVHLw0f198Nb4Oa+04wQRgtBPfnKJdIKFmeWktbB8skPg1Nx9f668cezXtoxT0haCISDlpzVlEpIw0OIuIlI9mziIiZaTBWUSkZPq4nrMGZxGptj4dnFVsX0SkhMKZ8+pKXJS7XuDfC57EbbbWl9M+Fpfj+NCnn30u7eOZ554P25dWVtI+xsbj+OKhJKYbYHhsMmzfO/WmtI/RZnwcjUZeKH9pIY7JbXfyGOVOLY65bRcoDN9J45zH0z6m9k2G7avL+TV2ZjF+/9udPO57ciy+gUFzfDjt40dHXwjbhwrcBGHPnjiG/cyZ/MYSU1PJtT5c4IYOJaD0bRGRsurTDEEta4hIpZlv/1GoX7MpM3vEzBbN7KiZ3ZFsP2hm3zezOF35LM2cRaS6epu+/QCwBhwEbgIeM7PD7j59ge0/CRwH8rU6NHMWkYqzzvYfaZ9mTeB24D53X3D3J4BHgTsvsP1PAB8Afr3ocWtwFpFq8x08YJ+ZPbnhcdemXq8DWu5+ZMNzh4HrL3AUvwX8CpB/O32WljVEpNJ2GK1xwt1vDtrHgM2l++bYYsnCzN4H1N39ETP72aIHoMFZRKrL6VW0xgKwOWZxAjiz8Ymzyx+fAd673R1ocBaRSutRnPMRoGFm17r7M2efuxHY/GXgtcDVwONmBjAI7DGzV4F3ufsLF9pBMjjHS9JOnqiwuhYnofzoxTyqZPrp74ftM6dPp30MDMQJACPN/AvURtKHEychAIyMT4XtWZIKQL0RJwCsrsbF1gHanTj5o1bLkx3ayYykyITFPf72ZWg4LwzvlryWgfyrlZXF+DpdW8uTUEaH4j+n+nh+fdRqcR8njp9M+9g7GSeQ7N9/SdrH3Hx8I40iyTCl0YPB2d0Xzexh4H4z+wjdaI1bgXdv2vQp4IoNP78b+E/AO+hGblyQvhAUkco6lyHYizhn4B5gBJgBHgLudvdpMztkZgsA7t5y91fPPYBZoHP25/ATX8saIlJd7j3LEHT3WeC2LZ5/nO4Xhlv9zh8DlxfpX4OziFSaamuIiJSRBmcRkfLRzFlEpGwc6PTn6KxoDRGREgpnzifm4qLcCwt50e6XXorjmGdmZtI+WsRxrCOjo2kfTv2i2gFq9Ti2c7i5J+1jeDTepu3556WvxzG3NctfS72RxKjX8vjiwcH4fHQKzFh8Nb6RQieJxwY4dTqO/V1r5XHfrVZ8rOtJvD7AqVNxH2PD+T9UD+w/GLYvLcU3SYD8b2r//jgOGqDZjGP6W6sFqgOVRX9OnLWsISLVpjVnEZEy6tM7oWhwFpFK08xZRKRsensnlJ7S4CwildWtrdGfo7MGZxGptj4KLNlIg7OIVJpmziIiZVPVNednXng+/OVXjh1Ld7C4GAfNN8fyIvfNJMlkZTVPELCkiHljaMsKf+fZd/DNYfvE3ryIuVv2eVggCSUuA0urk/87rm5xEkqjViCBxONkl05+LwYG41wH5k7FRd8BXrE4kWWsQHJQoxaf9/V2fk7PzK+E7fNjcTvApfs23/nofFddeVXax0svvRi2L8znyWPN0TjBaO9kfk7LoXclQ3tNM2cRqTSF0omIlJFmziIiJeNgitYQESkhzZxFREqoP8dmDc4iUm2KcxYRKaMqDs4/fvXl8JfbBV7z1IG4eLjV88+HVrKgPzGVFw8fGIxjpWuNOK6z20kclLuW14WnVotfjBWI+/HkYltby4vLDw3Fr9fX49hhgHry3rULxGwPjg6G7Xsn8zj40eE4oHp0JAmmBvaMx9fHXC2/gcH8/OmwfebEqbSPSw/E8cOXvTmOtQeYGI/P2elTJ9I+Ts3G20yM53kBcnF0myoRqS6nW1tju48CzGzKzB4xs0UzO2pmd1xgu0+Y2fNmNm9mL5vZb5ql2WganEWkugzHfPuPgh4A1oCDwPuBz5rZ9Vts9yjwDnefAG4AbgQ+lnWuNWcRqbYerDmbWRO4HbjB3ReAJ8zsUeBO4N7zd+/PbfxVunPzt2b70MxZRKrNffsP2GdmT2543LWp1+uAlrsf2fDcYWCrmTNmdoeZzQMn6M6c/0t22Jo5i0h1nVtz3r4T7n5z0D4GzG96bg7Y8ttYd/8y8GUzuxb4IJBWjdPMWUQqrUdrzgvA5hKCE8CZ6Jfc/RlgGvjP2Q40OItIte1sWSNzBGicnQmfcyPdgTfTAK7JNtLgLCIVtoOBucDg7O6LwMPA/WbWNLNbgFuBL23e1sw+YmYHzv7/TwH/Gvhmto9wzXlgcCT85dGhuB2gVh8I2zsFPqSazThBYGg4P452XJ+e5eXl/EBq8WsZzCrHA2tr8Y0BGo082WFlJS7abkkhfcgTWRqNODkEoJEkqhRJMGqvxce6sppn9uy5bF/Yvn9fXhi+3Y6PdXU9v6HDzKk4+WegQJLSmcX4nI6P5wuoQ0mh/AnPk7aOHZsJ23/8yqtpH9elW7wOnF5mCN4DfB6YAU4Cd7v7tJkdAr7m7ucydW4B/p2ZjQHHgf8B3Jd1ri8ERaTaelQy1N1ngdu2eP5xul8Ynvv5n+2kfw3OIlJpKnwkIlJGGpxFRErGKfbFVglpcBaRCtPdt0VEykmDs4hICVVxcB4eiYt2u+UxufUkXnZkJI9RrtfjXJnV1by4fCuJc24kMd0Ag4PxaylS5L5blOrC1tfzuJ/1JOa22DmN37uRJLYcoJXcbaHTygv2n0lito+uLqR9XH5pXPj9iuH9aR8zx+P9zJyYTfuYX4pf7+TUVNrH6np8TuuNIvOpJIZ9vUAM+0B8DT33/NG0j/ekW0hEM2cRqS59ISgiUkYO3qMslB7T4Cwi1VbFNWcRkb6mZQ0RkZLSzFlEpIQ0OIuIlI0yBEVEyseBThWjNZLi8gODcVFvgOHhZrxBrUBx+dXFsH11JS+UP7k3TkQYGM6TLtaSTJYihfKz5I8iiSyNJBFhYCB+3wCGh+MbAwwM5e9tZzVOIFleCG+n1j2O5JzVavlxnJqNE0hefTkuHA9waj6uhH96Lr/GZk/H52Nyb15tf3E5TjBaWF5K+9i7N76WV1fzGwc0BuO/26XlPpqNauYsIlJCGpxFRMrGFUonIlI6Dq4MQRGREtLMWUSkhLTmLCJSMu4VDaUTEel3VZw5D49OxL9cIJ7WavH4v95OquAD7U5coH5szyVpH54cx9JyHl88MBjHBpvFNwUAaLfjWNesHfJi+vUCseO1ZJtOgdlGuxUfa6fAaxkcic9pJykcD7C0Eu9nbT1/X+jE26ws5zcOWG/F5+zU6Tzue89g3MeB/UVupBD/TbUL3NBh3774b+qqq9+S9iEXp8BVKyLSv7zT2fajCDObMrNHzGzRzI6a2R0X2O6TZvaUmZ0xsx+a2SeL9K9lDRGpsJ7W1ngAWAMOAjcBj5nZYXef3rSdAR8EvgNcA/xvM3vR3f971LlmziJSXefqOW/3kTCzJnA7cJ+7L7j7E8CjwJ2vOQT3z7j7X7h7y92fBr4K3JLtQ4OziFSbd7b/gH1m9uSGx12ber0OaLn7kQ3PHQaujw7FzAw4BGyeXb+GljVEpLIc8J0loZxw95uD9jFgftNzc8B40u+n6U6Kv5AdgAZnEaku79kNXheAzeFsE8AFQ3LM7KN0154PuXsa/qPBWUQqbYcz58wRoGFm17r7M2efu5ELLFeY2YeBe4G/5+4vFdmB1pxFpNp2tuYcd+m+CDwM3G9mTTO7BbgV+NLmbc3s/cC/B37e3Z8vetjmfZo9IyKSMbOvA/t28Ksn3P0fJH1PAZ8Hfh44Cdzr7l82s0PA19x97Ox2PwQuBzYuZfxXd/8XYf8anEVEykfLGiIiJaTBWUSkhDQ4i4iUkAZnEZES0uAsIlJCGpxFREpIg7OISAlpcBYRKaH/D403SeKK3V/AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of values: min: 0.1843137254901961 - max: 1.0\n",
      "Image type: float64\n",
      "Image shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "display.display_image_prop(X_test_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "cnt = Counter(list(y_train_full))\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sorted(cnt)\n",
    "data = [(k, l) for k, l in sorted([(i, j) for i, j in cnt.items()], reverse=False)]\n",
    "value = [v[0] for v in data]\n",
    "count = [v[1] for v in data]\n",
    "# print(value)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\seaborn\\_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import plot\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(value, count, alpha=0.8)\n",
    "plt.title('Dataset')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "# configure early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=40)\n",
    "###\n",
    "\n",
    "### Model saving\n",
    "model_saving_encoder = keras.callbacks.ModelCheckpoint(\n",
    "    # Path where to save the model\n",
    "    # The two parameters below mean that we will overwrite\n",
    "    # the current checkpoint if and only if\n",
    "    # the `val_loss` score has improved.\n",
    "    # The saved model name will include the current epoch.\n",
    "    filepath=ckpt_dir_encoder + \"/val_mse={val_mse:.4f}_mse={mse:.4f}_ckpt={epoch}\",\n",
    "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "    monitor=\"val_mse\",\n",
    "    save_weights_only=True,\n",
    "    mode=\"min\",\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "### Callback class\n",
    "class MyCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        starting_lr = self.model.optimizer.lr\n",
    "        actual_lr = self.model.optimizer._decayed_lr(tf.float32)\n",
    "        tf.print(\"Starting Learning Rate = \", starting_lr)\n",
    "        tf.print(\"Actual Learning Rate = \", actual_lr)\n",
    "\n",
    "\n",
    "my_callback = MyCallback()\n",
    "\n",
    "###\n",
    "\n",
    "### init callbacks\n",
    "callbacks_encoder = [\n",
    "    model_saving_encoder,\n",
    "    early_stopping,\n",
    "    my_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_uncompiled_model_encoder():\n",
    "    stacked_encoder = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=[32, 32, 3]),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-1\"),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-2\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-3\"),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-4\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-5\"),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-6\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Flatten(name=\"flatten1\")\n",
    "    ])\n",
    "    stacked_decoder = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Reshape([32, 32, 1], input_shape=[1024]),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2DTranspose(64, kernel_size=3, padding=\"SAME\", activation=\"selu\",\n",
    "                                        kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Conv2DTranspose(64, kernel_size=3, padding=\"SAME\", activation=\"selu\",\n",
    "                                        kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2DTranspose(32, kernel_size=3, padding=\"SAME\", activation=\"selu\",\n",
    "                                        kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Conv2DTranspose(32, kernel_size=3, padding=\"SAME\", activation=\"selu\",\n",
    "                                        kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2DTranspose(16, kernel_size=3, padding=\"SAME\", activation=\"selu\",\n",
    "                                        kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Conv2DTranspose(16, kernel_size=3, padding=\"SAME\", activation=\"selu\",\n",
    "                                        kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(3, kernel_size=3, padding=\"SAME\", activation=\"sigmoid\", kernel_initializer=\"he_normal\"),\n",
    "    ])\n",
    "    # stacked_encoder = keras.models.Sequential([\n",
    "    #     keras.layers.Input([32, 32, 3]),\n",
    "    #     keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    #     keras.layers.MaxPool2D(pool_size=2),\n",
    "    #     keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    #     keras.layers.MaxPool2D(pool_size=2),\n",
    "    #     keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\"),\n",
    "    #     keras.layers.MaxPool2D(pool_size=2)\n",
    "    # ])\n",
    "    # # stacked_encoder.summary()\n",
    "    # stacked_decoder = keras.models.Sequential([\n",
    "    #     keras.layers.Conv2DTranspose(32, kernel_size=1, strides=2, padding=\"VALID\", activation=\"selu\",\n",
    "    #                                  input_shape=[4, 4, 64]),\n",
    "    #     keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"SAME\", activation=\"selu\"),\n",
    "    #     keras.layers.Conv2DTranspose(3, kernel_size=3, strides=2, padding=\"SAME\", activation=\"sigmoid\"),\n",
    "    #     keras.layers.Reshape([32, 32, 3])\n",
    "    # ])\n",
    "    stacked_ae = tf.keras.models.Sequential([stacked_encoder, stacked_decoder])\n",
    "    # return\n",
    "    return stacked_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_or_restore_model_weights_only_uncompiled(model_create, ckpt_dir):\n",
    "    model = model_create()\n",
    "    # checkpoints = [checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)]\n",
    "    latest = tf.train.latest_checkpoint(ckpt_dir)\n",
    "    if latest:\n",
    "        model.load_weights(latest)\n",
    "        print(\"Restoring from\", latest)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./ckpt/img_encoder\\val_mse=0.0024_mse=0.0027_ckpt=5\n"
     ]
    }
   ],
   "source": [
    "# model_encoder = get_uncompiled_model_encoder()\n",
    "model_encoder = make_or_restore_model_weights_only_uncompiled(get_uncompiled_model_encoder, ckpt_dir_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wg = []\n",
    "# for layer in model_encoder.layers:\n",
    "#     if layer.name.startswith(\"encoder\"):\n",
    "#         wg = layer.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "nadam = tf.keras.optimizers.Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999)  #0.0005\n",
    "\n",
    "#Loss\n",
    "mse = 'mse'\n",
    "model_encoder.compile(loss=mse,\n",
    "                      optimizer=nadam,\n",
    "                      metrics=[mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.permutation at 0x0000020C17DAAEE8>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.__init__.<locals>.slice_batch_indices at 0x0000020C36587E58>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(40000,) dtype=int64>,)\n",
      "    kwargs: {}\n",
      "\n",
      "INFO:tensorflow:Converted call: <function TensorLikeDataAdapter.slice_inputs.<locals>.grab_batch at 0x0000020C36595048>\n",
      "    args: (<tf.Tensor 'args_0:0' shape=(32,) dtype=int64>, (<tf.Tensor 'args_1:0' shape=(40000, 32, 32, 3) dtype=float32>, <tf.Tensor 'args_2:0' shape=(40000, 32, 32, 3) dtype=float32>))\n",
      "    kwargs: {}\n",
      "\n",
      "Epoch 1/20\n",
      " 110/1250 [=>............................] - ETA: 17s - loss: 0.0027 - mse: 0.0027"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_5184/1676845269.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m                             \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m                             \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_valid_unlb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_valid_unlb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m                             callbacks=callbacks_encoder)\n\u001B[0m",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m     \u001B[1;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1096\u001B[0m                 batch_size=batch_size):\n\u001B[0;32m   1097\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1098\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1099\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1100\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    778\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    805\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    806\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 807\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    808\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    809\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2828\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2829\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2831\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[1;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1846\u001B[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001B[0;32m   1847\u001B[0m         \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1848\u001B[1;33m         cancellation_manager=cancellation_manager)\n\u001B[0m\u001B[0;32m   1849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1850\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1922\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1923\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1924\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1925\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1926\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    548\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 60\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = model_encoder.fit(X_train_unlb,\n",
    "                            X_train_unlb,\n",
    "                            epochs=20,\n",
    "                            batch_size=32,\n",
    "                            validation_data=(X_valid_unlb, X_valid_unlb),\n",
    "                            callbacks=callbacks_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 14 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_no = 7\n",
    "img = X_valid_unlb[:img_no]\n",
    "predictions_list = model_encoder.predict(img)\n",
    "display.display_multiple_images(images=[img,predictions_list],\n",
    "                                titles=[[\"Original img\"]*img_no,[\"Output img\"]*img_no],\n",
    "                                figure_size=(30,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_uncompiled_model_predict():\n",
    "    stacked_encoder = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=[32, 32, 3]),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-1\"),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-2\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-3\"),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-4\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-5\"),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-6\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Flatten(name=\"flatten1\")\n",
    "    ])\n",
    "    stack_predict = keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=[1024]),\n",
    "        # tf.keras.layers.Dense(1024, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(256, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(512, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(n_classes, activation=\"softmax\", name=\"output\")\n",
    "    ])\n",
    "\n",
    "    stacked_model = tf.keras.models.Sequential([stacked_encoder, stack_predict])\n",
    "    return stacked_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transfer_load_enc():\n",
    "    # load model\n",
    "    model_enc_tmp = make_or_restore_model_weights_only_uncompiled(get_uncompiled_model_encoder, ckpt_dir_encoder)\n",
    "    # Modify encoder layer name -> easy to do transfer learning\n",
    "    model_enc_tmp.layers[0]._name = str(\"encoder\")\n",
    "\n",
    "    # Create predict model\n",
    "    model_predict = get_uncompiled_model_predict()\n",
    "    # Modify encoder layer name -> easy to do transfer learning\n",
    "    model_predict.layers[0]._name = str(\"encoder\")\n",
    "    #Transfer weights\n",
    "    model_predict.get_layer(\"encoder\").set_weights(model_enc_tmp.get_layer(\"encoder\").get_weights())\n",
    "\n",
    "    return model_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_monitor = 'val_sparse_categorical_accuracy'\n",
    "mode =\"max\"\n",
    "### Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "# configure early stopping\n",
    "early_stopping = EarlyStopping(monitor=val_monitor, patience=40)\n",
    "###\n",
    "\n",
    "### Model saving\n",
    "model_saving_predict = keras.callbacks.ModelCheckpoint(\n",
    "    # Path where to save the model\n",
    "    # The two parameters below mean that we will overwrite\n",
    "    # the current checkpoint if and only if\n",
    "    # the `val_loss` score has improved.\n",
    "    # The saved model name will include the current epoch.\n",
    "    filepath=ckpt_dir_predict + \"/val_acc={val_sparse_categorical_accuracy:.4f}_acc={sparse_categorical_accuracy:.4f}_ckpt={epoch}\",\n",
    "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "    monitor=val_monitor,\n",
    "    save_weights_only=True,\n",
    "    mode=mode,\n",
    "    verbose=1,\n",
    ")\n",
    "###\n",
    "\n",
    "### Callback class\n",
    "class MyCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        starting_lr = self.model.optimizer.lr\n",
    "        actual_lr = self.model.optimizer._decayed_lr(tf.float32)\n",
    "        tf.print(\"Starting Learning Rate = \", starting_lr)\n",
    "        tf.print(\"Actual Learning Rate = \", actual_lr)\n",
    "\n",
    "\n",
    "my_callback = MyCallback()\n",
    "\n",
    "###\n",
    "\n",
    "### init callbacks\n",
    "callbacks_encoder = [\n",
    "    model_saving_predict,\n",
    "    # early_stopping,\n",
    "    my_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./ckpt/img_encoder\\val_mse=0.0024_mse=0.0027_ckpt=5\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR) # Suppress warnings\n",
    "model_predict = transfer_load_enc()\n",
    "# print(model_predict.get_layer(\"encoder\").get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze the encoder at first\n",
    "model_predict.get_layer(\"encoder\").trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_full, y_train_full, random_state=100, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "nadam = tf.keras.optimizers.Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999)  #0.0005\n",
    "sgd = keras.optimizers.SGD(lr=0.0002, decay=(10 * 1e-5), momentum=0.95)\n",
    "#Loss\n",
    "# cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "scc=keras.losses.SparseCategoricalCrossentropy(),\n",
    "\n",
    "\n",
    "#Metric\n",
    "acc = 'accuracy'\n",
    "cat_acc = tf.keras.metrics.categorical_accuracy\n",
    "sca=keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model_predict.compile(loss=scc,\n",
    "                      optimizer=sgd,\n",
    "                      metrics=sca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./ckpt/predict\\val_acc=0.1067_acc=0.0756_ckpt=67\n"
     ]
    }
   ],
   "source": [
    "# model_predict = make_or_restore_model_weights_only_uncompiled(get_uncompiled_model_predict, ckpt_dir_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 8.5501 - sparse_categorical_accuracy: 0.0114    \n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.01000, saving model to ./ckpt/predict\\val_acc=0.0100_acc=0.0114_ckpt=1\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000195599016\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 8.5163 - sparse_categorical_accuracy: 0.0114 - val_loss: 5.0169 - val_sparse_categorical_accuracy: 0.0100\n",
      "Epoch 2/400\n",
      "213/225 [===========================>..] - ETA: 0s - loss: 7.3395 - sparse_categorical_accuracy: 0.0109  \n",
      "Epoch 00002: val_sparse_categorical_accuracy did not improve from 0.01000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.00019138756\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 7.3215 - sparse_categorical_accuracy: 0.0106 - val_loss: 4.8481 - val_sparse_categorical_accuracy: 0.0089\n",
      "Epoch 3/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 6.6987 - sparse_categorical_accuracy: 0.0126  \n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.01000 to 0.01556, saving model to ./ckpt/predict\\val_acc=0.0156_acc=0.0125_ckpt=3\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000187353624\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 6.6967 - sparse_categorical_accuracy: 0.0125 - val_loss: 4.8631 - val_sparse_categorical_accuracy: 0.0156\n",
      "Epoch 4/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 6.4476 - sparse_categorical_accuracy: 0.0104  \n",
      "Epoch 00004: val_sparse_categorical_accuracy did not improve from 0.01556\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000183486234\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 6.4514 - sparse_categorical_accuracy: 0.0108 - val_loss: 4.7333 - val_sparse_categorical_accuracy: 0.0156\n",
      "Epoch 5/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 6.1700 - sparse_categorical_accuracy: 0.0118  \n",
      "Epoch 00005: val_sparse_categorical_accuracy improved from 0.01556 to 0.01667, saving model to ./ckpt/predict\\val_acc=0.0167_acc=0.0117_ckpt=5\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000179775278\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 6.1713 - sparse_categorical_accuracy: 0.0117 - val_loss: 4.7149 - val_sparse_categorical_accuracy: 0.0167\n",
      "Epoch 6/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 6.1128 - sparse_categorical_accuracy: 0.0112  \n",
      "Epoch 00006: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000176211455\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 6.1142 - sparse_categorical_accuracy: 0.0111 - val_loss: 4.7199 - val_sparse_categorical_accuracy: 0.0111\n",
      "Epoch 7/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 6.0089 - sparse_categorical_accuracy: 0.0134  \n",
      "Epoch 00007: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000172786167\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 6.0070 - sparse_categorical_accuracy: 0.0133 - val_loss: 4.6772 - val_sparse_categorical_accuracy: 0.0111\n",
      "Epoch 8/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.9203 - sparse_categorical_accuracy: 0.01\n",
      "Epoch 00008: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000169491526\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.9190 - sparse_categorical_accuracy: 0.0122 - val_loss: 4.6842 - val_sparse_categorical_accuracy: 0.0167\n",
      "Epoch 9/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 5.8659 - sparse_categorical_accuracy: 0.0094  \n",
      "Epoch 00009: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000166320169\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.8659 - sparse_categorical_accuracy: 0.0094 - val_loss: 4.7224 - val_sparse_categorical_accuracy: 0.0111\n",
      "Epoch 10/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 5.8267 - sparse_categorical_accuracy: 0.0105  \n",
      "Epoch 00010: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.0001632653\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.8216 - sparse_categorical_accuracy: 0.0103 - val_loss: 4.6689 - val_sparse_categorical_accuracy: 0.0133\n",
      "Epoch 11/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 5.7994 - sparse_categorical_accuracy: 0.0116  \n",
      "Epoch 00011: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000160320647\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.7982 - sparse_categorical_accuracy: 0.0114 - val_loss: 4.6675 - val_sparse_categorical_accuracy: 0.0122\n",
      "Epoch 12/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.7202 - sparse_categorical_accuracy: 0.0090  \n",
      "Epoch 00012: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000157480317\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.7194 - sparse_categorical_accuracy: 0.0092 - val_loss: 4.6816 - val_sparse_categorical_accuracy: 0.0133\n",
      "Epoch 13/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 5.7215 - sparse_categorical_accuracy: 0.0136  \n",
      "Epoch 00013: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000154738867\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.7215 - sparse_categorical_accuracy: 0.0136 - val_loss: 4.6974 - val_sparse_categorical_accuracy: 0.0111\n",
      "Epoch 14/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.6341 - sparse_categorical_accuracy: 0.0109  \n",
      "Epoch 00014: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000152091248\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.6343 - sparse_categorical_accuracy: 0.0108 - val_loss: 4.6750 - val_sparse_categorical_accuracy: 0.0133\n",
      "Epoch 15/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 5.6572 - sparse_categorical_accuracy: 0.00\n",
      "Epoch 00015: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000149532716\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.6557 - sparse_categorical_accuracy: 0.0092 - val_loss: 4.6601 - val_sparse_categorical_accuracy: 0.0111\n",
      "Epoch 16/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 5.5894 - sparse_categorical_accuracy: 0.0121  \n",
      "Epoch 00016: val_sparse_categorical_accuracy did not improve from 0.01667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000147058818\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.5873 - sparse_categorical_accuracy: 0.0119 - val_loss: 4.6888 - val_sparse_categorical_accuracy: 0.0056\n",
      "Epoch 17/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 5.5870 - sparse_categorical_accuracy: 0.0100  \n",
      "Epoch 00017: val_sparse_categorical_accuracy improved from 0.01667 to 0.01889, saving model to ./ckpt/predict\\val_acc=0.0189_acc=0.0100_ckpt=17\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000144665464\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.5870 - sparse_categorical_accuracy: 0.0100 - val_loss: 4.6735 - val_sparse_categorical_accuracy: 0.0189\n",
      "Epoch 18/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 5.5729 - sparse_categorical_accuracy: 0.0092  \n",
      "Epoch 00018: val_sparse_categorical_accuracy did not improve from 0.01889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000142348756\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.5787 - sparse_categorical_accuracy: 0.0089 - val_loss: 4.6485 - val_sparse_categorical_accuracy: 0.0133\n",
      "Epoch 19/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 5.5309 - sparse_categorical_accuracy: 0.0099  \n",
      "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.01889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000140105069\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.5342 - sparse_categorical_accuracy: 0.0097 - val_loss: 4.6811 - val_sparse_categorical_accuracy: 0.0133\n",
      "Epoch 20/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 5.4640 - sparse_categorical_accuracy: 0.0079  \n",
      "Epoch 00020: val_sparse_categorical_accuracy did not improve from 0.01889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000137931027\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.4677 - sparse_categorical_accuracy: 0.0078 - val_loss: 4.6604 - val_sparse_categorical_accuracy: 0.0144\n",
      "Epoch 21/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.4654 - sparse_categorical_accuracy: 0.0090  \n",
      "Epoch 00021: val_sparse_categorical_accuracy did not improve from 0.01889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000135823429\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.4674 - sparse_categorical_accuracy: 0.0089 - val_loss: 4.6498 - val_sparse_categorical_accuracy: 0.0178\n",
      "Epoch 22/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 5.4457 - sparse_categorical_accuracy: 0.0127  \n",
      "Epoch 00022: val_sparse_categorical_accuracy did not improve from 0.01889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000133779264\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.4429 - sparse_categorical_accuracy: 0.0125 - val_loss: 4.6527 - val_sparse_categorical_accuracy: 0.0111\n",
      "Epoch 23/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 5.4159 - sparse_categorical_accuracy: 0.0118  \n",
      "Epoch 00023: val_sparse_categorical_accuracy improved from 0.01889 to 0.02111, saving model to ./ckpt/predict\\val_acc=0.0211_acc=0.0117_ckpt=23\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000131795721\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.4134 - sparse_categorical_accuracy: 0.0117 - val_loss: 4.6303 - val_sparse_categorical_accuracy: 0.0211\n",
      "Epoch 24/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 5.3802 - sparse_categorical_accuracy: 0.0114  \n",
      "Epoch 00024: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000129870125\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.3802 - sparse_categorical_accuracy: 0.0114 - val_loss: 4.6299 - val_sparse_categorical_accuracy: 0.0178\n",
      "Epoch 25/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.3546 - sparse_categorical_accuracy: 0.0142  \n",
      "Epoch 00025: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000128\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.3535 - sparse_categorical_accuracy: 0.0144 - val_loss: 4.6220 - val_sparse_categorical_accuracy: 0.0200\n",
      "Epoch 26/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.3545 - sparse_categorical_accuracy: 0.01\n",
      "Epoch 00026: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.00012618296\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.3535 - sparse_categorical_accuracy: 0.0108 - val_loss: 4.6004 - val_sparse_categorical_accuracy: 0.0211\n",
      "Epoch 27/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.3432 - sparse_categorical_accuracy: 0.0143  \n",
      "Epoch 00027: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000124416794\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.3443 - sparse_categorical_accuracy: 0.0142 - val_loss: 4.6514 - val_sparse_categorical_accuracy: 0.0100\n",
      "Epoch 28/400\n",
      "188/225 [========================>.....] - ETA: 0s - loss: 5.3175 - sparse_categorical_accuracy: 0.0130   \n",
      "Epoch 00028: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000122699377\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.3190 - sparse_categorical_accuracy: 0.0117 - val_loss: 4.6230 - val_sparse_categorical_accuracy: 0.0156\n",
      "Epoch 29/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.3242 - sparse_categorical_accuracy: 0.0095  \n",
      "Epoch 00029: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000121028745\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.3229 - sparse_categorical_accuracy: 0.0097 - val_loss: 4.6220 - val_sparse_categorical_accuracy: 0.0144\n",
      "Epoch 30/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 5.2753 - sparse_categorical_accuracy: 0.0146  \n",
      "Epoch 00030: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000119402983\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.2737 - sparse_categorical_accuracy: 0.0147 - val_loss: 4.5977 - val_sparse_categorical_accuracy: 0.0189\n",
      "Epoch 31/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 5.2468 - sparse_categorical_accuracy: 0.0163  \n",
      "Epoch 00031: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000117820324\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.2481 - sparse_categorical_accuracy: 0.0161 - val_loss: 4.6267 - val_sparse_categorical_accuracy: 0.0211\n",
      "Epoch 32/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 5.2434 - sparse_categorical_accuracy: 0.0089  \n",
      "Epoch 00032: val_sparse_categorical_accuracy did not improve from 0.02111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000116279065\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.2420 - sparse_categorical_accuracy: 0.0089 - val_loss: 4.5746 - val_sparse_categorical_accuracy: 0.0178\n",
      "Epoch 33/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.2175 - sparse_categorical_accuracy: 0.0123  \n",
      "Epoch 00033: val_sparse_categorical_accuracy improved from 0.02111 to 0.02667, saving model to ./ckpt/predict\\val_acc=0.0267_acc=0.0122_ckpt=33\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000114777613\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.2197 - sparse_categorical_accuracy: 0.0122 - val_loss: 4.6032 - val_sparse_categorical_accuracy: 0.0267\n",
      "Epoch 34/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.2167 - sparse_categorical_accuracy: 0.0149  \n",
      "Epoch 00034: val_sparse_categorical_accuracy did not improve from 0.02667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000113314447\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.2162 - sparse_categorical_accuracy: 0.0147 - val_loss: 4.5866 - val_sparse_categorical_accuracy: 0.0144\n",
      "Epoch 35/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 5.1753 - sparse_categorical_accuracy: 0.0138  \n",
      "Epoch 00035: val_sparse_categorical_accuracy did not improve from 0.02667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000111888112\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.1752 - sparse_categorical_accuracy: 0.0136 - val_loss: 4.5866 - val_sparse_categorical_accuracy: 0.0222\n",
      "Epoch 36/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 5.1471 - sparse_categorical_accuracy: 0.01\n",
      "Epoch 00036: val_sparse_categorical_accuracy improved from 0.02667 to 0.02889, saving model to ./ckpt/predict\\val_acc=0.0289_acc=0.0178_ckpt=36\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.00011049724\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.1452 - sparse_categorical_accuracy: 0.0178 - val_loss: 4.5737 - val_sparse_categorical_accuracy: 0.0289\n",
      "Epoch 37/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.1126 - sparse_categorical_accuracy: 0.01\n",
      "Epoch 00037: val_sparse_categorical_accuracy improved from 0.02889 to 0.03222, saving model to ./ckpt/predict\\val_acc=0.0322_acc=0.0181_ckpt=37\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000109140514\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.1085 - sparse_categorical_accuracy: 0.0181 - val_loss: 4.5866 - val_sparse_categorical_accuracy: 0.0322\n",
      "Epoch 38/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.1385 - sparse_categorical_accuracy: 0.0156  \n",
      "Epoch 00038: val_sparse_categorical_accuracy did not improve from 0.03222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000107816704\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.1368 - sparse_categorical_accuracy: 0.0156 - val_loss: 4.5638 - val_sparse_categorical_accuracy: 0.0211\n",
      "Epoch 39/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.0890 - sparse_categorical_accuracy: 0.0148  \n",
      "Epoch 00039: val_sparse_categorical_accuracy did not improve from 0.03222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000106524625\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0891 - sparse_categorical_accuracy: 0.0147 - val_loss: 4.5525 - val_sparse_categorical_accuracy: 0.0222\n",
      "Epoch 40/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.1415 - sparse_categorical_accuracy: 0.01\n",
      "Epoch 00040: val_sparse_categorical_accuracy did not improve from 0.03222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000105263156\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.1457 - sparse_categorical_accuracy: 0.0150 - val_loss: 4.5255 - val_sparse_categorical_accuracy: 0.0256\n",
      "Epoch 41/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 5.0792 - sparse_categorical_accuracy: 0.0196  \n",
      "Epoch 00041: val_sparse_categorical_accuracy improved from 0.03222 to 0.03667, saving model to ./ckpt/predict\\val_acc=0.0367_acc=0.0197_ckpt=41\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000104031213\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0791 - sparse_categorical_accuracy: 0.0197 - val_loss: 4.5405 - val_sparse_categorical_accuracy: 0.0367\n",
      "Epoch 42/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 5.1113 - sparse_categorical_accuracy: 0.0170  \n",
      "Epoch 00042: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000102827762\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.1079 - sparse_categorical_accuracy: 0.0167 - val_loss: 4.5436 - val_sparse_categorical_accuracy: 0.0178\n",
      "Epoch 43/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.0800 - sparse_categorical_accuracy: 0.01\n",
      "Epoch 00043: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000101651844\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0784 - sparse_categorical_accuracy: 0.0161 - val_loss: 4.5434 - val_sparse_categorical_accuracy: 0.0256\n",
      "Epoch 44/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 5.0556 - sparse_categorical_accuracy: 0.0170  \n",
      "Epoch 00044: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  0.000100502511\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0560 - sparse_categorical_accuracy: 0.0169 - val_loss: 4.5244 - val_sparse_categorical_accuracy: 0.0222\n",
      "Epoch 45/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.0391 - sparse_categorical_accuracy: 0.01\n",
      "Epoch 00045: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.93788854e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0396 - sparse_categorical_accuracy: 0.0139 - val_loss: 4.5233 - val_sparse_categorical_accuracy: 0.0256\n",
      "Epoch 46/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 5.0244 - sparse_categorical_accuracy: 0.0135   \n",
      "Epoch 00046: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.82801066e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0251 - sparse_categorical_accuracy: 0.0139 - val_loss: 4.5201 - val_sparse_categorical_accuracy: 0.0256\n",
      "Epoch 47/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 5.0059 - sparse_categorical_accuracy: 0.0241   \n",
      "Epoch 00047: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.72053458e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0139 - sparse_categorical_accuracy: 0.0231 - val_loss: 4.5145 - val_sparse_categorical_accuracy: 0.0278\n",
      "Epoch 48/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.9861 - sparse_categorical_accuracy: 0.0142  \n",
      "Epoch 00048: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.61538462e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.9861 - sparse_categorical_accuracy: 0.0142 - val_loss: 4.5058 - val_sparse_categorical_accuracy: 0.0267\n",
      "Epoch 49/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 5.0033 - sparse_categorical_accuracy: 0.0201  \n",
      "Epoch 00049: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.51248512e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0041 - sparse_categorical_accuracy: 0.0200 - val_loss: 4.4929 - val_sparse_categorical_accuracy: 0.0289\n",
      "Epoch 50/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.9874 - sparse_categorical_accuracy: 0.0158  \n",
      "Epoch 00050: val_sparse_categorical_accuracy did not improve from 0.03667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.41176477e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.9820 - sparse_categorical_accuracy: 0.0164 - val_loss: 4.4860 - val_sparse_categorical_accuracy: 0.0222\n",
      "Epoch 51/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 5.0026 - sparse_categorical_accuracy: 0.0166  \n",
      "Epoch 00051: val_sparse_categorical_accuracy improved from 0.03667 to 0.04000, saving model to ./ckpt/predict\\val_acc=0.0400_acc=0.0169_ckpt=51\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.31315444e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 5.0023 - sparse_categorical_accuracy: 0.0169 - val_loss: 4.4894 - val_sparse_categorical_accuracy: 0.0400\n",
      "Epoch 52/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.9324 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00052: val_sparse_categorical_accuracy did not improve from 0.04000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.21658939e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.9303 - sparse_categorical_accuracy: 0.0214 - val_loss: 4.4964 - val_sparse_categorical_accuracy: 0.0200\n",
      "Epoch 53/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.9471 - sparse_categorical_accuracy: 0.0182  \n",
      "Epoch 00053: val_sparse_categorical_accuracy did not improve from 0.04000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.1220063e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.9476 - sparse_categorical_accuracy: 0.0181 - val_loss: 4.4744 - val_sparse_categorical_accuracy: 0.0233\n",
      "Epoch 54/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.9332 - sparse_categorical_accuracy: 0.0184  \n",
      "Epoch 00054: val_sparse_categorical_accuracy did not improve from 0.04000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  9.02934553e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.9294 - sparse_categorical_accuracy: 0.0183 - val_loss: 4.4702 - val_sparse_categorical_accuracy: 0.0278\n",
      "Epoch 55/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.8863 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00055: val_sparse_categorical_accuracy did not improve from 0.04000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.9385474e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8907 - sparse_categorical_accuracy: 0.0233 - val_loss: 4.4647 - val_sparse_categorical_accuracy: 0.0267\n",
      "Epoch 56/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.8947 - sparse_categorical_accuracy: 0.0200  \n",
      "Epoch 00056: val_sparse_categorical_accuracy did not improve from 0.04000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.84955734e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8931 - sparse_categorical_accuracy: 0.0200 - val_loss: 4.4491 - val_sparse_categorical_accuracy: 0.0300\n",
      "Epoch 57/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.9121 - sparse_categorical_accuracy: 0.0186  \n",
      "Epoch 00057: val_sparse_categorical_accuracy did not improve from 0.04000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.76232225e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.9172 - sparse_categorical_accuracy: 0.0186 - val_loss: 4.4396 - val_sparse_categorical_accuracy: 0.0322\n",
      "Epoch 58/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.8719 - sparse_categorical_accuracy: 0.0218   \n",
      "Epoch 00058: val_sparse_categorical_accuracy improved from 0.04000 to 0.04111, saving model to ./ckpt/predict\\val_acc=0.0411_acc=0.0228_ckpt=58\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.67679e-05\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 4.8625 - sparse_categorical_accuracy: 0.0228 - val_loss: 4.4562 - val_sparse_categorical_accuracy: 0.0411\n",
      "Epoch 59/400\n",
      "198/225 [=========================>....] - ETA: 0s - loss: 4.8601 - sparse_categorical_accuracy: 0.0234  \n",
      "Epoch 00059: val_sparse_categorical_accuracy did not improve from 0.04111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.59291104e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8631 - sparse_categorical_accuracy: 0.0231 - val_loss: 4.4647 - val_sparse_categorical_accuracy: 0.0356\n",
      "Epoch 60/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.8705 - sparse_categorical_accuracy: 0.0201   \n",
      "Epoch 00060: val_sparse_categorical_accuracy did not improve from 0.04111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.51063814e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8743 - sparse_categorical_accuracy: 0.0203 - val_loss: 4.4421 - val_sparse_categorical_accuracy: 0.0356\n",
      "Epoch 61/400\n",
      "200/225 [=========================>....] - ETA: 0s - loss: 4.8762 - sparse_categorical_accuracy: 0.0194  \n",
      "Epoch 00061: val_sparse_categorical_accuracy did not improve from 0.04111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.42992595e-05\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 4.8679 - sparse_categorical_accuracy: 0.0192 - val_loss: 4.4238 - val_sparse_categorical_accuracy: 0.0411\n",
      "Epoch 62/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.8693 - sparse_categorical_accuracy: 0.0217  \n",
      "Epoch 00062: val_sparse_categorical_accuracy did not improve from 0.04111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.35073079e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8545 - sparse_categorical_accuracy: 0.0214 - val_loss: 4.4192 - val_sparse_categorical_accuracy: 0.0333\n",
      "Epoch 63/400\n",
      "195/225 [=========================>....] - ETA: 0s - loss: 4.8319 - sparse_categorical_accuracy: 0.0212  \n",
      "Epoch 00063: val_sparse_categorical_accuracy did not improve from 0.04111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.27300901e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8232 - sparse_categorical_accuracy: 0.0214 - val_loss: 4.4222 - val_sparse_categorical_accuracy: 0.0400\n",
      "Epoch 64/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.8315 - sparse_categorical_accuracy: 0.0264   \n",
      "Epoch 00064: val_sparse_categorical_accuracy did not improve from 0.04111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.19672059e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8290 - sparse_categorical_accuracy: 0.0261 - val_loss: 4.4121 - val_sparse_categorical_accuracy: 0.0411\n",
      "Epoch 65/400\n",
      "202/225 [=========================>....] - ETA: 0s - loss: 4.8385 - sparse_categorical_accuracy: 0.0238  \n",
      "Epoch 00065: val_sparse_categorical_accuracy did not improve from 0.04111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.12182698e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8254 - sparse_categorical_accuracy: 0.0244 - val_loss: 4.3936 - val_sparse_categorical_accuracy: 0.0389\n",
      "Epoch 66/400\n",
      "209/225 [==========================>...] - ETA: 0s - loss: 4.7644 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00066: val_sparse_categorical_accuracy improved from 0.04111 to 0.05000, saving model to ./ckpt/predict\\val_acc=0.0500_acc=0.0253_ckpt=66\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  8.04828887e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7688 - sparse_categorical_accuracy: 0.0253 - val_loss: 4.3894 - val_sparse_categorical_accuracy: 0.0500\n",
      "Epoch 67/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.7843 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00067: val_sparse_categorical_accuracy did not improve from 0.05000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.97607208e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7875 - sparse_categorical_accuracy: 0.0272 - val_loss: 4.3935 - val_sparse_categorical_accuracy: 0.0411\n",
      "Epoch 68/400\n",
      "208/225 [==========================>...] - ETA: 0s - loss: 4.7979 - sparse_categorical_accuracy: 0.0198  \n",
      "Epoch 00068: val_sparse_categorical_accuracy did not improve from 0.05000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.90513805e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.8006 - sparse_categorical_accuracy: 0.0206 - val_loss: 4.3972 - val_sparse_categorical_accuracy: 0.0378\n",
      "Epoch 69/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.7523 - sparse_categorical_accuracy: 0.0289   \n",
      "Epoch 00069: val_sparse_categorical_accuracy did not improve from 0.05000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.83545547e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7639 - sparse_categorical_accuracy: 0.0289 - val_loss: 4.3793 - val_sparse_categorical_accuracy: 0.0422\n",
      "Epoch 70/400\n",
      "199/225 [=========================>....] - ETA: 0s - loss: 4.7667 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00070: val_sparse_categorical_accuracy did not improve from 0.05000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.76699089e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7717 - sparse_categorical_accuracy: 0.0244 - val_loss: 4.3868 - val_sparse_categorical_accuracy: 0.0400\n",
      "Epoch 71/400\n",
      "208/225 [==========================>...] - ETA: 0s - loss: 4.7515 - sparse_categorical_accuracy: 0.0240  \n",
      "Epoch 00071: val_sparse_categorical_accuracy did not improve from 0.05000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.69971157e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7449 - sparse_categorical_accuracy: 0.0242 - val_loss: 4.3793 - val_sparse_categorical_accuracy: 0.0456\n",
      "Epoch 72/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.7457 - sparse_categorical_accuracy: 0.0267  \n",
      "Epoch 00072: val_sparse_categorical_accuracy did not improve from 0.05000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.63358767e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7459 - sparse_categorical_accuracy: 0.0261 - val_loss: 4.3526 - val_sparse_categorical_accuracy: 0.0400\n",
      "Epoch 73/400\n",
      "200/225 [=========================>....] - ETA: 0s - loss: 4.7339 - sparse_categorical_accuracy: 0.0222  \n",
      "Epoch 00073: val_sparse_categorical_accuracy improved from 0.05000 to 0.05111, saving model to ./ckpt/predict\\val_acc=0.0511_acc=0.0222_ckpt=73\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.56859e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7364 - sparse_categorical_accuracy: 0.0222 - val_loss: 4.3514 - val_sparse_categorical_accuracy: 0.0511\n",
      "Epoch 74/400\n",
      "203/225 [==========================>...] - ETA: 0s - loss: 4.7249 - sparse_categorical_accuracy: 0.0296  \n",
      "Epoch 00074: val_sparse_categorical_accuracy did not improve from 0.05111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.50469044e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7224 - sparse_categorical_accuracy: 0.0275 - val_loss: 4.3466 - val_sparse_categorical_accuracy: 0.0400\n",
      "Epoch 75/400\n",
      "213/225 [===========================>..] - ETA: 0s - loss: 4.7367 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00075: val_sparse_categorical_accuracy did not improve from 0.05111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.44186e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7393 - sparse_categorical_accuracy: 0.0289 - val_loss: 4.3523 - val_sparse_categorical_accuracy: 0.0489\n",
      "Epoch 76/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.7356 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00076: val_sparse_categorical_accuracy did not improve from 0.05111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.38007366e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7342 - sparse_categorical_accuracy: 0.0272 - val_loss: 4.3331 - val_sparse_categorical_accuracy: 0.0511\n",
      "Epoch 77/400\n",
      "199/225 [=========================>....] - ETA: 0s - loss: 4.6910 - sparse_categorical_accuracy: 0.0267  \n",
      "Epoch 00077: val_sparse_categorical_accuracy improved from 0.05111 to 0.05333, saving model to ./ckpt/predict\\val_acc=0.0533_acc=0.0250_ckpt=77\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.31930413e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.7064 - sparse_categorical_accuracy: 0.0250 - val_loss: 4.3356 - val_sparse_categorical_accuracy: 0.0533\n",
      "Epoch 78/400\n",
      "213/225 [===========================>..] - ETA: 0s - loss: 4.7045 - sparse_categorical_accuracy: 0.0290   \n",
      "Epoch 00078: val_sparse_categorical_accuracy did not improve from 0.05333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.25952777e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7078 - sparse_categorical_accuracy: 0.0297 - val_loss: 4.3303 - val_sparse_categorical_accuracy: 0.0500\n",
      "Epoch 79/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.7202 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00079: val_sparse_categorical_accuracy improved from 0.05333 to 0.05556, saving model to ./ckpt/predict\\val_acc=0.0556_acc=0.0267_ckpt=79\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.20072e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.7146 - sparse_categorical_accuracy: 0.0267 - val_loss: 4.3177 - val_sparse_categorical_accuracy: 0.0556\n",
      "Epoch 80/400\n",
      "205/225 [==========================>...] - ETA: 0s - loss: 4.6786 - sparse_categorical_accuracy: 0.0302  \n",
      "Epoch 00080: val_sparse_categorical_accuracy did not improve from 0.05556\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.14285707e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.6858 - sparse_categorical_accuracy: 0.0300 - val_loss: 4.3184 - val_sparse_categorical_accuracy: 0.0522\n",
      "Epoch 81/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.6765 - sparse_categorical_accuracy: 0.0314  \n",
      "Epoch 00081: val_sparse_categorical_accuracy did not improve from 0.05556\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.08591688e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6766 - sparse_categorical_accuracy: 0.0300 - val_loss: 4.3056 - val_sparse_categorical_accuracy: 0.0544\n",
      "Epoch 82/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.6740 - sparse_categorical_accuracy: 0.0308  \n",
      "Epoch 00082: val_sparse_categorical_accuracy did not improve from 0.05556\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  7.02987745e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6619 - sparse_categorical_accuracy: 0.0314 - val_loss: 4.3021 - val_sparse_categorical_accuracy: 0.0522\n",
      "Epoch 83/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.6588 - sparse_categorical_accuracy: 0.0288  \n",
      "Epoch 00083: val_sparse_categorical_accuracy did not improve from 0.05556\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.97471696e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6666 - sparse_categorical_accuracy: 0.0286 - val_loss: 4.2912 - val_sparse_categorical_accuracy: 0.0511\n",
      "Epoch 84/400\n",
      "206/225 [==========================>...] - ETA: 0s - loss: 4.6661 - sparse_categorical_accuracy: 0.028\n",
      "Epoch 00084: val_sparse_categorical_accuracy did not improve from 0.05556\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.92041503e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6668 - sparse_categorical_accuracy: 0.0283 - val_loss: 4.2965 - val_sparse_categorical_accuracy: 0.0500\n",
      "Epoch 85/400\n",
      "212/225 [===========================>..] - ETA: 0s - loss: 4.6433 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00085: val_sparse_categorical_accuracy did not improve from 0.05556\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.86695275e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.6469 - sparse_categorical_accuracy: 0.0314 - val_loss: 4.3008 - val_sparse_categorical_accuracy: 0.0544\n",
      "Epoch 86/400\n",
      "200/225 [=========================>....] - ETA: 0s - loss: 4.6708 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00086: val_sparse_categorical_accuracy improved from 0.05556 to 0.06000, saving model to ./ckpt/predict\\val_acc=0.0600_acc=0.0289_ckpt=86\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.81431e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6642 - sparse_categorical_accuracy: 0.0289 - val_loss: 4.2891 - val_sparse_categorical_accuracy: 0.0600\n",
      "Epoch 87/400\n",
      "194/225 [========================>.....] - ETA: 0s - loss: 4.6396 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00087: val_sparse_categorical_accuracy did not improve from 0.06000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.76246782e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6376 - sparse_categorical_accuracy: 0.0317 - val_loss: 4.2999 - val_sparse_categorical_accuracy: 0.0533\n",
      "Epoch 88/400\n",
      "206/225 [==========================>...] - ETA: 0s - loss: 4.6384 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00088: val_sparse_categorical_accuracy improved from 0.06000 to 0.06333, saving model to ./ckpt/predict\\val_acc=0.0633_acc=0.0292_ckpt=88\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.71140951e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6367 - sparse_categorical_accuracy: 0.0292 - val_loss: 4.2953 - val_sparse_categorical_accuracy: 0.0633\n",
      "Epoch 89/400\n",
      "211/225 [===========================>..] - ETA: 0s - loss: 4.6239 - sparse_categorical_accuracy: 0.0335  \n",
      "Epoch 00089: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.66111519e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6196 - sparse_categorical_accuracy: 0.0336 - val_loss: 4.2765 - val_sparse_categorical_accuracy: 0.0556\n",
      "Epoch 90/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.6312 - sparse_categorical_accuracy: 0.0308  \n",
      "Epoch 00090: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.61157e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6308 - sparse_categorical_accuracy: 0.0314 - val_loss: 4.2901 - val_sparse_categorical_accuracy: 0.0533\n",
      "Epoch 91/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.6266 - sparse_categorical_accuracy: 0.0324  \n",
      "Epoch 00091: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.56275661e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6290 - sparse_categorical_accuracy: 0.0328 - val_loss: 4.2747 - val_sparse_categorical_accuracy: 0.0522\n",
      "Epoch 92/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.6244 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00092: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.51465816e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6229 - sparse_categorical_accuracy: 0.0369 - val_loss: 4.2769 - val_sparse_categorical_accuracy: 0.0533\n",
      "Epoch 93/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.6113 - sparse_categorical_accuracy: 0.02\n",
      "Epoch 00093: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.46726e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6084 - sparse_categorical_accuracy: 0.0281 - val_loss: 4.2770 - val_sparse_categorical_accuracy: 0.0633\n",
      "Epoch 94/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.6027 - sparse_categorical_accuracy: 0.0334  \n",
      "Epoch 00094: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.42054583e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.6005 - sparse_categorical_accuracy: 0.0342 - val_loss: 4.2523 - val_sparse_categorical_accuracy: 0.0544\n",
      "Epoch 95/400\n",
      "213/225 [===========================>..] - ETA: 0s - loss: 4.5760 - sparse_categorical_accuracy: 0.0440  \n",
      "Epoch 00095: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.37450139e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5791 - sparse_categorical_accuracy: 0.0436 - val_loss: 4.2808 - val_sparse_categorical_accuracy: 0.0533\n",
      "Epoch 96/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.6015 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00096: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.32911397e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5961 - sparse_categorical_accuracy: 0.0336 - val_loss: 4.2887 - val_sparse_categorical_accuracy: 0.0489\n",
      "Epoch 97/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.5817 - sparse_categorical_accuracy: 0.0388  \n",
      "Epoch 00097: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.28436756e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5791 - sparse_categorical_accuracy: 0.0400 - val_loss: 4.2696 - val_sparse_categorical_accuracy: 0.0589\n",
      "Epoch 98/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.5692 - sparse_categorical_accuracy: 0.0377  \n",
      "Epoch 00098: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.24025e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5700 - sparse_categorical_accuracy: 0.0375 - val_loss: 4.2576 - val_sparse_categorical_accuracy: 0.0556\n",
      "Epoch 99/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.5668 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00099: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.19674684e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5732 - sparse_categorical_accuracy: 0.0353 - val_loss: 4.2564 - val_sparse_categorical_accuracy: 0.0556\n",
      "Epoch 100/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.5835 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00100: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.15384633e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5867 - sparse_categorical_accuracy: 0.0369 - val_loss: 4.2385 - val_sparse_categorical_accuracy: 0.0567\n",
      "Epoch 101/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.5783 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00101: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.11153519e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5742 - sparse_categorical_accuracy: 0.0317 - val_loss: 4.2452 - val_sparse_categorical_accuracy: 0.0567\n",
      "Epoch 102/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.5358 - sparse_categorical_accuracy: 0.0359  \n",
      "Epoch 00102: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.06980284e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5405 - sparse_categorical_accuracy: 0.0356 - val_loss: 4.2322 - val_sparse_categorical_accuracy: 0.0622\n",
      "Epoch 103/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.5567 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00103: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  6.0286362e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5593 - sparse_categorical_accuracy: 0.0344 - val_loss: 4.2358 - val_sparse_categorical_accuracy: 0.0589\n",
      "Epoch 104/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.5255 - sparse_categorical_accuracy: 0.0322  \n",
      "Epoch 00104: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.98802399e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5246 - sparse_categorical_accuracy: 0.0325 - val_loss: 4.2399 - val_sparse_categorical_accuracy: 0.0544\n",
      "Epoch 105/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.5742 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00105: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.94795529e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5739 - sparse_categorical_accuracy: 0.0308 - val_loss: 4.2503 - val_sparse_categorical_accuracy: 0.0544\n",
      "Epoch 106/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.5582 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00106: val_sparse_categorical_accuracy did not improve from 0.06333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.90841919e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5598 - sparse_categorical_accuracy: 0.0311 - val_loss: 4.2409 - val_sparse_categorical_accuracy: 0.0589\n",
      "Epoch 107/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.5435 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00107: val_sparse_categorical_accuracy improved from 0.06333 to 0.06667, saving model to ./ckpt/predict\\val_acc=0.0667_acc=0.0356_ckpt=107\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.8694055e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5416 - sparse_categorical_accuracy: 0.0356 - val_loss: 4.2210 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 108/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.5384 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00108: val_sparse_categorical_accuracy did not improve from 0.06667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.83090405e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5397 - sparse_categorical_accuracy: 0.0361 - val_loss: 4.2509 - val_sparse_categorical_accuracy: 0.0533\n",
      "Epoch 109/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.5213 - sparse_categorical_accuracy: 0.0317  \n",
      "Epoch 00109: val_sparse_categorical_accuracy did not improve from 0.06667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.7929039e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5180 - sparse_categorical_accuracy: 0.0319 - val_loss: 4.2386 - val_sparse_categorical_accuracy: 0.0633\n",
      "Epoch 110/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.5319 - sparse_categorical_accuracy: 0.0372  \n",
      "Epoch 00110: val_sparse_categorical_accuracy did not improve from 0.06667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.75539561e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5345 - sparse_categorical_accuracy: 0.0364 - val_loss: 4.2204 - val_sparse_categorical_accuracy: 0.0656\n",
      "Epoch 111/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.5036 - sparse_categorical_accuracy: 0.0376  \n",
      "Epoch 00111: val_sparse_categorical_accuracy improved from 0.06667 to 0.07222, saving model to ./ckpt/predict\\val_acc=0.0722_acc=0.0375_ckpt=111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.71837e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5058 - sparse_categorical_accuracy: 0.0375 - val_loss: 4.1997 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 112/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.5179 - sparse_categorical_accuracy: 0.0402  \n",
      "Epoch 00112: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.68181822e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5190 - sparse_categorical_accuracy: 0.0400 - val_loss: 4.2173 - val_sparse_categorical_accuracy: 0.0633\n",
      "Epoch 113/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.5042 - sparse_categorical_accuracy: 0.0392  \n",
      "Epoch 00113: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.6457302e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5016 - sparse_categorical_accuracy: 0.0400 - val_loss: 4.2084 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 114/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.5228 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00114: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.61009838e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5273 - sparse_categorical_accuracy: 0.0361 - val_loss: 4.2093 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 115/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.4926 - sparse_categorical_accuracy: 0.0347  \n",
      "Epoch 00115: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.57491294e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4954 - sparse_categorical_accuracy: 0.0344 - val_loss: 4.2079 - val_sparse_categorical_accuracy: 0.0578\n",
      "Epoch 116/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.4776 - sparse_categorical_accuracy: 0.0415  \n",
      "Epoch 00116: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.54016624e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4742 - sparse_categorical_accuracy: 0.0411 - val_loss: 4.2026 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 117/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.4882 - sparse_categorical_accuracy: 0.0400  \n",
      "Epoch 00117: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.50585e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4930 - sparse_categorical_accuracy: 0.0394 - val_loss: 4.2131 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 118/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.5040 - sparse_categorical_accuracy: 0.0365  \n",
      "Epoch 00118: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.47195596e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.5014 - sparse_categorical_accuracy: 0.0358 - val_loss: 4.2067 - val_sparse_categorical_accuracy: 0.0656\n",
      "Epoch 119/400\n",
      "188/225 [========================>.....] - ETA: 0s - loss: 4.4985 - sparse_categorical_accuracy: 0.0366   \n",
      "Epoch 00119: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.43847709e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4990 - sparse_categorical_accuracy: 0.0353 - val_loss: 4.2147 - val_sparse_categorical_accuracy: 0.0600\n",
      "Epoch 120/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.4332 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00120: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.40540532e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4393 - sparse_categorical_accuracy: 0.0389 - val_loss: 4.2050 - val_sparse_categorical_accuracy: 0.0589\n",
      "Epoch 121/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.4998 - sparse_categorical_accuracy: 0.0368  \n",
      "Epoch 00121: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.37273336e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4940 - sparse_categorical_accuracy: 0.0375 - val_loss: 4.2058 - val_sparse_categorical_accuracy: 0.0589\n",
      "Epoch 122/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.4809 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00122: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.34045394e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4791 - sparse_categorical_accuracy: 0.0386 - val_loss: 4.1918 - val_sparse_categorical_accuracy: 0.0622\n",
      "Epoch 123/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.4928 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00123: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.30856e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4917 - sparse_categorical_accuracy: 0.0375 - val_loss: 4.1892 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 124/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.4781 - sparse_categorical_accuracy: 0.0414  \n",
      "Epoch 00124: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.27704469e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4806 - sparse_categorical_accuracy: 0.0411 - val_loss: 4.1817 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 125/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.4295 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00125: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.24590141e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4325 - sparse_categorical_accuracy: 0.0486 - val_loss: 4.1886 - val_sparse_categorical_accuracy: 0.0544\n",
      "Epoch 126/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.4834 - sparse_categorical_accuracy: 0.0350  \n",
      "Epoch 00126: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.21512375e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4809 - sparse_categorical_accuracy: 0.0350 - val_loss: 4.1729 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 127/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.4406 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00127: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.18470515e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4432 - sparse_categorical_accuracy: 0.0394 - val_loss: 4.1694 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 128/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.4291 - sparse_categorical_accuracy: 0.0437  \n",
      "Epoch 00128: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.15463908e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4276 - sparse_categorical_accuracy: 0.0439 - val_loss: 4.1685 - val_sparse_categorical_accuracy: 0.0611\n",
      "Epoch 129/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.4466 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00129: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.12492e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4480 - sparse_categorical_accuracy: 0.0431 - val_loss: 4.1846 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 130/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.4373 - sparse_categorical_accuracy: 0.0442  \n",
      "Epoch 00130: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.0955412e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4356 - sparse_categorical_accuracy: 0.0436 - val_loss: 4.1699 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 131/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.4362 - sparse_categorical_accuracy: 0.0394  \n",
      "Epoch 00131: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.06649776e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4355 - sparse_categorical_accuracy: 0.0400 - val_loss: 4.1580 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 132/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.4595 - sparse_categorical_accuracy: 0.0386  \n",
      "Epoch 00132: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.03778319e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4573 - sparse_categorical_accuracy: 0.0392 - val_loss: 4.1652 - val_sparse_categorical_accuracy: 0.0622\n",
      "Epoch 133/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.4256 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00133: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  5.00939277e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4297 - sparse_categorical_accuracy: 0.0425 - val_loss: 4.1685 - val_sparse_categorical_accuracy: 0.0633\n",
      "Epoch 134/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.4018 - sparse_categorical_accuracy: 0.0493  \n",
      "Epoch 00134: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.98132e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4039 - sparse_categorical_accuracy: 0.0486 - val_loss: 4.1778 - val_sparse_categorical_accuracy: 0.0600\n",
      "Epoch 135/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.4267 - sparse_categorical_accuracy: 0.0426  \n",
      "Epoch 00135: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.95356035e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4226 - sparse_categorical_accuracy: 0.0439 - val_loss: 4.1679 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 136/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.4289 - sparse_categorical_accuracy: 0.0443  \n",
      "Epoch 00136: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.92610816e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4271 - sparse_categorical_accuracy: 0.0447 - val_loss: 4.1712 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 137/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.4453 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00137: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.89895901e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4440 - sparse_categorical_accuracy: 0.0403 - val_loss: 4.1628 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 138/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.3996 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00138: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.87210709e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4015 - sparse_categorical_accuracy: 0.0467 - val_loss: 4.1497 - val_sparse_categorical_accuracy: 0.0678\n",
      "Epoch 139/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.4103 - sparse_categorical_accuracy: 0.0464  \n",
      "Epoch 00139: val_sparse_categorical_accuracy did not improve from 0.07222\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.84554839e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4103 - sparse_categorical_accuracy: 0.0464 - val_loss: 4.1639 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 140/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.4078 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00140: val_sparse_categorical_accuracy improved from 0.07222 to 0.07444, saving model to ./ckpt/predict\\val_acc=0.0744_acc=0.0419_ckpt=140\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.81927746e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4078 - sparse_categorical_accuracy: 0.0419 - val_loss: 4.1607 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 141/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.4373 - sparse_categorical_accuracy: 0.0428  \n",
      "Epoch 00141: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.79328955e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4345 - sparse_categorical_accuracy: 0.0425 - val_loss: 4.1524 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 142/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.4301 - sparse_categorical_accuracy: 0.0489  \n",
      "Epoch 00142: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.76758069e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4336 - sparse_categorical_accuracy: 0.0489 - val_loss: 4.1635 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 143/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.4044 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00143: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.74214612e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3993 - sparse_categorical_accuracy: 0.0483 - val_loss: 4.1557 - val_sparse_categorical_accuracy: 0.0622\n",
      "Epoch 144/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.3847 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00144: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.71698113e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3845 - sparse_categorical_accuracy: 0.0444 - val_loss: 4.1489 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 145/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.3888 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00145: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.69208208e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3888 - sparse_categorical_accuracy: 0.0494 - val_loss: 4.1543 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 146/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.3990 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00146: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.66744459e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.4011 - sparse_categorical_accuracy: 0.0500 - val_loss: 4.1598 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 147/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3953 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00147: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.64306431e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3888 - sparse_categorical_accuracy: 0.0453 - val_loss: 4.1486 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 148/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.3802 - sparse_categorical_accuracy: 0.0431  \n",
      "Epoch 00148: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.6189376e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3795 - sparse_categorical_accuracy: 0.0442 - val_loss: 4.1460 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 149/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.3806 - sparse_categorical_accuracy: 0.0505  \n",
      "Epoch 00149: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.59506e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3825 - sparse_categorical_accuracy: 0.0497 - val_loss: 4.1430 - val_sparse_categorical_accuracy: 0.0633\n",
      "Epoch 150/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3954 - sparse_categorical_accuracy: 0.0437   \n",
      "Epoch 00150: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.57142851e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3941 - sparse_categorical_accuracy: 0.0444 - val_loss: 4.1507 - val_sparse_categorical_accuracy: 0.0622\n",
      "Epoch 151/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.3649 - sparse_categorical_accuracy: 0.03\n",
      "Epoch 00151: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.54803849e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.3617 - sparse_categorical_accuracy: 0.0408 - val_loss: 4.1324 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 152/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.3836 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00152: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.52488675e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3812 - sparse_categorical_accuracy: 0.0481 - val_loss: 4.1430 - val_sparse_categorical_accuracy: 0.0689\n",
      "Epoch 153/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.3638 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00153: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.50196931e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3663 - sparse_categorical_accuracy: 0.0450 - val_loss: 4.1403 - val_sparse_categorical_accuracy: 0.0689\n",
      "Epoch 154/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.3681 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00154: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.47928287e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3730 - sparse_categorical_accuracy: 0.0486 - val_loss: 4.1362 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 155/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.3789 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00155: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.45682417e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3815 - sparse_categorical_accuracy: 0.0444 - val_loss: 4.1255 - val_sparse_categorical_accuracy: 0.0656\n",
      "Epoch 156/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3900 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00156: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.43458957e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3905 - sparse_categorical_accuracy: 0.0500 - val_loss: 4.1218 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 157/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.3581 - sparse_categorical_accuracy: 0.0471  \n",
      "Epoch 00157: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.41257544e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3554 - sparse_categorical_accuracy: 0.0475 - val_loss: 4.1256 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 158/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.3664 - sparse_categorical_accuracy: 0.0446  \n",
      "Epoch 00158: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.39077958e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3646 - sparse_categorical_accuracy: 0.0447 - val_loss: 4.1321 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 159/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3612 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00159: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.36919727e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3616 - sparse_categorical_accuracy: 0.0450 - val_loss: 4.1225 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 160/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.3587 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00160: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.34782596e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3587 - sparse_categorical_accuracy: 0.0467 - val_loss: 4.1193 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 161/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.3424 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00161: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.32666311e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3438 - sparse_categorical_accuracy: 0.0514 - val_loss: 4.1240 - val_sparse_categorical_accuracy: 0.0689\n",
      "Epoch 162/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.3841 - sparse_categorical_accuracy: 0.0391  \n",
      "Epoch 00162: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.30570508e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3823 - sparse_categorical_accuracy: 0.0386 - val_loss: 4.1211 - val_sparse_categorical_accuracy: 0.0600\n",
      "Epoch 163/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.3481 - sparse_categorical_accuracy: 0.0523  \n",
      "Epoch 00163: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.28494895e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3508 - sparse_categorical_accuracy: 0.0519 - val_loss: 4.1157 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 164/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.3476 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00164: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.26439256e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3488 - sparse_categorical_accuracy: 0.0464 - val_loss: 4.1157 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 165/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.3433 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00165: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.24403224e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3429 - sparse_categorical_accuracy: 0.0475 - val_loss: 4.1217 - val_sparse_categorical_accuracy: 0.0633\n",
      "Epoch 166/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.3581 - sparse_categorical_accuracy: 0.0464  \n",
      "Epoch 00166: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.22386511e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3571 - sparse_categorical_accuracy: 0.0458 - val_loss: 4.1248 - val_sparse_categorical_accuracy: 0.0622\n",
      "Epoch 167/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3494 - sparse_categorical_accuracy: 0.0449  \n",
      "Epoch 00167: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.2038886e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3456 - sparse_categorical_accuracy: 0.0461 - val_loss: 4.1207 - val_sparse_categorical_accuracy: 0.0678\n",
      "Epoch 168/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.3620 - sparse_categorical_accuracy: 0.0510  \n",
      "Epoch 00168: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.18410054e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3572 - sparse_categorical_accuracy: 0.0517 - val_loss: 4.1240 - val_sparse_categorical_accuracy: 0.0644\n",
      "Epoch 169/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.3585 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00169: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.16449766e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3568 - sparse_categorical_accuracy: 0.0453 - val_loss: 4.1291 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 170/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.3540 - sparse_categorical_accuracy: 0.0462  \n",
      "Epoch 00170: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.14507776e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3567 - sparse_categorical_accuracy: 0.0458 - val_loss: 4.1160 - val_sparse_categorical_accuracy: 0.0656\n",
      "Epoch 171/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.3429 - sparse_categorical_accuracy: 0.0504  \n",
      "Epoch 00171: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.12583795e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3421 - sparse_categorical_accuracy: 0.0506 - val_loss: 4.1112 - val_sparse_categorical_accuracy: 0.0689\n",
      "Epoch 172/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3444 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00172: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.10677603e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3435 - sparse_categorical_accuracy: 0.0519 - val_loss: 4.1123 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 173/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3489 - sparse_categorical_accuracy: 0.0486  \n",
      "Epoch 00173: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.08788947e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3513 - sparse_categorical_accuracy: 0.0478 - val_loss: 4.1027 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 174/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.3178 - sparse_categorical_accuracy: 0.0443   \n",
      "Epoch 00174: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.06917607e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3140 - sparse_categorical_accuracy: 0.0458 - val_loss: 4.1080 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 175/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.3222 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00175: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.05063292e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3203 - sparse_categorical_accuracy: 0.0561 - val_loss: 4.0998 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 176/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3537 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00176: val_sparse_categorical_accuracy did not improve from 0.07444\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.03225786e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3485 - sparse_categorical_accuracy: 0.0475 - val_loss: 4.1050 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 177/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3369 - sparse_categorical_accuracy: 0.0497  \n",
      "Epoch 00177: val_sparse_categorical_accuracy improved from 0.07444 to 0.07889, saving model to ./ckpt/predict\\val_acc=0.0789_acc=0.0494_ckpt=177\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  4.01404905e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3336 - sparse_categorical_accuracy: 0.0494 - val_loss: 4.1048 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 178/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.3492 - sparse_categorical_accuracy: 0.0458  \n",
      "Epoch 00178: val_sparse_categorical_accuracy did not improve from 0.07889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.99600394e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3488 - sparse_categorical_accuracy: 0.0456 - val_loss: 4.0947 - val_sparse_categorical_accuracy: 0.0667\n",
      "Epoch 179/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.3332 - sparse_categorical_accuracy: 0.0529   \n",
      "Epoch 00179: val_sparse_categorical_accuracy did not improve from 0.07889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.97812037e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3336 - sparse_categorical_accuracy: 0.0506 - val_loss: 4.0934 - val_sparse_categorical_accuracy: 0.0656\n",
      "Epoch 180/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.3223 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00180: val_sparse_categorical_accuracy did not improve from 0.07889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.96039613e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3234 - sparse_categorical_accuracy: 0.0450 - val_loss: 4.0949 - val_sparse_categorical_accuracy: 0.0689\n",
      "Epoch 181/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.2958 - sparse_categorical_accuracy: 0.052\n",
      "Epoch 00181: val_sparse_categorical_accuracy did not improve from 0.07889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.94282906e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3010 - sparse_categorical_accuracy: 0.0522 - val_loss: 4.0851 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 182/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.3284 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00182: val_sparse_categorical_accuracy did not improve from 0.07889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.92541697e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3284 - sparse_categorical_accuracy: 0.0458 - val_loss: 4.1015 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 183/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.3089 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00183: val_sparse_categorical_accuracy did not improve from 0.07889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.9081584e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3092 - sparse_categorical_accuracy: 0.0531 - val_loss: 4.1015 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 184/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.3167 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00184: val_sparse_categorical_accuracy did not improve from 0.07889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.89105044e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3179 - sparse_categorical_accuracy: 0.0503 - val_loss: 4.1111 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 185/400\n",
      "191/225 [========================>.....] - ETA: 0s - loss: 4.3075 - sparse_categorical_accuracy: 0.0576   \n",
      "Epoch 00185: val_sparse_categorical_accuracy improved from 0.07889 to 0.08000, saving model to ./ckpt/predict\\val_acc=0.0800_acc=0.0556_ckpt=185\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.874092e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3195 - sparse_categorical_accuracy: 0.0556 - val_loss: 4.1021 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 186/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.3001 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00186: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.85728054e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2966 - sparse_categorical_accuracy: 0.0564 - val_loss: 4.1001 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 187/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.3029 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00187: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.84061459e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3026 - sparse_categorical_accuracy: 0.0544 - val_loss: 4.1024 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 188/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.3075 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00188: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.82409162e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3074 - sparse_categorical_accuracy: 0.0522 - val_loss: 4.1002 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 189/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.3033 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00189: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.80771053e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3042 - sparse_categorical_accuracy: 0.0558 - val_loss: 4.0911 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 190/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.2836 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00190: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.79146913e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2864 - sparse_categorical_accuracy: 0.0539 - val_loss: 4.0903 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 191/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 4.2708 - sparse_categorical_accuracy: 0.050\n",
      "Epoch 00191: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.77536599e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2800 - sparse_categorical_accuracy: 0.0483 - val_loss: 4.0895 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 192/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.2920 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00192: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.75939853e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2908 - sparse_categorical_accuracy: 0.0550 - val_loss: 4.0902 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 193/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2902 - sparse_categorical_accuracy: 0.0574  \n",
      "Epoch 00193: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.74356568e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2886 - sparse_categorical_accuracy: 0.0572 - val_loss: 4.0985 - val_sparse_categorical_accuracy: 0.0689\n",
      "Epoch 194/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2954 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00194: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.72786599e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2954 - sparse_categorical_accuracy: 0.0542 - val_loss: 4.0984 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 195/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2970 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00195: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.71229689e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2970 - sparse_categorical_accuracy: 0.0539 - val_loss: 4.0879 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 196/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.3101 - sparse_categorical_accuracy: 0.050\n",
      "Epoch 00196: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.69685768e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2997 - sparse_categorical_accuracy: 0.0522 - val_loss: 4.0860 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 197/400\n",
      "188/225 [========================>.....] - ETA: 0s - loss: 4.2968 - sparse_categorical_accuracy: 0.053\n",
      "Epoch 00197: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.68154615e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.3027 - sparse_categorical_accuracy: 0.0519 - val_loss: 4.0868 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 198/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2911 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00198: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.66636123e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2903 - sparse_categorical_accuracy: 0.0536 - val_loss: 4.0896 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 199/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2944 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00199: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.65130072e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2994 - sparse_categorical_accuracy: 0.0558 - val_loss: 4.0727 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 200/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.2939 - sparse_categorical_accuracy: 0.0493  \n",
      "Epoch 00200: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.63636354e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2914 - sparse_categorical_accuracy: 0.0483 - val_loss: 4.0736 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 201/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.2848 - sparse_categorical_accuracy: 0.053\n",
      "Epoch 00201: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.62154824e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2828 - sparse_categorical_accuracy: 0.0517 - val_loss: 4.0818 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 202/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.2829 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00202: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.60685299e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2845 - sparse_categorical_accuracy: 0.0567 - val_loss: 4.0735 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 203/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.2873 - sparse_categorical_accuracy: 0.0611  \n",
      "Epoch 00203: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.59227633e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2866 - sparse_categorical_accuracy: 0.0606 - val_loss: 4.0776 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 204/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 4.2671 - sparse_categorical_accuracy: 0.056\n",
      "Epoch 00204: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.57781755e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2651 - sparse_categorical_accuracy: 0.0558 - val_loss: 4.0765 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 205/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.2607 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00205: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.56347446e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2595 - sparse_categorical_accuracy: 0.0600 - val_loss: 4.0817 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 206/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.2866 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00206: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.54924596e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2832 - sparse_categorical_accuracy: 0.0536 - val_loss: 4.0803 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 207/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.2561 - sparse_categorical_accuracy: 0.0572  \n",
      "Epoch 00207: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.53513024e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2663 - sparse_categorical_accuracy: 0.0564 - val_loss: 4.0814 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 208/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.2976 - sparse_categorical_accuracy: 0.0537  \n",
      "Epoch 00208: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.52112693e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.2978 - sparse_categorical_accuracy: 0.0553 - val_loss: 4.0782 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 209/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.2676 - sparse_categorical_accuracy: 0.0571  \n",
      "Epoch 00209: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.50723349e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.2716 - sparse_categorical_accuracy: 0.0575 - val_loss: 4.0735 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 210/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.2497 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00210: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.49345e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2490 - sparse_categorical_accuracy: 0.0594 - val_loss: 4.0876 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 211/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.2973 - sparse_categorical_accuracy: 0.0505  \n",
      "Epoch 00211: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.47977366e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2947 - sparse_categorical_accuracy: 0.0517 - val_loss: 4.0847 - val_sparse_categorical_accuracy: 0.0700\n",
      "Epoch 212/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.2602 - sparse_categorical_accuracy: 0.0528  \n",
      "Epoch 00212: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.46620436e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2612 - sparse_categorical_accuracy: 0.0536 - val_loss: 4.0765 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 213/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.2730 - sparse_categorical_accuracy: 0.0551  \n",
      "Epoch 00213: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.45274057e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2764 - sparse_categorical_accuracy: 0.0550 - val_loss: 4.0848 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 214/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.2884 - sparse_categorical_accuracy: 0.0482  \n",
      "Epoch 00214: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.43938082e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2881 - sparse_categorical_accuracy: 0.0481 - val_loss: 4.0865 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 215/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.2446 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00215: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.42612402e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2492 - sparse_categorical_accuracy: 0.0572 - val_loss: 4.0787 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 216/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.2856 - sparse_categorical_accuracy: 0.0543  \n",
      "Epoch 00216: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.41296945e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2801 - sparse_categorical_accuracy: 0.0531 - val_loss: 4.0824 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 217/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.2541 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00217: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.39991493e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2518 - sparse_categorical_accuracy: 0.0631 - val_loss: 4.0845 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 218/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.2499 - sparse_categorical_accuracy: 0.0587  \n",
      "Epoch 00218: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.38696045e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2467 - sparse_categorical_accuracy: 0.0589 - val_loss: 4.0700 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 219/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.2752 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00219: val_sparse_categorical_accuracy did not improve from 0.08000\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.37410384e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2722 - sparse_categorical_accuracy: 0.0561 - val_loss: 4.0742 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 220/400\n",
      "213/225 [===========================>..] - ETA: 0s - loss: 4.2598 - sparse_categorical_accuracy: 0.0552  \n",
      "Epoch 00220: val_sparse_categorical_accuracy improved from 0.08000 to 0.08111, saving model to ./ckpt/predict\\val_acc=0.0811_acc=0.0550_ckpt=220\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.36134472e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2613 - sparse_categorical_accuracy: 0.0550 - val_loss: 4.0719 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 221/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.2661 - sparse_categorical_accuracy: 0.0561  \n",
      "Epoch 00221: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.34868164e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2632 - sparse_categorical_accuracy: 0.0567 - val_loss: 4.0748 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 222/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.2448 - sparse_categorical_accuracy: 0.0564  \n",
      "Epoch 00222: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.33611351e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2435 - sparse_categorical_accuracy: 0.0578 - val_loss: 4.0817 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 223/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.2611 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00223: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.32363925e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2603 - sparse_categorical_accuracy: 0.0569 - val_loss: 4.0766 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 224/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.2334 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00224: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.31125811e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2376 - sparse_categorical_accuracy: 0.0619 - val_loss: 4.0677 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 225/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.2621 - sparse_categorical_accuracy: 0.0602  \n",
      "Epoch 00225: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.29896902e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2641 - sparse_categorical_accuracy: 0.0608 - val_loss: 4.0728 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 226/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.2370 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00226: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.28677052e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2361 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.0577 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 227/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.2248 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00227: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.27466223e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2239 - sparse_categorical_accuracy: 0.0642 - val_loss: 4.0686 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 228/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.2269 - sparse_categorical_accuracy: 0.0625  \n",
      "Epoch 00228: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.26264271e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2289 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.0649 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 229/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.2741 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00229: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.25071123e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2722 - sparse_categorical_accuracy: 0.0553 - val_loss: 4.0747 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 230/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.2379 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00230: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.23886634e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2342 - sparse_categorical_accuracy: 0.0581 - val_loss: 4.0668 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 231/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.2424 - sparse_categorical_accuracy: 0.0525  \n",
      "Epoch 00231: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.22710766e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2466 - sparse_categorical_accuracy: 0.0528 - val_loss: 4.0654 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 232/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.2264 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00232: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.21543412e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2272 - sparse_categorical_accuracy: 0.0667 - val_loss: 4.0636 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 233/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2472 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00233: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.20384461e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2433 - sparse_categorical_accuracy: 0.0589 - val_loss: 4.0650 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 234/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2399 - sparse_categorical_accuracy: 0.0560  \n",
      "Epoch 00234: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.19233841e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2403 - sparse_categorical_accuracy: 0.0558 - val_loss: 4.0617 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 235/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.2630 - sparse_categorical_accuracy: 0.0510  \n",
      "Epoch 00235: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.18091443e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2594 - sparse_categorical_accuracy: 0.0517 - val_loss: 4.0645 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 236/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.2332 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00236: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.16957194e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2335 - sparse_categorical_accuracy: 0.0611 - val_loss: 4.0587 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 237/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.2294 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00237: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.15831021e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2253 - sparse_categorical_accuracy: 0.0561 - val_loss: 4.0611 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 238/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.2226 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00238: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.14712815e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2226 - sparse_categorical_accuracy: 0.0636 - val_loss: 4.0616 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 239/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2204 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00239: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.13602504e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2228 - sparse_categorical_accuracy: 0.0603 - val_loss: 4.0464 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 240/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.2155 - sparse_categorical_accuracy: 0.0572  \n",
      "Epoch 00240: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.12499978e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2157 - sparse_categorical_accuracy: 0.0569 - val_loss: 4.0607 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 241/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.2444 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00241: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.11405238e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2475 - sparse_categorical_accuracy: 0.0583 - val_loss: 4.0544 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 242/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2218 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00242: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.10318101e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2199 - sparse_categorical_accuracy: 0.0567 - val_loss: 4.0622 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 243/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2318 - sparse_categorical_accuracy: 0.0511  \n",
      "Epoch 00243: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.09238494e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2318 - sparse_categorical_accuracy: 0.0511 - val_loss: 4.0626 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 244/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.2394 - sparse_categorical_accuracy: 0.0572   \n",
      "Epoch 00244: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.08166418e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2385 - sparse_categorical_accuracy: 0.0572 - val_loss: 4.0673 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 245/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.2300 - sparse_categorical_accuracy: 0.062\n",
      "Epoch 00245: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.07101727e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2192 - sparse_categorical_accuracy: 0.0650 - val_loss: 4.0596 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 246/400\n",
      "208/225 [==========================>...] - ETA: 0s - loss: 4.2222 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00246: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.06044385e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2243 - sparse_categorical_accuracy: 0.0611 - val_loss: 4.0540 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 247/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.2167 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00247: val_sparse_categorical_accuracy did not improve from 0.08111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.04994282e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2160 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.0495 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 248/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.2064 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00248: val_sparse_categorical_accuracy improved from 0.08111 to 0.08333, saving model to ./ckpt/predict\\val_acc=0.0833_acc=0.0594_ckpt=248\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.03951365e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2081 - sparse_categorical_accuracy: 0.0594 - val_loss: 4.0446 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 249/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.2255 - sparse_categorical_accuracy: 0.061\n",
      "Epoch 00249: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.0291556e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2275 - sparse_categorical_accuracy: 0.0608 - val_loss: 4.0435 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 250/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 4.2107 - sparse_categorical_accuracy: 0.062\n",
      "Epoch 00250: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.01886794e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2252 - sparse_categorical_accuracy: 0.0594 - val_loss: 4.0565 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 251/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2313 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00251: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  3.00864976e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2342 - sparse_categorical_accuracy: 0.0575 - val_loss: 4.0578 - val_sparse_categorical_accuracy: 0.0711\n",
      "Epoch 252/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2299 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00252: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.99850071e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2299 - sparse_categorical_accuracy: 0.0611 - val_loss: 4.0595 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 253/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.2265 - sparse_categorical_accuracy: 0.0541  \n",
      "Epoch 00253: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.98841987e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2264 - sparse_categorical_accuracy: 0.0539 - val_loss: 4.0544 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 254/400\n",
      "191/225 [========================>.....] - ETA: 0s - loss: 4.2257 - sparse_categorical_accuracy: 0.059\n",
      "Epoch 00254: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.9784067e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2220 - sparse_categorical_accuracy: 0.0592 - val_loss: 4.0580 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 255/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2004 - sparse_categorical_accuracy: 0.0597  \n",
      "Epoch 00255: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.9684601e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2004 - sparse_categorical_accuracy: 0.0600 - val_loss: 4.0502 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 256/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.2008 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00256: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.9585799e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1976 - sparse_categorical_accuracy: 0.0614 - val_loss: 4.0550 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 257/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.2076 - sparse_categorical_accuracy: 0.063\n",
      "Epoch 00257: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.94876518e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2108 - sparse_categorical_accuracy: 0.0617 - val_loss: 4.0443 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 258/400\n",
      "191/225 [========================>.....] - ETA: 0s - loss: 4.2037 - sparse_categorical_accuracy: 0.058\n",
      "Epoch 00258: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.93901539e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2136 - sparse_categorical_accuracy: 0.0594 - val_loss: 4.0542 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 259/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.2124 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00259: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.92932982e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2137 - sparse_categorical_accuracy: 0.0611 - val_loss: 4.0496 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 260/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.2184 - sparse_categorical_accuracy: 0.0645   \n",
      "Epoch 00260: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.91970791e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2235 - sparse_categorical_accuracy: 0.0653 - val_loss: 4.0494 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 261/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.2273 - sparse_categorical_accuracy: 0.0564  \n",
      "Epoch 00261: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.91014912e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2258 - sparse_categorical_accuracy: 0.0564 - val_loss: 4.0499 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 262/400\n",
      "191/225 [========================>.....] - ETA: 0s - loss: 4.2098 - sparse_categorical_accuracy: 0.0645   \n",
      "Epoch 00262: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.90065254e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2131 - sparse_categorical_accuracy: 0.0644 - val_loss: 4.0476 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 263/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.2019 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00263: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.89121781e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2038 - sparse_categorical_accuracy: 0.0683 - val_loss: 4.0454 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 264/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.2194 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00264: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.88184419e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2187 - sparse_categorical_accuracy: 0.0664 - val_loss: 4.0485 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 265/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.1948 - sparse_categorical_accuracy: 0.0636  \n",
      "Epoch 00265: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.87253151e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1962 - sparse_categorical_accuracy: 0.0636 - val_loss: 4.0530 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 266/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2054 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00266: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.8632785e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2092 - sparse_categorical_accuracy: 0.0631 - val_loss: 4.0528 - val_sparse_categorical_accuracy: 0.0733\n",
      "Epoch 267/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.1756 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00267: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.85408496e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1779 - sparse_categorical_accuracy: 0.0683 - val_loss: 4.0485 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 268/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2315 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00268: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.84495018e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2315 - sparse_categorical_accuracy: 0.0603 - val_loss: 4.0524 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 269/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2041 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00269: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.83587378e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2041 - sparse_categorical_accuracy: 0.0594 - val_loss: 4.0531 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 270/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2305 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00270: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.82685505e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2305 - sparse_categorical_accuracy: 0.0633 - val_loss: 4.0524 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 271/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1932 - sparse_categorical_accuracy: 0.0633  \n",
      "Epoch 00271: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.81789362e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1939 - sparse_categorical_accuracy: 0.0633 - val_loss: 4.0371 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 272/400\n",
      "189/225 [========================>.....] - ETA: 0s - loss: 4.1768 - sparse_categorical_accuracy: 0.067\n",
      "Epoch 00272: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.80898876e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1887 - sparse_categorical_accuracy: 0.0667 - val_loss: 4.0416 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 273/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.2106 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00273: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.80014e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2108 - sparse_categorical_accuracy: 0.0611 - val_loss: 4.0501 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 274/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.2026 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00274: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.79134674e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2014 - sparse_categorical_accuracy: 0.0606 - val_loss: 4.0521 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 275/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1686 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00275: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.78260868e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1679 - sparse_categorical_accuracy: 0.0628 - val_loss: 4.0477 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 276/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.2079 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00276: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.77392501e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2079 - sparse_categorical_accuracy: 0.0633 - val_loss: 4.0418 - val_sparse_categorical_accuracy: 0.0744\n",
      "Epoch 277/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.2006 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00277: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.76529536e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2006 - sparse_categorical_accuracy: 0.0625 - val_loss: 4.0399 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 278/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 4.1994 - sparse_categorical_accuracy: 0.061\n",
      "Epoch 00278: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.75671955e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2006 - sparse_categorical_accuracy: 0.0617 - val_loss: 4.0377 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 279/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.1741 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00279: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.74819649e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1741 - sparse_categorical_accuracy: 0.0664 - val_loss: 4.0346 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 280/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1785 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00280: val_sparse_categorical_accuracy did not improve from 0.08333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.73972601e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1814 - sparse_categorical_accuracy: 0.0644 - val_loss: 4.0296 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 281/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1907 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00281: val_sparse_categorical_accuracy improved from 0.08333 to 0.08667, saving model to ./ckpt/predict\\val_acc=0.0867_acc=0.0622_ckpt=281\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.73130772e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1901 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.0266 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 282/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1759 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00282: val_sparse_categorical_accuracy did not improve from 0.08667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.72294074e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1800 - sparse_categorical_accuracy: 0.0647 - val_loss: 4.0422 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 283/400\n",
      "211/225 [===========================>..] - ETA: 0s - loss: 4.1810 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00283: val_sparse_categorical_accuracy did not improve from 0.08667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.71462504e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1818 - sparse_categorical_accuracy: 0.0642 - val_loss: 4.0361 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 284/400\n",
      "198/225 [=========================>....] - ETA: 0s - loss: 4.1888 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00284: val_sparse_categorical_accuracy did not improve from 0.08667\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.70636e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1927 - sparse_categorical_accuracy: 0.0642 - val_loss: 4.0342 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 285/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1804 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00285: val_sparse_categorical_accuracy improved from 0.08667 to 0.08778, saving model to ./ckpt/predict\\val_acc=0.0878_acc=0.0642_ckpt=285\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.698145e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1772 - sparse_categorical_accuracy: 0.0642 - val_loss: 4.0239 - val_sparse_categorical_accuracy: 0.0878\n",
      "Epoch 286/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.2063 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00286: val_sparse_categorical_accuracy did not improve from 0.08778\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.68997974e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.2049 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.0377 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 287/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.1574 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00287: val_sparse_categorical_accuracy did not improve from 0.08778\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.68186377e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.1598 - sparse_categorical_accuracy: 0.0697 - val_loss: 4.0322 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 288/400\n",
      "199/225 [=========================>....] - ETA: 0s - loss: 4.1757 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00288: val_sparse_categorical_accuracy did not improve from 0.08778\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.67379673e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1811 - sparse_categorical_accuracy: 0.0703 - val_loss: 4.0266 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 289/400\n",
      "210/225 [===========================>..] - ETA: 0s - loss: 4.1916 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00289: val_sparse_categorical_accuracy did not improve from 0.08778\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.6657779e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1900 - sparse_categorical_accuracy: 0.0561 - val_loss: 4.0344 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 290/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1979 - sparse_categorical_accuracy: 0.0642  \n",
      "Epoch 00290: val_sparse_categorical_accuracy did not improve from 0.08778\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.65780745e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1959 - sparse_categorical_accuracy: 0.0650 - val_loss: 4.0385 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 291/400\n",
      "205/225 [==========================>...] - ETA: 0s - loss: 4.1773 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00291: val_sparse_categorical_accuracy did not improve from 0.08778\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.64988412e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1776 - sparse_categorical_accuracy: 0.0619 - val_loss: 4.0305 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 292/400\n",
      "204/225 [==========================>...] - ETA: 0s - loss: 4.1658 - sparse_categorical_accuracy: 0.067\n",
      "Epoch 00292: val_sparse_categorical_accuracy improved from 0.08778 to 0.08889, saving model to ./ckpt/predict\\val_acc=0.0889_acc=0.0669_ckpt=292\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.64200789e-05\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.1683 - sparse_categorical_accuracy: 0.0669 - val_loss: 4.0364 - val_sparse_categorical_accuracy: 0.0889\n",
      "Epoch 293/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.1830 - sparse_categorical_accuracy: 0.060\n",
      "Epoch 00293: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.63417842e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1784 - sparse_categorical_accuracy: 0.0614 - val_loss: 4.0303 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 294/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.1906 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00294: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.62639533e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1946 - sparse_categorical_accuracy: 0.0650 - val_loss: 4.0403 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 295/400\n",
      "212/225 [===========================>..] - ETA: 0s - loss: 4.1793 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00295: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.61865789e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1807 - sparse_categorical_accuracy: 0.0597 - val_loss: 4.0381 - val_sparse_categorical_accuracy: 0.0722\n",
      "Epoch 296/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1865 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00296: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.61096611e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1884 - sparse_categorical_accuracy: 0.0619 - val_loss: 4.0301 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 297/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.1773 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00297: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.60331926e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1772 - sparse_categorical_accuracy: 0.0683 - val_loss: 4.0277 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 298/400\n",
      "199/225 [=========================>....] - ETA: 0s - loss: 4.1831 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00298: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.59571698e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1787 - sparse_categorical_accuracy: 0.0658 - val_loss: 4.0317 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 299/400\n",
      "205/225 [==========================>...] - ETA: 0s - loss: 4.1857 - sparse_categorical_accuracy: 0.0607  \n",
      "Epoch 00299: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.58815908e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1797 - sparse_categorical_accuracy: 0.0603 - val_loss: 4.0227 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 300/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.1685 - sparse_categorical_accuracy: 0.063\n",
      "Epoch 00300: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.58064501e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1690 - sparse_categorical_accuracy: 0.0636 - val_loss: 4.0273 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 301/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.1911 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00301: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.57317461e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1958 - sparse_categorical_accuracy: 0.0642 - val_loss: 4.0271 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 302/400\n",
      "197/225 [=========================>....] - ETA: 0s - loss: 4.1505 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00302: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.56574731e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1528 - sparse_categorical_accuracy: 0.0625 - val_loss: 4.0320 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 303/400\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 4.1509 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00303: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.55836276e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1522 - sparse_categorical_accuracy: 0.0733 - val_loss: 4.0266 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 304/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1727 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00304: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.55102041e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1733 - sparse_categorical_accuracy: 0.0661 - val_loss: 4.0216 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 305/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1702 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00305: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.54372026e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1705 - sparse_categorical_accuracy: 0.0664 - val_loss: 4.0291 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 306/400\n",
      "224/225 [============================>.] - ETA: 0s - loss: 4.1619 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00306: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.53646158e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1631 - sparse_categorical_accuracy: 0.0636 - val_loss: 4.0219 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 307/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 4.1674 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00307: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.52924438e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1836 - sparse_categorical_accuracy: 0.0650 - val_loss: 4.0429 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 308/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1562 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00308: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.5220681e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1583 - sparse_categorical_accuracy: 0.0675 - val_loss: 4.0269 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 309/400\n",
      "209/225 [==========================>...] - ETA: 0s - loss: 4.1694 - sparse_categorical_accuracy: 0.064\n",
      "Epoch 00309: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.51493238e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1687 - sparse_categorical_accuracy: 0.0653 - val_loss: 4.0263 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 310/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.1960 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00310: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.50783687e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1942 - sparse_categorical_accuracy: 0.0694 - val_loss: 4.0286 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 311/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1548 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00311: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.50078137e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1533 - sparse_categorical_accuracy: 0.0653 - val_loss: 4.0198 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 312/400\n",
      "210/225 [===========================>..] - ETA: 0s - loss: 4.1590 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00312: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.49376535e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1688 - sparse_categorical_accuracy: 0.0631 - val_loss: 4.0224 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 313/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1777 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00313: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.48678898e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1746 - sparse_categorical_accuracy: 0.0661 - val_loss: 4.0195 - val_sparse_categorical_accuracy: 0.0889\n",
      "Epoch 314/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1456 - sparse_categorical_accuracy: 0.0662  \n",
      "Epoch 00314: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.47985099e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1444 - sparse_categorical_accuracy: 0.0664 - val_loss: 4.0222 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 315/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.1531 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00315: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.47295211e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1526 - sparse_categorical_accuracy: 0.0706 - val_loss: 4.0175 - val_sparse_categorical_accuracy: 0.0878\n",
      "Epoch 316/400\n",
      "191/225 [========================>.....] - ETA: 0s - loss: 4.1552 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00316: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.46609125e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1601 - sparse_categorical_accuracy: 0.0656 - val_loss: 4.0169 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 317/400\n",
      "206/225 [==========================>...] - ETA: 0s - loss: 4.1582 - sparse_categorical_accuracy: 0.068\n",
      "Epoch 00317: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.4592684e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1607 - sparse_categorical_accuracy: 0.0672 - val_loss: 4.0161 - val_sparse_categorical_accuracy: 0.0889\n",
      "Epoch 318/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.1630 - sparse_categorical_accuracy: 0.066\n",
      "Epoch 00318: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.4524832e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1668 - sparse_categorical_accuracy: 0.0661 - val_loss: 4.0177 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 319/400\n",
      "208/225 [==========================>...] - ETA: 0s - loss: 4.1453 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00319: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.4457353e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1509 - sparse_categorical_accuracy: 0.0703 - val_loss: 4.0213 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 320/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.1665 - sparse_categorical_accuracy: 0.0743   \n",
      "Epoch 00320: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.43902432e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1693 - sparse_categorical_accuracy: 0.0714 - val_loss: 4.0182 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 321/400\n",
      "212/225 [===========================>..] - ETA: 0s - loss: 4.1522 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00321: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.43235027e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1528 - sparse_categorical_accuracy: 0.0644 - val_loss: 4.0223 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 322/400\n",
      "210/225 [===========================>..] - ETA: 0s - loss: 4.1753 - sparse_categorical_accuracy: 0.061\n",
      "Epoch 00322: val_sparse_categorical_accuracy did not improve from 0.08889\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.42571259e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1814 - sparse_categorical_accuracy: 0.0603 - val_loss: 4.0294 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 323/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.1456 - sparse_categorical_accuracy: 0.0671  \n",
      "Epoch 00323: val_sparse_categorical_accuracy improved from 0.08889 to 0.09111, saving model to ./ckpt/predict\\val_acc=0.0911_acc=0.0678_ckpt=323\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.41911093e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1447 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0077 - val_sparse_categorical_accuracy: 0.0911\n",
      "Epoch 324/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 4.1530 - sparse_categorical_accuracy: 0.0691  \n",
      "Epoch 00324: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.41254511e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1616 - sparse_categorical_accuracy: 0.0683 - val_loss: 4.0209 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 325/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1645 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00325: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.40601494e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1616 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0201 - val_sparse_categorical_accuracy: 0.0878\n",
      "Epoch 326/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1315 - sparse_categorical_accuracy: 0.0699  \n",
      "Epoch 00326: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.39952e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1334 - sparse_categorical_accuracy: 0.0694 - val_loss: 4.0196 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 327/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1224 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00327: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.39306009e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1227 - sparse_categorical_accuracy: 0.0708 - val_loss: 4.0159 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 328/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1403 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00328: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.38663506e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1421 - sparse_categorical_accuracy: 0.0708 - val_loss: 4.0197 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 329/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1389 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00329: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.38024386e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1351 - sparse_categorical_accuracy: 0.0739 - val_loss: 4.0194 - val_sparse_categorical_accuracy: 0.0778\n",
      "Epoch 330/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1582 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00330: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.3738874e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1568 - sparse_categorical_accuracy: 0.0653 - val_loss: 4.0225 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 331/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1553 - sparse_categorical_accuracy: 0.0659  \n",
      "Epoch 00331: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.36756423e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1549 - sparse_categorical_accuracy: 0.0658 - val_loss: 4.0202 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 332/400\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 4.1565 - sparse_categorical_accuracy: 0.0672  \n",
      "Epoch 00332: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.36127526e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1590 - sparse_categorical_accuracy: 0.0669 - val_loss: 4.0251 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 333/400\n",
      "212/225 [===========================>..] - ETA: 0s - loss: 4.1623 - sparse_categorical_accuracy: 0.0672  \n",
      "Epoch 00333: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.35501902e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1572 - sparse_categorical_accuracy: 0.0667 - val_loss: 4.0188 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 334/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.1532 - sparse_categorical_accuracy: 0.0694  \n",
      "Epoch 00334: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.34879626e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1551 - sparse_categorical_accuracy: 0.0686 - val_loss: 4.0251 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 335/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1365 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00335: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.34260606e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1408 - sparse_categorical_accuracy: 0.0644 - val_loss: 4.0171 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 336/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.1415 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00336: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.3364486e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1409 - sparse_categorical_accuracy: 0.0664 - val_loss: 4.0168 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 337/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.1527 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00337: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.33032315e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1484 - sparse_categorical_accuracy: 0.0703 - val_loss: 4.0095 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 338/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1487 - sparse_categorical_accuracy: 0.0682  \n",
      "Epoch 00338: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.32423e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1506 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0112 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 339/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1530 - sparse_categorical_accuracy: 0.0659  \n",
      "Epoch 00339: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.31816866e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1531 - sparse_categorical_accuracy: 0.0653 - val_loss: 4.0250 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 340/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.1518 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00340: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.31213871e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1508 - sparse_categorical_accuracy: 0.0731 - val_loss: 4.0131 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 341/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.1621 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00341: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.30614e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1609 - sparse_categorical_accuracy: 0.0681 - val_loss: 4.0069 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 342/400\n",
      "211/225 [===========================>..] - ETA: 0s - loss: 4.1616 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00342: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.30017249e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1617 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0035 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 343/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.1416 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00343: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.29423567e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1431 - sparse_categorical_accuracy: 0.0681 - val_loss: 4.0098 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 344/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1330 - sparse_categorical_accuracy: 0.0685  \n",
      "Epoch 00344: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.2883296e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1328 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0160 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 345/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1455 - sparse_categorical_accuracy: 0.0699  \n",
      "Epoch 00345: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.28245372e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1464 - sparse_categorical_accuracy: 0.0700 - val_loss: 4.0090 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 346/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.1317 - sparse_categorical_accuracy: 0.0706  \n",
      "Epoch 00346: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.27660785e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1344 - sparse_categorical_accuracy: 0.0694 - val_loss: 4.0088 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 347/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.1414 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00347: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.27079199e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1417 - sparse_categorical_accuracy: 0.0714 - val_loss: 4.0150 - val_sparse_categorical_accuracy: 0.0900\n",
      "Epoch 348/400\n",
      "190/225 [========================>.....] - ETA: 0s - loss: 4.1628 - sparse_categorical_accuracy: 0.0645  \n",
      "Epoch 00348: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.26500561e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1549 - sparse_categorical_accuracy: 0.0681 - val_loss: 4.0137 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 349/400\n",
      "197/225 [=========================>....] - ETA: 0s - loss: 4.1233 - sparse_categorical_accuracy: 0.0685  \n",
      "Epoch 00349: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.25924869e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1278 - sparse_categorical_accuracy: 0.0686 - val_loss: 4.0103 - val_sparse_categorical_accuracy: 0.0900\n",
      "Epoch 350/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1397 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00350: val_sparse_categorical_accuracy did not improve from 0.09111\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.25352105e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1443 - sparse_categorical_accuracy: 0.0758 - val_loss: 4.0087 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 351/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1378 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00351: val_sparse_categorical_accuracy improved from 0.09111 to 0.09333, saving model to ./ckpt/predict\\val_acc=0.0933_acc=0.0625_ckpt=351\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.24782234e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1395 - sparse_categorical_accuracy: 0.0625 - val_loss: 3.9965 - val_sparse_categorical_accuracy: 0.0933\n",
      "Epoch 352/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.1329 - sparse_categorical_accuracy: 0.0677  \n",
      "Epoch 00352: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.24215237e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1350 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0044 - val_sparse_categorical_accuracy: 0.0933\n",
      "Epoch 353/400\n",
      "206/225 [==========================>...] - ETA: 0s - loss: 4.1351 - sparse_categorical_accuracy: 0.0692  \n",
      "Epoch 00353: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.23651114e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1319 - sparse_categorical_accuracy: 0.0681 - val_loss: 4.0092 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 354/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.1340 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00354: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.23089792e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1336 - sparse_categorical_accuracy: 0.0719 - val_loss: 4.0047 - val_sparse_categorical_accuracy: 0.0878\n",
      "Epoch 355/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1355 - sparse_categorical_accuracy: 0.0707  \n",
      "Epoch 00355: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.22531307e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1348 - sparse_categorical_accuracy: 0.0708 - val_loss: 4.0095 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 356/400\n",
      "194/225 [========================>.....] - ETA: 0s - loss: 4.1508 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00356: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.2197557e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1531 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.0064 - val_sparse_categorical_accuracy: 0.0911\n",
      "Epoch 357/400\n",
      "208/225 [==========================>...] - ETA: 0s - loss: 4.1431 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00357: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.21422633e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1465 - sparse_categorical_accuracy: 0.0722 - val_loss: 4.0117 - val_sparse_categorical_accuracy: 0.0789\n",
      "Epoch 358/400\n",
      "191/225 [========================>.....] - ETA: 0s - loss: 4.1450 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00358: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.20872462e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1382 - sparse_categorical_accuracy: 0.0658 - val_loss: 4.0091 - val_sparse_categorical_accuracy: 0.0889\n",
      "Epoch 359/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1424 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00359: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.20324982e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1449 - sparse_categorical_accuracy: 0.0664 - val_loss: 4.0063 - val_sparse_categorical_accuracy: 0.0878\n",
      "Epoch 360/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.1351 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00360: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.19780231e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1331 - sparse_categorical_accuracy: 0.0642 - val_loss: 4.0065 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 361/400\n",
      "207/225 [==========================>...] - ETA: 0s - loss: 4.1284 - sparse_categorical_accuracy: 0.0704  \n",
      "Epoch 00361: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.19238154e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1264 - sparse_categorical_accuracy: 0.0714 - val_loss: 4.0096 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 362/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1314 - sparse_categorical_accuracy: 0.0651   \n",
      "Epoch 00362: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.18698751e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1311 - sparse_categorical_accuracy: 0.0650 - val_loss: 4.0091 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 363/400\n",
      "197/225 [=========================>....] - ETA: 0s - loss: 4.1348 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00363: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.18161986e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1339 - sparse_categorical_accuracy: 0.0694 - val_loss: 3.9999 - val_sparse_categorical_accuracy: 0.0900\n",
      "Epoch 364/400\n",
      "196/225 [=========================>....] - ETA: 0s - loss: 4.1342 - sparse_categorical_accuracy: 0.0651  \n",
      "Epoch 00364: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.17627858e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1305 - sparse_categorical_accuracy: 0.0658 - val_loss: 4.0029 - val_sparse_categorical_accuracy: 0.0900\n",
      "Epoch 365/400\n",
      "195/225 [=========================>....] - ETA: 0s - loss: 4.1394 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00365: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.17096349e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1328 - sparse_categorical_accuracy: 0.0714 - val_loss: 4.0075 - val_sparse_categorical_accuracy: 0.0878\n",
      "Epoch 366/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.1095 - sparse_categorical_accuracy: 0.0703  \n",
      "Epoch 00366: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.16567405e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1140 - sparse_categorical_accuracy: 0.0697 - val_loss: 4.0077 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 367/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1427 - sparse_categorical_accuracy: 0.0727  \n",
      "Epoch 00367: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.16041044e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1465 - sparse_categorical_accuracy: 0.0728 - val_loss: 4.0053 - val_sparse_categorical_accuracy: 0.0889\n",
      "Epoch 368/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1407 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00368: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.15517248e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1392 - sparse_categorical_accuracy: 0.0742 - val_loss: 4.0058 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 369/400\n",
      "213/225 [===========================>..] - ETA: 0s - loss: 4.1311 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00369: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.14995962e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1360 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0090 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 370/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1647 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00370: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.14477204e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1651 - sparse_categorical_accuracy: 0.0656 - val_loss: 4.0087 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 371/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1443 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00371: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.13960957e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1427 - sparse_categorical_accuracy: 0.0683 - val_loss: 4.0094 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 372/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1226 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00372: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.13447165e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1209 - sparse_categorical_accuracy: 0.0650 - val_loss: 4.0016 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 373/400\n",
      "192/225 [========================>.....] - ETA: 0s - loss: 4.1284 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00373: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.12935847e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1281 - sparse_categorical_accuracy: 0.0725 - val_loss: 4.0081 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 374/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.1224 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00374: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.12426967e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1239 - sparse_categorical_accuracy: 0.0778 - val_loss: 4.0089 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 375/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1359 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00375: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.11920524e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1343 - sparse_categorical_accuracy: 0.0622 - val_loss: 4.0010 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 376/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.1227 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00376: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.11416482e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1227 - sparse_categorical_accuracy: 0.0769 - val_loss: 4.0073 - val_sparse_categorical_accuracy: 0.0889\n",
      "Epoch 377/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1152 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00377: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.10914841e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1161 - sparse_categorical_accuracy: 0.0689 - val_loss: 4.0103 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 378/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1225 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00378: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.10415565e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1194 - sparse_categorical_accuracy: 0.0697 - val_loss: 3.9991 - val_sparse_categorical_accuracy: 0.0889\n",
      "Epoch 379/400\n",
      "223/225 [============================>.] - ETA: 0s - loss: 4.1448 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00379: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.09918653e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1448 - sparse_categorical_accuracy: 0.0661 - val_loss: 3.9971 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 380/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1194 - sparse_categorical_accuracy: 0.0679   \n",
      "Epoch 00380: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.0942407e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1210 - sparse_categorical_accuracy: 0.0678 - val_loss: 4.0008 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 381/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1260 - sparse_categorical_accuracy: 0.073\n",
      "Epoch 00381: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.08931833e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1244 - sparse_categorical_accuracy: 0.0731 - val_loss: 4.0075 - val_sparse_categorical_accuracy: 0.0844\n",
      "Epoch 382/400\n",
      "212/225 [===========================>..] - ETA: 0s - loss: 4.1243 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00382: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.08441907e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1255 - sparse_categorical_accuracy: 0.0667 - val_loss: 3.9978 - val_sparse_categorical_accuracy: 0.0900\n",
      "Epoch 383/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1324 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00383: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.07954254e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1343 - sparse_categorical_accuracy: 0.0681 - val_loss: 3.9963 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 384/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1116 - sparse_categorical_accuracy: 0.0708  \n",
      "Epoch 00384: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.07468893e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1110 - sparse_categorical_accuracy: 0.0711 - val_loss: 4.0003 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 385/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.1177 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00385: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.06985769e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1181 - sparse_categorical_accuracy: 0.0669 - val_loss: 3.9970 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 386/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1333 - sparse_categorical_accuracy: 0.0684  \n",
      "Epoch 00386: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.06504919e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1339 - sparse_categorical_accuracy: 0.0689 - val_loss: 4.0037 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 387/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.0990 - sparse_categorical_accuracy: 0.08\n",
      "Epoch 00387: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.0602627e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.0967 - sparse_categorical_accuracy: 0.0825 - val_loss: 3.9982 - val_sparse_categorical_accuracy: 0.0800\n",
      "Epoch 388/400\n",
      "221/225 [============================>.] - ETA: 0s - loss: 4.1218 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00388: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.05549859e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1191 - sparse_categorical_accuracy: 0.0697 - val_loss: 3.9931 - val_sparse_categorical_accuracy: 0.0933\n",
      "Epoch 389/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1149 - sparse_categorical_accuracy: 0.0705  \n",
      "Epoch 00389: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.0507563e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1168 - sparse_categorical_accuracy: 0.0700 - val_loss: 3.9943 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 390/400\n",
      "219/225 [============================>.] - ETA: 0s - loss: 4.1225 - sparse_categorical_accuracy: 0.0716  \n",
      "Epoch 00390: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.04603584e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1158 - sparse_categorical_accuracy: 0.0725 - val_loss: 3.9944 - val_sparse_categorical_accuracy: 0.0867\n",
      "Epoch 391/400\n",
      "217/225 [===========================>..] - ETA: 0s - loss: 4.1208 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00391: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.04133703e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1249 - sparse_categorical_accuracy: 0.0728 - val_loss: 3.9968 - val_sparse_categorical_accuracy: 0.0833\n",
      "Epoch 392/400\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 4.1202 - sparse_categorical_accuracy: 0.06\n",
      "Epoch 00392: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.03665986e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1225 - sparse_categorical_accuracy: 0.0681 - val_loss: 3.9932 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 393/400\n",
      "220/225 [============================>.] - ETA: 0s - loss: 4.1110 - sparse_categorical_accuracy: 0.0719  \n",
      "Epoch 00393: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.03200416e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1091 - sparse_categorical_accuracy: 0.0714 - val_loss: 3.9998 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 394/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.1179 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00394: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.02736956e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1105 - sparse_categorical_accuracy: 0.0775 - val_loss: 3.9969 - val_sparse_categorical_accuracy: 0.0756\n",
      "Epoch 395/400\n",
      "222/225 [============================>.] - ETA: 0s - loss: 4.1075 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00395: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.02275605e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1078 - sparse_categorical_accuracy: 0.0778 - val_loss: 4.0036 - val_sparse_categorical_accuracy: 0.0856\n",
      "Epoch 396/400\n",
      "201/225 [=========================>....] - ETA: 0s - loss: 4.1194 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00396: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.01816347e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1250 - sparse_categorical_accuracy: 0.0769 - val_loss: 4.0096 - val_sparse_categorical_accuracy: 0.0811\n",
      "Epoch 397/400\n",
      "205/225 [==========================>...] - ETA: 0s - loss: 4.1299 - sparse_categorical_accuracy: 0.0716  \n",
      "Epoch 00397: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.0135918e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1237 - sparse_categorical_accuracy: 0.0733 - val_loss: 4.0010 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 398/400\n",
      "225/225 [==============================] - ETA: 0s - loss: 4.0961 - sparse_categorical_accuracy: 0.0694   \n",
      "Epoch 00398: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.00904069e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.0961 - sparse_categorical_accuracy: 0.0694 - val_loss: 4.0056 - val_sparse_categorical_accuracy: 0.0767\n",
      "Epoch 399/400\n",
      "192/225 [========================>.....] - ETA: 0s - loss: 4.1236 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00399: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2.00451013e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1346 - sparse_categorical_accuracy: 0.0739 - val_loss: 4.0069 - val_sparse_categorical_accuracy: 0.0822\n",
      "Epoch 400/400\n",
      "218/225 [============================>.] - ETA: 0s - loss: 4.1209 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00400: val_sparse_categorical_accuracy did not improve from 0.09333\n",
      "Starting Learning Rate =  0.0002\n",
      "Actual Learning Rate =  2e-05\n",
      "225/225 [==============================] - 0s 2ms/step - loss: 4.1243 - sparse_categorical_accuracy: 0.0706 - val_loss: 3.9996 - val_sparse_categorical_accuracy: 0.0822\n"
     ]
    }
   ],
   "source": [
    "history = model_predict.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs=400,\n",
    "                            batch_size=16,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=callbacks_encoder\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEeCAYAAABWldSEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAgDElEQVR4nO3da2yc53Un8P+ZK4fD+00kRd2vlmRbjgzXlqIkbtFNUaAbZ737xW26QNEPTVqg2MVmURQIsBssdoF82Q9dI4UW2wabbdMEgV3b8SZu49qObFeJL2vZli3LkkhdKF5EUiTnwiE5M2c/UC4YhjpnJIf18774/wB+IM/MmXcufPjwec9zXlFVEBFRWBKf9AEQEdEv4uBMRBQgDs5ERAHi4ExEFCAOzkREAeLgTEQUoJQVfP61kllnV6tW3QfwKvVqNb+Ur1arm/FqQ8fhPE69gZJC+zBQT9T840jYSdLppJujMj9vxueKc24OSTabcZWcm6O4MG3Gs2n7MQCgqandjIv4rym8t7Ymfgrn8yHSSA77WBX+59R7Mo1Vvto3qtedD3Ijj9DAcfzhYwf9F22Dff7hvE7PNPAZWuONtxefU9Xf2IBDapg5OBMRRdnUTA0/fW7otu+XHrjQswGHc1s4OBNRjClq+vH/U/gkcHAmothSAHVv3StQHJyJKNbq3smiQHFwJqLYUihqEe0fxMGZiGKNyxpERIFRALU4Ds71ml0fWK83UNfr/EvRyH8cXo5fxnE09v7Za1darbgZmnJ26Wc64R/IshTMeGH+ipsjlW0z48lki5ujXJ4w47nufjcHkDajlYr/miYTGfsG+vHnIA3VOXufD6cOeuU23u/Lxx9oGsvxy6i3DkNUZ87cIUhEFCAuaxBRbCnAE4JERCGKZiEdB2ciijGFxvOEIBFRpCnQQG+1IHFwJqLYWtm+HU0cnIkoxgQ1fOKdS+8IB2ciii1FY63aQ+QMzt5fnI//F6mR4v6k03++5myWARpoMN7AUxHnRnX1m6kvFu1G+R199uYQACgnb5jxanXMzVFcsDeQpFL+caRSC2Z8U0efm2OhZL8eM9Pjbo6WjgEznsraDf2BRt7+Rn7Dvc+Hv63gn2MTSiO/L95zaeDXNhicORMRBWZl+zYHZyKi4NSVgzMRUVA4cyYiCpBCUItoCyEOzkQUa1zWICIKDJc1iIiCJKg1UMIYInNwloRdGyxe7TDgFpBKA/9y1J0q8mTSf/ETzk0aunq6cxzZTM5NMTFzyYyXS5fdHMv1aSfHsJtjacl+wuWy/4IM9HSY8Y6mfW6OSxfPmvErY1fdHHvbe8x42nvzASic2t9GCnvrdkG+NDQXso9DEn4tvd8ov5GZZDya7a9s347m4BzNoyYiijkuaxBRrHHNmYgoMKoxXXMmIoq6OmfORERhWSml48yZiCgwXNYgIgpOlEvpODgTUazVYrl9W5xKcy8Ov4m5W/x/81bmYzibZQBA3Cbmfo5kwn42+VzWzTHQ323G33n3bTfH6NgZMz5fmHRz5PN5M14qVNwcu7fsNeOpRMnN8dZbL5rx5g57gwkA5LL2+7K8UHZzpLL2e6cJ54oPACD2DK2h+ZvzGWtkA4l3YYnGGuXHo9k+Gx8REQWqkSvQhIiDMxHFFqs1iIgCpJCYrjkTEUUcqzWIiAKjCtY5ExGFRyK7fTuaf1KIiBqgWJk53+5XI0SkS0SeFJGSiFwSkcducbusiPy5iEyIyIyIPCMim7385sw54RQz1hsodhTnNrWaX+fs1UJLA/XW3h/PVMLP0ZKxa13zzX4tbCadNuNDm7vcHJN2r33MzRXcHIXCghlPJv33pal5yX6MefvCAgCQS9nHcfzIXW6O6rJd153N9Lk5NGG/L1Xx6+BV7Ncs4VysoRENNbl3bqO/hCJl7/c6JBtYrfE4gCUAmwAcBvCsiJxW1bUbEf4YwEMA7gEwB+AEgD8D8K+s5Jw5ExHdJhHJA3gUwNdUtaiqLwN4GsCX1rn5DgDPqeqEqlYAfBfAQe8xODgTUWwpBHW9/a8G7AVQVdVzq352GusPuv8LwDERGRSRZgC/DeCH3gPwhCARxdodLmv0iMjrq74/oaonVn3fAmB+zX3mALSuk+tDAFcAjGLlIpHvAPgj7wA4OBNRbCnuePv2lKreb8SLANrW/KwNwHonfB4HkAXQDaAE4D9iZeb8K9YBcFmDiGJMULuDrwacA5ASkT2rfnYvgPW6kh0G8C1VnVHVRaycDHxARMyuXhyciSi2Ppo53+6Xm1e1BOAJAF8XkbyIHAPwBQDfXufmrwH4XRFpF5E0gK8AuKaqU9ZjcHAmoljboJkzsDLI5gBMAvgOgC+r6hkROS4ixVW3+w8AKlhZe74O4DcBfNFLzjVnIootVdmwlqGqOgPgkXV+fhIrJww/+n4aKxUat8UcnFMpe+z2mno3IuE0Fwf8wvuG6vKdG6UaeP+asvbzbaDXPhKwc4xePe/mKDkbSCplfzPM4uKiGW/KVd0c42NXzfhSwf98HLlnixnv67CfKwBk0vZmmGLJ35RTXLI3odTFn8fU6s5mqbqfI5m0P0TSQNP/qvsPcSMbv+KDvTWIiAKzcg3B6OxmXI2DMxHFGK++TUQUnJVqDc6ciYiCw8tUEREF5qPeGlHEwZmIYo2XqSIiCszKZapiOHNWr9qxgeesToPxeqKBJubOXz6p+QeSTi6b8awTB4By2a65rdX942jK2I/T2Z53c5w7e8WMj0/6tcFpp+l/e7v/vgyPXDfjY+k5N8eWzXYj/MqCn+P++5rMeH9vzs0xPGY/l0rJn32VFspmPFH164uHBneZ8eW6X0xfcz7KDV0kw71FdER1WSOa830iopjjsgYRxdbKCcFozkE5OBNRrN1GI6OgcHAmotjiJhQioiBxWYOIKEhsfEREFJjY1jkTEUVdLJc1Us7GjEpl7ZXB12NvZmig/ziqNbvBeKreQHP5BXuTQbKp5OZYKNubDK7P+ZsMpGZeNgzTU2NujnQyY8bLFf+51Ct2g/pE0p9tXJ+0H2fzoP9LMTdnf4a62/vdHN67v1C+5uaYmbphxucW/I0sk9OTZrwt739OK2X7F2JJet0c9WyXGfc2hgHxabbP3hpERIHimjMRUWBYSkdEFKhYrjkTEUWacs2ZiCg4vMArEVGgOHMmIgpMbE8ItuXsut5KcdR9gOVq0Yzn0u1uDk3ZNabzc3aNKgA0Zey63gsX3nJzlBcKZny5bjd9B4DBHvv56nLFzZFN2bep1/065yrspu3FBb/ZPmr2iZbuVv/16OpoMeMPHD7k5qirXT/8wflzbo6rV+3P8nTJv4CBJuxBIJf2a5Rnb9h17t2bNrs5Km7ldwP1+B/7BuGI6uAczdOYREQxx2UNIoot7hAkIgoUqzWIiEKj0V1z5uBMRLEV22oNIqKo4+BMRBQYnhAkIgqUxnFwzuiIfe/Fi+4DjI8Om/Henm1ujs0D2834dPl9N0d53t5AUl2acHNcGhkx44WS/yHoOnKvGe/vbXNzPPIvP2PGl37whpvjvYv2xQcWlu1NOwDQ3bfFjN910N9Akk7apfZTM/4Go6UlO0c262+GmZq4bMaTWf+9TeXyZry12d74AwA7tw2a8dLyopsjCfs2yYR9sQYAfrd9ic6Ax2oNIqLAKKs1iIjCFMtlDSKiaOMJQSKiIHHmTEQUGG5CISIKka6cFIwiDs5EFGuxLKUbGX7NvHOpZDfSB4Dy7LgZv1H1G393t9p1rPXqNTfHP756yox3tG9ycywW7eOYnZp1cxTm5sz4QHfazZHK2LfJ571m60C2yXscvxb28pj9XL731Itujs52+0IK4jT0B4Aj995nxhX+hQMKhWUznq3685jpq1fMeAZ+s/35TVNmfLbsX0ihVLcv6NDXs8fNkUzan6F6vermCIGCa85ERAGKbrUGr4RCRBQgzpyJKNZ4QpCIKEBRXXPmsgYRxZbqyuB8u1+NEJEuEXlSREoicklEHjNu+ykR+YmIFEVkQkT+2MvPmTMRxdoGnhB8HMASgE0ADgN4VkROq+qZ1TcSkR4APwLw7wB8HyulUENecg7ORBRrG7HmLCJ5AI8COKSqRQAvi8jTAL4E4E/W3PzfA3hOVf/q5veLANw+x1zWIKJY26Bljb0Aqqp6btXPTgM4uM5tHwQwIyKvisikiDwjIlu9BzBnzi/85P+Zd/61h+/38iOdsovZ+7rd2T3qUjbjTa3+i1lZsv9JePrpN90cLTm7uL/SwAaBjtwFM568x25gDwC9vfbGjekpu5E+ADQ7G1mWFv3NQbNz9uaOiWt+w/7+AXta05r3/7k79Yb9OZ2YsTfLAEBhwf6cppP+65HL2Bszzl94x80xNT5qxrft/VU3R+/Qp824qP+aijPbTCaicZJN0fga8ho9IvL6qu9PqOqJVd+3AJhfc585AK3r5BoC8CkAvw7gHQDfAPAdAMesA+CyBhHF2h2uakypqjX7LAJYe9miNgDrXXJpAcCTqvoaAIjIfwYwJSLtqnrLmQOXNYgovjauWuMcgJSIrN4Lfy+AM+vc9m38/N+Ihv5ecHAmonjTO/jyUqqWADwB4OsikheRYwC+AODb69z8LwF8UUQOi0gawNcAvGzNmgEOzkQUcxtV5wzgKwByACaxsob8ZVU9IyLHReSfusKp6j8A+FMAz9687W4At6yJ/gjXnIko1jZq+7aqzgB4ZJ2fn8TKCcPVP/smgG/eTn4OzkQUW2wZSkQUIgUQx8H54sUR885bh7rdB0g4/1O89PzP3By5NvvFHdzuH0dnd5MZzzuPAQDFwtqyxp830N/n5oDTB//d987ZNwDw2c983owvLWb941C7uXxnu/96JMR+Mk1J+zUHgOPHjpjx+rJ9nADw3A9fNePFRf//2nST/Zq1dvoN+zvb7AsUZNJ2fToAbNmx14z39w+6OSD2sdZ10U2RTnoXW4jOgMeudEREIYro4MxqDSKiAHHmTEQxdsfbtz9xHJyJKN4iuqzBwZmI4ktZSkdEFCbOnImIQsSZMxFReOI4c3744YfNOx/Yv9N9gEvnR8x4a+uMmyPXYm92uDziN5cvFe1G6Ec/e9jNcfbcB2Y8US+acQB48LMPmfHlRfvCAgDwg+eeN+O1mr/5o1K2H2f/rn43h8De7FCY95/LbudxZsbXa4/78zJJ+yIIpXn/MzbQ2mzGP/+5+/wc3fZmqCo2uTk6Ntm/UxdHGng9svZFDjo6/Zmkwv59UX9PTjjiODgTEUVaXLdvExFFHbdvExGFiIMzEVGAuKxBRBQe70rioeLgTETx1eA1AUPEwZmIYkziuaxx7KFj5p3zzWn3AZaK9p+tffvucnOMXDlrxssf1Nwco8PDZjzZZNeGAkB7W68Zvzxyzc3x9juXzfihA3azdQDIt3eY8aEt9nECQPN03oxPXrvi5ujbZNcX53N+MezIhffs4xj135fywoIZ7+n1X49K0c5RmPZr2Duy9lxn18EDbo7zY6NmfLHqX3zg0MF9ZjzdYr/3ADBXsBvyL1b837lgcOZMRBSgiA7ObLZPRBQgzpyJKN4iOnPm4ExE8cXt20REYWKdMxFRiCI6OPOEIBFRgDhzJqJYi+eyxoJdeF8Vuwk+AJSKJTu+kHFzTE3bOfbu6nRzdGXsBvQvvPiWm+P6vP0uO/3rAQCnfnrOjBfm/b+Xnb12Q/ZCacrNsW3Ibvw+fPG8myOdcjYiVP0XZGHOfm/ffsfelAEAy1V7w0RfV6ubozRvb5i5etXfYLTvkH3hgGvT9oYbADjz5gUzfvfeB9wc3Vn7NV1Sf/NYAfZtJOnnCAZPCBIRBYa9NYiIAsXBmYgoPPFccyYiijoOzkREAeLgTEQUFlEuaxARhSmOpXTFG1fNOyeSfv3o/v1DZnyu7P99eOWU3fh9Jm83SgeA4w/tN+MDQ592c/zv7/6jGa/V/eNYqs2b8Rvzfl3vwFb7NW1v9xvU93c5Dderds0uAOSb7SnJYL/d9B0AbsxUzfhyzf98bN+y3YwP9Po1ubt27TbjB/bZcQBIZO1a+r/+/lNujq42+70d7O92c4yO2vXUZfh7CyRvX/QhkbLr5IPCmTMRUXiiuqzB3hpERAHizJmI4i2iM2cOzkQUX6zWICIKFAdnIqIAcXAmIgpPVJc1WK1BRBQgc+Y8tN0uRM8020X3ALBcs3fnvHfxpJtjU3+vGd85dMTN8e6Zs2b8lVdOuTnGr9mbTCrLzW4OrxH++6UP3Bz1ZXuTyZbNzgYTAF3tm814tafdzZFrspvc57Jtbo5nTv3UjHd0+5th0mJvqtCac1EAALt22o/T3+e/Hs/84GUzfn204OZYdj4f//fvfujmSKSWzfjB+x50c3Tkd5hxVfviBEGJ6MyZyxpEFF+s1iAiClREB2euORNRvOkdfDVARLpE5EkRKYnIJRF5zLl9RkTeFxG7adFNnDkTUWwJNnRZ43EASwA2ATgM4FkROa2qZ25x+68CuA7A7xgHzpyJKO42YOYsInkAjwL4mqoWVfVlAE8D+NItbr8DwO8A+G+NHjZnzkQUX3d+QrBHRF5f9f0JVT2x6vu9AKqqem7Vz04D+Owt8v0ZgD8F4PcVvomDMxHF250NzlOqer8RbwGwtjn7HNZZshCRLwJIquqTIvK5Rg/AHJwzbfeYd54vldwHmJy+bsazWbu5OAB0d9h/Q777vSfdHC+dfNWMHzgw4OZ4wGnY/+OX/Brl1lZ7uenQXf7rMX75mhnf1Ok32y/N23Wq2wb73ByphF23+9JP33dznL84a8Z7y36N8nLZnow0p/y67wvD42b8oYd2uTlSaXsUOH7Ury/+4Jx9rmh8YsbNceSB+8x4b7f/XDRh16jXExGa123MmnMRwNoXqQ3Az/1S3Fz++AaA37zdB4jQK0xEdPs26ITgOQApEdmjqh/e/Nm9ANaeDNwDYDuAkyICABkA7SIyDuBBVR251QNwcCYiuk2qWhKRJwB8XUR+HyvVGl8AcHTNTd8FsGXV90cB/A8An8JK5cYtsVqDiOJtg+qcAXwFQA7AJIDvAPiyqp4RkeMiUgQAVa2q6vhHXwBmANRvfm+u2XHmTETxdXuD7e2lVp0B8Mg6Pz+JlROG693nRQD+iSVwcCaimGNvDSKiEHFwJiIKD2fOREQhiuPg/M2/+J555907D7kPsHnLoBn/8NLaTTa/6JUX3jDjwxf9zQ7/9nfshvzbdtvHCQDvD98w41299oUFAKCn275wgDawu7NUtuPLFb8x/NDgVjve1+HmKBfsjSxj10bcHPm81wPGv6BD/6D9fKXq53jv/fNmfK7ovy9DW3rM+PDwW24OZHJmuKN/k5vib1+wLxxxeCbr5jj6kH0xhrZ1T3cFaANPCG40zpyJKLbk5lcUcXAmonjjzJmIKDw8IUhEFCIOzkREAeLgTEQUGF59m4goUHEcnF/88VPmnacOj7kPsHTKbvxeKM25OT68YDfsb0qn3RyfO36rq8esGNyy3c0xv/iaHd9VdXOo2q/H1dHLbo5isWjGT/7DqJtjsNNuSHj40QNujmsL9nPp7bNrqQHgg0t27fjk+Kybo6ez24wPbM64OSYm7NtMjPuf03TWbuq/JEk3x/Xr9md9Ie3XKHf22heOmJpbdHPMzdufsa42u6Y7JJw5ExGFiIMzEVF4ojpzZrN9IqIAceZMRPHF3hpERIHi4ExEFBZBdNecOTgTUbxxcCYiCo9oNEdnc3Du6bIL8xeXR9wHmK/MmvGxUbvYHQCqdfs22/b5zeXPDl8w4xNTfmH+lj57s8P8zKSbI5mxN8zs3L3dzXGq9q4Zn5uYdnP0dPSb8eujy26OJ/72LTN+YdhvUL+waDfsz6T8+UN3l92wv6PL37hx6J5dZvzSRXtzCADkmpvtxziyz81RXqyZ8akb/u/LctXuYLy1177gAwDoov3eLS+V3BxB4AlBIqIwcc2ZiChEHJyJiMLDmTMRUYg4OBMRBYb9nImIAsXBmYgoLLHdIZhrbTHv3NvpNw8/MrTHjJ/r8Wtyf/D0m2b87NmKm2Nq6lUznm+gYf8jv/VrZnz/nh1ujg8uTJnxF1447eaYGLNzdORybo5t++xm+vWMn+PDYbvW9eLlGTdHc7tdo751yK/JnRizH6cwV3BzdHfbNeylsl/Xm8o2mfHRa34d/Oys/fvQ0uq/L0P9dg37QK9dSw0A7W32hSPmCuNuDuCeBm7zzyCOm1CIiKIuljNnIqJIi/AOQTbbJyIKEGfORBRrYrdvCRYHZyKKt4gua3BwJqJY4wlBIqLQKFhKR0QUoljOnI8ePWze+eiv7HQfYPSSvUHgr//qW26OyyN2g/HmJnuzDACUF+zC+/277YbtADB+zd78kfR7uuOp758045dG59wcLe328+1osTdDAEBV7b/LiYyfQxP2xRiasnYcALo77Rdt7saEm+PqVbsRfruz0QUAygX79Vgo+RcfSGZmzXjbov967Nw6aMa3b/U35VQW7M+QLF11c4xftjd2bd4eyAaTRsRxcCYiirLYbt8mIoo0Va45ExGFiDNnIqIQcXAmIgoPZ85ERKFRAPVojs4cnIko3qI5NtuD890H9pp3npj0a3L/+58/Y8ZPvTHq5shm7PrQTM5/9Qecpu0H773bzXFu2G6W/uJLdkN/ABifmDfjfQMDbo5y2a77nhwbc3P85Hn7WP/1b/0LN8eurXaD+srCDTdHd2enGX/9Tbu2HAAWlpwa9eKimyNVs4+1o9N+rgDQ1mU3eWzO+E0gN3XZr8e2gS1ujkLRrlGfmvEb5e/ZO2TGd23xm/6HIqrLGmwZSkTx9lE53e18NUBEukTkSREpicglEXnsFrf7qoi8KyIFERkWka82kp/LGkQUaxs4c34cwBKATQAOA3hWRE6r6pm1hwDgdwG8DWAXgL8TkSuq+jdWcs6ciYhuk4jkATwK4GuqWlTVlwE8DeBLa2+rqt9Q1TdVtaqqHwB4CsAx7zE4OBNRfOkdfvn2Aqiq6rlVPzsN4KB1JxERAMcBrJ1d/wIuaxBRbK301rijdY0eEXl91fcnVPXEqu9bAKw9uz8HwOug9p+wMin+S+8AODgTUbzd2WWqplT1fiNeBNC25mdtAAq3uoOI/BFW1p6Pq6pbQsTBmYhi7Q5nzp5zAFIiskdVP7z5s3txi+UKEfk9AH8C4DOq6vdsBdeciSjONmjNWVVLAJ4A8HURyYvIMQBfAPDttbcVkd8G8F8B/LqqXmz00M2Z85VrZfPO01NV9wEGttsbWVreOuvm2LPVLojPZP1Xc/fOPjP+/tlzZhwAXj913ozPzvkN2Zva7CWp4tIt/yv6J+WivWEi2+J3/S8U7M0wzXn/7/axI7vNeEqW3BzvXrQ3mZQqaTfH0qL9fHfv6HBz7N/TY8ZHnd8FABifst+XTM6/oENVm83437/wMzdHa4e9CSXV6r+m28X5r7vmXwQhDBvaMvQrAP4CwCSAaQBfVtUzInIcwA9V9aOrYvwXAN0AXls5HwgA+D+q+gdWci5rEFGsbVSds6rOAHhknZ+fxMoJw4++33En+Tk4E1G8sdk+EVFgFJA7q9b4xHFwJqJ448yZiChA0RybOTgTUbxtUJ3zhuPgTETxFsfBeW7W3siidb/hdnPWrqm8+9A2N8eOoXb7OODXW09cs5/LzLRfx9rWateP1mp+/ehyXcx4OunXF89X7Q/bYiXp5liq2H+Xc7kWMw4Ae/bvMuOFxYqb48qUXeecSdr12ACwULPP+LQ023XyAFAp2zXqk1P2BQ4AYLpkP9/ieb/J/VLNfl+GBv2m/+++Z9fsFyolN0c+kzfjfa0RabavuNPt2584zpyJKLYEymUNIqIgRXRwZm8NIqIAceZMRPEW0ZkzB2ciii+eECQiChNPCBIRhYiDMxFRaDa0n/OGMgfnVHLSvHO6yd6UAQDvvP4TM75jp19U//Cv3mXGi0W/yf3PXrWb+nfku9wcN1pmzfj0hN+wv7JgbxBp7u11c3R22scqFXujCwBUSvZCXHOT3xi+c4/93knWbhwPAFeujZjx9ry/waiwaDeGX67W3ByzM/brseDvp0F5wb64gCLj5hgZuWbGCwV/s1Rz3v4M7RvY7+ZoyW0x48OX/QspBEERz8GZiCjyeEKQiCg8PCFIRBQiDs5ERIFRAHUOzkREgYlptQYRUeRxcCYiClAcB+cPzp8377x181b3Ae7Zb9dc3nXAz7Fnu90s/cL5UTdHa7NdT7NY8ettWrb3mPFUzq+n1YTdxHxs/IabI9dk1w/PXPUb1G/qs48jm/YbFt6YtuvgX3v1NTdHYtF+3e/b59efP3j0gBn/n9866eaYStrzlAPt/kUhxsY6zXhTk19/nk3an6Ed++92c3QO2sfakvfrrXfv3mfGte7vLQgC15yJiEKkgEaz0JmDMxHFW0SXNdhsn4goQJw5E1F8cc2ZiChQEV3W4OBMRPHGwZmIKDTcIUhEFB4FUI9hKV1fr13Mns7YjeMBYOduexNKS77FzXH+3JgZP/v+JTfH9fE5M97UnHZzdLbZxS07dvgXDqhq1o5X/Wbq3gmOxWb/fZm5YW92ef3N990ce/fbm4OWlv2G7JK0N0RUlv1Zz1xhwYwv1/zjSCTspv7tHf6FJdry7WY80+R/xkTt4zh0YKebY9dde8x4toGLZIxfuWzfoDrr5ggGZ85ERAHi4ExEFBplKR0RUXAUUG7fJiIKEGfOREQB4pozEVFgVONZSkdEFHlxnDl/+thnzDuPj3/YwAPYL0xHh18bfO3qlBlPp/zm4d099uOk0/7fqS39m824pPwmf6+cOm3GS7OLbo72FrvZ/n332s3nAWDi6rgZ/9HfP+/mGNz8b8z4wYN3uTl+/PyEGf/wPTsOAJK13//yov/etnXnzHg2O+vmmLxRMuMXhv33NufUSg8Mtbk59u6266lV/TpnWZ424zem7c9PSJQzZyKi0HD7NhFReNgylIgoUBGtc+aVUIiIAsSZMxHFlgJQLmsQEQVGefVtIqIgceZMRBSiiM6cRSNaA0hE5BGRHwHouYO7Tqnqb/yyj+d2cHAmIgoQS+mIiALEwZmIKEAcnImIAsTBmYgoQByciYgC9P8B0Ve0pS8bqFkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of values: min: 0.00784313725490196 - max: 0.9568627450980393\n",
      "Image type: float64\n",
      "Image shape: (32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "display.display_image_prop(X_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0951 - sparse_categorical_accuracy: 0.080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.095147609710693, 0.07999999821186066]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_monitor = 'val_sparse_categorical_accuracy'\n",
    "mode =\"max\"\n",
    "### Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "# configure early stopping\n",
    "early_stopping = EarlyStopping(monitor=val_monitor, patience=25)\n",
    "###\n",
    "\n",
    "### Model saving\n",
    "model_saving_predict = keras.callbacks.ModelCheckpoint(\n",
    "    # Path where to save the model\n",
    "    # The two parameters below mean that we will overwrite\n",
    "    # the current checkpoint if and only if\n",
    "    # the `val_loss` score has improved.\n",
    "    # The saved model name will include the current epoch.\n",
    "    filepath=ckpt_dir_predict_retrained + \"/val_acc={val_sparse_categorical_accuracy:.4f}_acc={sparse_categorical_accuracy:.4f}_ckpt={epoch}\",\n",
    "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "    monitor=val_monitor,\n",
    "    save_weights_only=True,\n",
    "    mode=mode,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "### Callback class\n",
    "class MyCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        starting_lr = self.model.optimizer.lr\n",
    "        actual_lr = self.model.optimizer._decayed_lr(tf.float32)\n",
    "        tf.print(\"Starting Learning Rate = \", starting_lr)\n",
    "        tf.print(\"Actual Learning Rate = \", actual_lr)\n",
    "\n",
    "\n",
    "my_callback = MyCallback()\n",
    "\n",
    "###\n",
    "\n",
    "### init callbacks\n",
    "callbacks_encoder = [\n",
    "    model_saving_predict,\n",
    "    early_stopping,\n",
    "    my_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Transfer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./ckpt/pred_ret\\val_acc=0.1722_acc=0.1481_ckpt=133\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0._outbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1._inbound_nodes\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.momentum\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.layer_with_weights-1.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.layer_with_weights-1.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.layer_with_weights-2.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "model_predict_retrained = make_or_restore_model_weights_only_uncompiled(get_uncompiled_model_predict, ckpt_dir_predict_retrained)\n",
    "# model_predict_retrained = make_or_restore_model_weights_only_uncompiled(get_uncompiled_model_predict, ckpt_dir_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_predict_retrained = model_predict\n",
    "model_predict_retrained.get_layer(\"encoder\").trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "nadam = tf.keras.optimizers.Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999)  #0.0005\n",
    "sgd = keras.optimizers.SGD(lr=0.00002, decay=(5 * 1e-5), momentum=0.95)\n",
    "#Loss\n",
    "# cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "scc=keras.losses.SparseCategoricalCrossentropy(),\n",
    "\n",
    "\n",
    "#Metric\n",
    "acc = 'accuracy'\n",
    "cat_acc = tf.keras.metrics.categorical_accuracy\n",
    "sca=keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model_predict_retrained.compile(loss=scc,\n",
    "                      optimizer=sgd,\n",
    "                      metrics=sca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "213/225 [===========================>..] - ETA: 0s - loss: 3.9397 - sparse_categorical_accuracy: 0.099\n",
      "Epoch 00001: val_sparse_categorical_accuracy did not improve from 0.10222\n",
      "Starting Learning Rate =  2e-05\n",
      "Actual Learning Rate =  1.49532707e-05\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 3.9292 - sparse_categorical_accuracy: 0.1014 - val_loss: 3.8256 - val_sparse_categorical_accuracy: 0.0911\n",
      "Epoch 2/300\n",
      "215/225 [===========================>..] - ETA: 0s - loss: 3.9310 - sparse_categorical_accuracy: 0.0951   \n",
      "Epoch 00002: val_sparse_categorical_accuracy did not improve from 0.10222\n",
      "Starting Learning Rate =  2e-05\n",
      "Actual Learning Rate =  1.48285444e-05\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 3.9310 - sparse_categorical_accuracy: 0.0944 - val_loss: 3.8338 - val_sparse_categorical_accuracy: 0.0878\n",
      "Epoch 3/300\n",
      "216/225 [===========================>..] - ETA: 0s - loss: 3.9167 - sparse_categorical_accuracy: 0.097\n",
      "Epoch 00003: val_sparse_categorical_accuracy did not improve from 0.10222\n",
      "Starting Learning Rate =  2e-05\n",
      "Actual Learning Rate =  1.47058818e-05\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 3.9160 - sparse_categorical_accuracy: 0.0964 - val_loss: 3.8303 - val_sparse_categorical_accuracy: 0.0922\n",
      "Epoch 4/300\n",
      "214/225 [===========================>..] - ETA: 0s - loss: 3.9471 - sparse_categorical_accuracy: 0.097\n",
      "Epoch 00004: val_sparse_categorical_accuracy did not improve from 0.10222\n",
      "Starting Learning Rate =  2e-05\n",
      "Actual Learning Rate =  1.45852318e-05\n",
      "225/225 [==============================] - 1s 3ms/step - loss: 3.9484 - sparse_categorical_accuracy: 0.0975 - val_loss: 3.8253 - val_sparse_categorical_accuracy: 0.0911\n",
      "Epoch 5/300\n",
      "194/225 [========================>.....] - ETA: 0s - loss: 3.9243 - sparse_categorical_accuracy: 0.09"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_12108/4227792851.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m                             \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m16\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m                             \u001B[0mvalidation_split\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0.2\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m                             \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks_encoder\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m                             )\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m     \u001B[1;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1101\u001B[0m               \u001B[0mlogs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtmp_logs\u001B[0m  \u001B[1;31m# No error, now safe to assign to logs.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1102\u001B[0m               \u001B[0mend_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep_increment\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1103\u001B[1;33m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mend_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1104\u001B[0m         \u001B[0mepoch_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1105\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001B[0m in \u001B[0;36mon_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    438\u001B[0m     \"\"\"\n\u001B[0;32m    439\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_should_call_train_batch_hooks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 440\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mModeKeys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTRAIN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'end'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    441\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    442\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mon_test_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001B[0m in \u001B[0;36m_call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    287\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_begin_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    288\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'end'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 289\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_end_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    290\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    291\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Unrecognized hook: {}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001B[0m in \u001B[0;36m_call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    307\u001B[0m       \u001B[0mbatch_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_batch_start_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 309\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_hook_helper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhook_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    310\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    311\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_timing\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\callbacks.py\u001B[0m in \u001B[0;36m_call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    343\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    344\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mnumpy_logs\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# Only convert once.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 345\u001B[1;33m           \u001B[0mnumpy_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_numpy_or_python_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    346\u001B[0m         \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnumpy_logs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    347\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001B[0m in \u001B[0;36mto_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    535\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m  \u001B[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 537\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_to_single_numpy_or_python_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    538\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    539\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001B[0m in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    633\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    634\u001B[0m   return pack_sequence_as(\n\u001B[1;32m--> 635\u001B[1;33m       \u001B[0mstructure\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    636\u001B[0m       expand_composites=expand_composites)\n\u001B[0;32m    637\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    633\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    634\u001B[0m   return pack_sequence_as(\n\u001B[1;32m--> 635\u001B[1;33m       \u001B[0mstructure\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    636\u001B[0m       expand_composites=expand_composites)\n\u001B[0;32m    637\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001B[0m in \u001B[0;36m_to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    531\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_to_single_numpy_or_python_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    532\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 533\u001B[1;33m       \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    534\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    535\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m  \u001B[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mnumpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1061\u001B[0m     \"\"\"\n\u001B[0;32m   1062\u001B[0m     \u001B[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1063\u001B[1;33m     \u001B[0mmaybe_arr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1064\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mmaybe_arr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmaybe_arr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mmaybe_arr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1065\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1027\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1028\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1029\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_numpy_internal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1030\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1031\u001B[0m       \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "history = model_predict_retrained.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs=300,\n",
    "                            batch_size=16,\n",
    "                            validation_split=0.2,\n",
    "                            callbacks=callbacks_encoder\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7918 - sparse_categorical_accuracy: 0.154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.791811943054199, 0.15399999916553497]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict_retrained.evaluate(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Starting Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_uncompiled_model_predict():\n",
    "    stacked_encoder = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=[32, 32, 3]),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-1\"),\n",
    "        tf.keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-2\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-3\"),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-4\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-5\"),\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-6\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-7\"),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", activation=\"selu\", kernel_initializer=\"he_normal\",\n",
    "                               name=\"base-8\"),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Flatten(name=\"flatten1\")\n",
    "    ])\n",
    "    # stack_predict = keras.models.Sequential([\n",
    "    #     tf.keras.layers.Input(shape=[1024]),\n",
    "    #     # tf.keras.layers.Dense(1024, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "    #     tf.keras.layers.Dropout(0.4),\n",
    "    #     tf.keras.layers.Dense(256, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "    #     tf.keras.layers.Dropout(0.4),\n",
    "    #     tf.keras.layers.Dense(512, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "    #     tf.keras.layers.Dropout(0.4),\n",
    "    #     tf.keras.layers.Dense(n_classes, activation=\"softmax\", name=\"output\")\n",
    "    # ])\n",
    "    stack_predict = keras.models.Sequential([\n",
    "        tf.keras.layers.Input(shape=[128]),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(512, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(256, activation=\"selu\", kernel_initializer=\"he_normal\"),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(n_classes, activation=\"softmax\", name=\"output\")\n",
    "    ])\n",
    "    stacked_model = tf.keras.models.Sequential([stacked_encoder, stack_predict])\n",
    "    return stacked_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aug_types=[0,1,2,3]\n",
    "# Prepare data augment\n",
    "X_full = []\n",
    "y_full = []\n",
    "for type in aug_types:\n",
    "    X_full.extend([(path, type) for path in X_train_full])\n",
    "    y_full.extend([label for label in y_train_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_full2, X_test_full2, y_train_full2, y_test_full2 = train_test_split(X_full, y_full, random_state=42,\n",
    "                                                                        test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for fitting - one batch at a time'\n",
    "\n",
    "    def __init__(self, X_dataset, labels, batch_size=32, dim=(158, 158),\n",
    "                 transforms=None, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.X_dataset = copy.deepcopy(X_dataset)\n",
    "        self.labels = copy.deepcopy(labels)\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.transforms = transforms\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        batches_per_epoch = int(np.floor(len(self.X_dataset) / self.batch_size))\n",
    "        return batches_per_epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "        return X, y\n",
    "\n",
    "    def __call__(self):\n",
    "        for i in range(self.__len__()):\n",
    "            yield self.__getitem__(i)\n",
    "            if i == self.__len__() - 1:\n",
    "                self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.X_dataset))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        # Find list of paths\n",
    "        X_dataset_temp = [self.X_dataset[k] for k in indexes]\n",
    "        # Find labels\n",
    "        y = [self.labels[k] for k in indexes]\n",
    "        'Generates data containing batch_size samples'  # X : (n_samples, *dim)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, img_prop in enumerate(X_dataset_temp):\n",
    "            img = img_prop[0]\n",
    "            aug_type = img_prop[1]\n",
    "            # Store sample\n",
    "            X[i,] = self.__preprocess_image(img, aug_type)\n",
    "        return X, np.array(y)\n",
    "\n",
    "    def __preprocess_image(self, path, aug_type):\n",
    "        'Preprocess the images'\n",
    "        img = self.transforms(path, aug_type)\n",
    "        return img\n",
    "\n",
    "def preprocess_img(img, aug_type):\n",
    "    img = augment(img, aug_type)  # augment the images\n",
    "    return img\n",
    "\n",
    "def augment(img, aug_type):\n",
    "    if aug_type == 0:\n",
    "        return img\n",
    "    elif aug_type == 1:\n",
    "        return tf.image.random_flip_left_right(img)\n",
    "    elif aug_type == 2:\n",
    "        return tf.image.flip_up_down(img)\n",
    "    elif aug_type == 3:\n",
    "        return tf.image.rot90(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_predict_retrained = get_uncompiled_model_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_monitor = 'val_sparse_categorical_accuracy'\n",
    "mode =\"max\"\n",
    "### Early Stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
    "\n",
    "# configure early stopping\n",
    "early_stopping = EarlyStopping(monitor=val_monitor, patience=25)\n",
    "###\n",
    "\n",
    "### Model saving\n",
    "model_saving_predict = keras.callbacks.ModelCheckpoint(\n",
    "    # Path where to save the model\n",
    "    # The two parameters below mean that we will overwrite\n",
    "    # the current checkpoint if and only if\n",
    "    # the `val_loss` score has improved.\n",
    "    # The saved model name will include the current epoch.\n",
    "    filepath=ckpt_dir_predict_retrained + \"/val_acc={val_sparse_categorical_accuracy:.4f}_acc={sparse_categorical_accuracy:.4f}_ckpt={epoch}\",\n",
    "    save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "    monitor=val_monitor,\n",
    "    save_weights_only=True,\n",
    "    mode=mode,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "### Callback class\n",
    "class MyCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        starting_lr = self.model.optimizer.lr\n",
    "        actual_lr = self.model.optimizer._decayed_lr(tf.float32)\n",
    "        tf.print(\"Starting Learning Rate = \", starting_lr)\n",
    "        tf.print(\"Actual Learning Rate = \", actual_lr)\n",
    "\n",
    "\n",
    "my_callback = MyCallback()\n",
    "\n",
    "###\n",
    "\n",
    "### init callbacks\n",
    "callbacks_clean = [\n",
    "    model_saving_predict,\n",
    "    early_stopping,\n",
    "    my_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "nadam = tf.keras.optimizers.Nadam(lr=0.0002, beta_1=0.9, beta_2=0.999)  #0.0005\n",
    "sgd = keras.optimizers.SGD(lr=0.00002, decay=(5 * 1e-5), momentum=0.95)\n",
    "#Loss\n",
    "# cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "scc=keras.losses.SparseCategoricalCrossentropy(),\n",
    "\n",
    "\n",
    "#Metric\n",
    "acc = 'accuracy'\n",
    "cat_acc = tf.keras.metrics.categorical_accuracy\n",
    "sca=keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "model_predict_retrained.compile(loss=scc,\n",
    "                      optimizer=nadam,\n",
    "                      metrics=sca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 5.2137 - sparse_categorical_accuracy: 0.0363   \n",
      "Epoch 00001: val_sparse_categorical_accuracy improved from -inf to 0.09415, saving model to ./ckpt/pred_ret\\val_acc=0.0942_acc=0.0362_ckpt=1\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  5.28700875e-05\n",
      "468/468 [==============================] - 10s 22ms/step - loss: 5.2130 - sparse_categorical_accuracy: 0.0362 - val_loss: 3.9274 - val_sparse_categorical_accuracy: 0.0942\n",
      "Epoch 2/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 4.8476 - sparse_categorical_accuracy: 0.04\n",
      "Epoch 00002: val_sparse_categorical_accuracy improved from 0.09415 to 0.10437, saving model to ./ckpt/pred_ret\\val_acc=0.1044_acc=0.0459_ckpt=2\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.12152585e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 4.8476 - sparse_categorical_accuracy: 0.0459 - val_loss: 3.8088 - val_sparse_categorical_accuracy: 0.1044\n",
      "Epoch 3/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 4.5698 - sparse_categorical_accuracy: 0.05\n",
      "Epoch 00003: val_sparse_categorical_accuracy improved from 0.10437 to 0.12119, saving model to ./ckpt/pred_ret\\val_acc=0.1212_acc=0.0577_ckpt=3\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  3.37707424e-05\n",
      "468/468 [==============================] - 10s 22ms/step - loss: 4.5698 - sparse_categorical_accuracy: 0.0577 - val_loss: 3.6905 - val_sparse_categorical_accuracy: 0.1212\n",
      "Epoch 4/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 4.3531 - sparse_categorical_accuracy: 0.069\n",
      "Epoch 00004: val_sparse_categorical_accuracy improved from 0.12119 to 0.13782, saving model to ./ckpt/pred_ret\\val_acc=0.1378_acc=0.0696_ckpt=4\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  2.86041177e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 4.3531 - sparse_categorical_accuracy: 0.0696 - val_loss: 3.6208 - val_sparse_categorical_accuracy: 0.1378\n",
      "Epoch 5/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 4.1820 - sparse_categorical_accuracy: 0.07\n",
      "Epoch 00005: val_sparse_categorical_accuracy improved from 0.13782 to 0.14683, saving model to ./ckpt/pred_ret\\val_acc=0.1468_acc=0.0795_ckpt=5\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  2.48086162e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 4.1820 - sparse_categorical_accuracy: 0.0795 - val_loss: 3.5206 - val_sparse_categorical_accuracy: 0.1468\n",
      "Epoch 6/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 4.0228 - sparse_categorical_accuracy: 0.09\n",
      "Epoch 00006: val_sparse_categorical_accuracy improved from 0.14683 to 0.15785, saving model to ./ckpt/pred_ret\\val_acc=0.1579_acc=0.0931_ckpt=6\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  2.19023768e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 4.0229 - sparse_categorical_accuracy: 0.0931 - val_loss: 3.4723 - val_sparse_categorical_accuracy: 0.1579\n",
      "Epoch 7/100\n",
      "465/468 [============================>.] - ETA: 0s - loss: 3.8861 - sparse_categorical_accuracy: 0.1054  \n",
      "Epoch 00007: val_sparse_categorical_accuracy improved from 0.15785 to 0.17508, saving model to ./ckpt/pred_ret\\val_acc=0.1751_acc=0.1055_ckpt=7\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.96056444e-05\n",
      "468/468 [==============================] - 10s 22ms/step - loss: 3.8851 - sparse_categorical_accuracy: 0.1055 - val_loss: 3.3431 - val_sparse_categorical_accuracy: 0.1751\n",
      "Epoch 8/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 3.7600 - sparse_categorical_accuracy: 0.121\n",
      "Epoch 00008: val_sparse_categorical_accuracy improved from 0.17508 to 0.19511, saving model to ./ckpt/pred_ret\\val_acc=0.1951_acc=0.1211_ckpt=8\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.77448783e-05\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 3.7600 - sparse_categorical_accuracy: 0.1211 - val_loss: 3.2824 - val_sparse_categorical_accuracy: 0.1951\n",
      "Epoch 9/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 3.6182 - sparse_categorical_accuracy: 0.140\n",
      "Epoch 00009: val_sparse_categorical_accuracy improved from 0.19511 to 0.20693, saving model to ./ckpt/pred_ret\\val_acc=0.2069_acc=0.1406_ckpt=9\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.62067045e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 3.6182 - sparse_categorical_accuracy: 0.1406 - val_loss: 3.1928 - val_sparse_categorical_accuracy: 0.2069\n",
      "Epoch 10/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 3.5105 - sparse_categorical_accuracy: 0.153\n",
      "Epoch 00010: val_sparse_categorical_accuracy improved from 0.20693 to 0.20994, saving model to ./ckpt/pred_ret\\val_acc=0.2099_acc=0.1534_ckpt=10\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.49139241e-05\n",
      "468/468 [==============================] - 12s 27ms/step - loss: 3.5112 - sparse_categorical_accuracy: 0.1534 - val_loss: 3.1580 - val_sparse_categorical_accuracy: 0.2099\n",
      "Epoch 11/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 3.3901 - sparse_categorical_accuracy: 0.17\n",
      "Epoch 00011: val_sparse_categorical_accuracy improved from 0.20994 to 0.22676, saving model to ./ckpt/pred_ret\\val_acc=0.2268_acc=0.1715_ckpt=11\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.38121532e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 3.3891 - sparse_categorical_accuracy: 0.1715 - val_loss: 3.0663 - val_sparse_categorical_accuracy: 0.2268\n",
      "Epoch 12/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 3.2991 - sparse_categorical_accuracy: 0.187\n",
      "Epoch 00012: val_sparse_categorical_accuracy improved from 0.22676 to 0.24038, saving model to ./ckpt/pred_ret\\val_acc=0.2404_acc=0.1871_ckpt=12\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.28619713e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 3.2991 - sparse_categorical_accuracy: 0.1871 - val_loss: 2.9984 - val_sparse_categorical_accuracy: 0.2404\n",
      "Epoch 13/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 3.2153 - sparse_categorical_accuracy: 0.198\n",
      "Epoch 00013: val_sparse_categorical_accuracy improved from 0.24038 to 0.25541, saving model to ./ckpt/pred_ret\\val_acc=0.2554_acc=0.1982_ckpt=13\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.20341074e-05\n",
      "468/468 [==============================] - 12s 25ms/step - loss: 3.2161 - sparse_categorical_accuracy: 0.1982 - val_loss: 2.9668 - val_sparse_categorical_accuracy: 0.2554\n",
      "Epoch 14/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 3.1309 - sparse_categorical_accuracy: 0.209\n",
      "Epoch 00014: val_sparse_categorical_accuracy did not improve from 0.25541\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.13063697e-05\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 3.1309 - sparse_categorical_accuracy: 0.2094 - val_loss: 2.9430 - val_sparse_categorical_accuracy: 0.2482\n",
      "Epoch 15/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 3.0654 - sparse_categorical_accuracy: 0.225\n",
      "Epoch 00015: val_sparse_categorical_accuracy improved from 0.25541 to 0.26723, saving model to ./ckpt/pred_ret\\val_acc=0.2672_acc=0.2256_ckpt=15\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.06616289e-05\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 3.0666 - sparse_categorical_accuracy: 0.2256 - val_loss: 2.9059 - val_sparse_categorical_accuracy: 0.2672\n",
      "Epoch 16/100\n",
      "465/468 [============================>.] - ETA: 0s - loss: 2.9919 - sparse_categorical_accuracy: 0.23\n",
      "Epoch 00016: val_sparse_categorical_accuracy did not improve from 0.26723\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  1.00864545e-05\n",
      "468/468 [==============================] - 11s 22ms/step - loss: 2.9920 - sparse_categorical_accuracy: 0.2354 - val_loss: 2.8890 - val_sparse_categorical_accuracy: 0.2632\n",
      "Epoch 17/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 2.9117 - sparse_categorical_accuracy: 0.24\n",
      "Epoch 00017: val_sparse_categorical_accuracy improved from 0.26723 to 0.27744, saving model to ./ckpt/pred_ret\\val_acc=0.2774_acc=0.2485_ckpt=17\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  9.57016164e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 2.9121 - sparse_categorical_accuracy: 0.2485 - val_loss: 2.8241 - val_sparse_categorical_accuracy: 0.2774\n",
      "Epoch 18/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 2.8533 - sparse_categorical_accuracy: 0.263 - ETA: 8s - loss: 2.8144 - sparse_categorical_accuracy: 0.264\n",
      "Epoch 00018: val_sparse_categorical_accuracy improved from 0.27744 to 0.28425, saving model to ./ckpt/pred_ret\\val_acc=0.2843_acc=0.2638_ckpt=18\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  9.1041511e-06\n",
      "468/468 [==============================] - 12s 25ms/step - loss: 2.8538 - sparse_categorical_accuracy: 0.2638 - val_loss: 2.8031 - val_sparse_categorical_accuracy: 0.2843\n",
      "Epoch 19/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 2.7974 - sparse_categorical_accuracy: 0.27 - ETA: 7s - loss: 2.7655 - sparse_categorical_accuracy: 0.275\n",
      "Epoch 00019: val_sparse_categorical_accuracy did not improve from 0.28425\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  8.68141615e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 2.7974 - sparse_categorical_accuracy: 0.2728 - val_loss: 2.7985 - val_sparse_categorical_accuracy: 0.2800\n",
      "Epoch 20/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 2.7250 - sparse_categorical_accuracy: 0.281\n",
      "Epoch 00020: val_sparse_categorical_accuracy did not improve from 0.28425\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  8.29619694e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 2.7250 - sparse_categorical_accuracy: 0.2817 - val_loss: 2.7741 - val_sparse_categorical_accuracy: 0.2812\n",
      "Epoch 21/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 2.6666 - sparse_categorical_accuracy: 0.299\n",
      "Epoch 00021: val_sparse_categorical_accuracy improved from 0.28425 to 0.28606, saving model to ./ckpt/pred_ret\\val_acc=0.2861_acc=0.2999_ckpt=21\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  7.94371226e-06\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 2.6656 - sparse_categorical_accuracy: 0.2999 - val_loss: 2.7971 - val_sparse_categorical_accuracy: 0.2861\n",
      "Epoch 22/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 2.5915 - sparse_categorical_accuracy: 0.31\n",
      "Epoch 00022: val_sparse_categorical_accuracy improved from 0.28606 to 0.29647, saving model to ./ckpt/pred_ret\\val_acc=0.2965_acc=0.3133_ckpt=22\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  7.61995898e-06\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 2.5930 - sparse_categorical_accuracy: 0.3133 - val_loss: 2.7700 - val_sparse_categorical_accuracy: 0.2965\n",
      "Epoch 23/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.5604 - sparse_categorical_accuracy: 0.31\n",
      "Epoch 00023: val_sparse_categorical_accuracy improved from 0.29647 to 0.30008, saving model to ./ckpt/pred_ret\\val_acc=0.3001_acc=0.3164_ckpt=23\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  7.32156286e-06\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 2.5596 - sparse_categorical_accuracy: 0.3164 - val_loss: 2.7589 - val_sparse_categorical_accuracy: 0.3001\n",
      "Epoch 24/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 2.4990 - sparse_categorical_accuracy: 0.331\n",
      "Epoch 00024: val_sparse_categorical_accuracy did not improve from 0.30008\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  7.04565537e-06\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 2.4990 - sparse_categorical_accuracy: 0.3317 - val_loss: 2.7737 - val_sparse_categorical_accuracy: 0.2961\n",
      "Epoch 25/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 2.4468 - sparse_categorical_accuracy: 0.34\n",
      "Epoch 00025: val_sparse_categorical_accuracy did not improve from 0.30008\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  6.78978722e-06\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 2.4479 - sparse_categorical_accuracy: 0.3417 - val_loss: 2.7889 - val_sparse_categorical_accuracy: 0.2903\n",
      "Epoch 26/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 2.4121 - sparse_categorical_accuracy: 0.35\n",
      "Epoch 00026: val_sparse_categorical_accuracy improved from 0.30008 to 0.31671, saving model to ./ckpt/pred_ret\\val_acc=0.3167_acc=0.3526_ckpt=26\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  6.55185295e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 2.4123 - sparse_categorical_accuracy: 0.3526 - val_loss: 2.6920 - val_sparse_categorical_accuracy: 0.3167\n",
      "Epoch 27/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.3599 - sparse_categorical_accuracy: 0.361\n",
      "Epoch 00027: val_sparse_categorical_accuracy did not improve from 0.31671\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  6.33002901e-06\n",
      "468/468 [==============================] - 12s 25ms/step - loss: 2.3597 - sparse_categorical_accuracy: 0.3619 - val_loss: 2.7127 - val_sparse_categorical_accuracy: 0.3137\n",
      "Epoch 28/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.3097 - sparse_categorical_accuracy: 0.371 - ETA: 5s - loss: 2.2759 - sparse_categorical_accuracy: 0.37\n",
      "Epoch 00028: val_sparse_categorical_accuracy did not improve from 0.31671\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  6.12273425e-06\n",
      "468/468 [==============================] - 10s 22ms/step - loss: 2.3102 - sparse_categorical_accuracy: 0.3711 - val_loss: 2.7685 - val_sparse_categorical_accuracy: 0.3155\n",
      "Epoch 29/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.2832 - sparse_categorical_accuracy: 0.3687\n",
      "Epoch 00029: val_sparse_categorical_accuracy improved from 0.31671 to 0.31771, saving model to ./ckpt/pred_ret\\val_acc=0.3177_acc=0.3687_ckpt=29\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  5.92858532e-06\n",
      "468/468 [==============================] - 12s 25ms/step - loss: 2.2830 - sparse_categorical_accuracy: 0.3687 - val_loss: 2.7539 - val_sparse_categorical_accuracy: 0.3177\n",
      "Epoch 30/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.2204 - sparse_categorical_accuracy: 0.38\n",
      "Epoch 00030: val_sparse_categorical_accuracy did not improve from 0.31771\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  5.74637124e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 2.2206 - sparse_categorical_accuracy: 0.3884 - val_loss: 2.7386 - val_sparse_categorical_accuracy: 0.3151\n",
      "Epoch 31/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 2.1785 - sparse_categorical_accuracy: 0.400\n",
      "Epoch 00031: val_sparse_categorical_accuracy improved from 0.31771 to 0.32091, saving model to ./ckpt/pred_ret\\val_acc=0.3209_acc=0.4000_ckpt=31\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  5.57502335e-06\n",
      "468/468 [==============================] - 12s 25ms/step - loss: 2.1785 - sparse_categorical_accuracy: 0.4000 - val_loss: 2.7349 - val_sparse_categorical_accuracy: 0.3209\n",
      "Epoch 32/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.1489 - sparse_categorical_accuracy: 0.406\n",
      "Epoch 00032: val_sparse_categorical_accuracy improved from 0.32091 to 0.32372, saving model to ./ckpt/pred_ret\\val_acc=0.3237_acc=0.4067_ckpt=32\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  5.41359805e-06\n",
      "468/468 [==============================] - 12s 26ms/step - loss: 2.1485 - sparse_categorical_accuracy: 0.4067 - val_loss: 2.7208 - val_sparse_categorical_accuracy: 0.3237\n",
      "Epoch 33/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 2.0895 - sparse_categorical_accuracy: 0.41\n",
      "Epoch 00033: val_sparse_categorical_accuracy improved from 0.32372 to 0.32612, saving model to ./ckpt/pred_ret\\val_acc=0.3261_acc=0.4165_ckpt=33\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  5.26125859e-06\n",
      "468/468 [==============================] - 12s 25ms/step - loss: 2.0895 - sparse_categorical_accuracy: 0.4165 - val_loss: 2.7577 - val_sparse_categorical_accuracy: 0.3261\n",
      "Epoch 34/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 2.0759 - sparse_categorical_accuracy: 0.423\n",
      "Epoch 00034: val_sparse_categorical_accuracy did not improve from 0.32612\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  5.11725784e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 2.0759 - sparse_categorical_accuracy: 0.4233 - val_loss: 2.7449 - val_sparse_categorical_accuracy: 0.3237\n",
      "Epoch 35/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 2.0341 - sparse_categorical_accuracy: 0.431\n",
      "Epoch 00035: val_sparse_categorical_accuracy improved from 0.32612 to 0.32913, saving model to ./ckpt/pred_ret\\val_acc=0.3291_acc=0.4316_ckpt=35\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.98092959e-06\n",
      "468/468 [==============================] - 12s 27ms/step - loss: 2.0341 - sparse_categorical_accuracy: 0.4316 - val_loss: 2.7371 - val_sparse_categorical_accuracy: 0.3291\n",
      "Epoch 36/100\n",
      "466/468 [============================>.] - ETA: 0s - loss: 1.9893 - sparse_categorical_accuracy: 0.4410 - ETA: 6s - loss: 1.8930 - sparse_categorical_accuracy: 0.466\n",
      "Epoch 00036: val_sparse_categorical_accuracy improved from 0.32913 to 0.33634, saving model to ./ckpt/pred_ret\\val_acc=0.3363_acc=0.4411_ckpt=36\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.85167675e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 1.9900 - sparse_categorical_accuracy: 0.4411 - val_loss: 2.7554 - val_sparse_categorical_accuracy: 0.3363\n",
      "Epoch 37/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.9532 - sparse_categorical_accuracy: 0.45\n",
      "Epoch 00037: val_sparse_categorical_accuracy did not improve from 0.33634\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.72896272e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 1.9532 - sparse_categorical_accuracy: 0.4511 - val_loss: 2.8160 - val_sparse_categorical_accuracy: 0.3183\n",
      "Epoch 38/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.9272 - sparse_categorical_accuracy: 0.45\n",
      "Epoch 00038: val_sparse_categorical_accuracy did not improve from 0.33634\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.61230275e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 1.9272 - sparse_categorical_accuracy: 0.4567 - val_loss: 2.7620 - val_sparse_categorical_accuracy: 0.3337\n",
      "Epoch 39/100\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.8971 - sparse_categorical_accuracy: 0.464\n",
      "Epoch 00039: val_sparse_categorical_accuracy improved from 0.33634 to 0.34335, saving model to ./ckpt/pred_ret\\val_acc=0.3433_acc=0.4643_ckpt=39\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.50126e-06\n",
      "468/468 [==============================] - 12s 25ms/step - loss: 1.8974 - sparse_categorical_accuracy: 0.4643 - val_loss: 2.7736 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 40/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.8823 - sparse_categorical_accuracy: 0.46\n",
      "Epoch 00040: val_sparse_categorical_accuracy improved from 0.34335 to 0.34355, saving model to ./ckpt/pred_ret\\val_acc=0.3435_acc=0.4691_ckpt=40\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.39543828e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 1.8823 - sparse_categorical_accuracy: 0.4691 - val_loss: 2.7606 - val_sparse_categorical_accuracy: 0.3435\n",
      "Epoch 41/100\n",
      "468/468 [==============================] - ETA: 0s - loss: 1.8306 - sparse_categorical_accuracy: 0.47\n",
      "Epoch 00041: val_sparse_categorical_accuracy did not improve from 0.34355\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.294478e-06\n",
      "468/468 [==============================] - 11s 24ms/step - loss: 1.8306 - sparse_categorical_accuracy: 0.4789 - val_loss: 2.8491 - val_sparse_categorical_accuracy: 0.3337\n",
      "Epoch 42/100\n",
      "465/468 [============================>.] - ETA: 0s - loss: 1.8159 - sparse_categorical_accuracy: 0.483 - ETA: 7s - loss: 1.7341 - sparse_categorical_accuracy: 0.513\n",
      "Epoch 00042: val_sparse_categorical_accuracy improved from 0.34355 to 0.35256, saving model to ./ckpt/pred_ret\\val_acc=0.3526_acc=0.4836_ckpt=42\n",
      "Starting Learning Rate =  0.00035\n",
      "Actual Learning Rate =  4.19805201e-06\n",
      "468/468 [==============================] - 11s 23ms/step - loss: 1.8170 - sparse_categorical_accuracy: 0.4836 - val_loss: 2.7516 - val_sparse_categorical_accuracy: 0.3526\n",
      "Epoch 43/100\n",
      " 47/468 [==>...........................] - ETA: 7s - loss: 1.7174 - sparse_categorical_accuracy: 0.51"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_36272/114578582.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalidation_generator_batch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mepochs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m100\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m     \u001B[0mcallbacks\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcallbacks_clean\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m )\n\u001B[0;32m     13\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDataFrame\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhistory\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfigsize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m8\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    106\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    110\u001B[0m     \u001B[1;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1096\u001B[0m                 batch_size=batch_size):\n\u001B[0;32m   1097\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1098\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1099\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1100\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    778\u001B[0m       \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 780\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    781\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    805\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    806\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 807\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    808\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    809\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2828\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2829\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2830\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2831\u001B[0m   \u001B[1;33m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[1;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1846\u001B[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001B[0;32m   1847\u001B[0m         \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1848\u001B[1;33m         cancellation_manager=cancellation_manager)\n\u001B[0m\u001B[0;32m   1849\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1850\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1922\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1923\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[1;32m-> 1924\u001B[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[0;32m   1925\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[0;32m   1926\u001B[0m         \u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    548\u001B[0m               \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m               ctx=ctx)\n\u001B[0m\u001B[0;32m    551\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[1;32mC:\\ProgramData\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[1;32m---> 60\u001B[1;33m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "params = {'batch_size': 16,\n",
    "          'dim': (32, 32, 3),\n",
    "          'shuffle': True,\n",
    "          'transforms': preprocess_img}\n",
    "training_generator_batch = DataGenerator(X_dataset=X_train_full2, labels=y_train_full2, **params)\n",
    "validation_generator_batch = DataGenerator(X_dataset=X_test_full2, labels=y_test_full2, **params)\n",
    "history = model_predict_retrained.fit(\n",
    "    x=training_generator_batch,\n",
    "    validation_data=validation_generator_batch,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks_clean\n",
    ")\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step - loss: 3.7918 - sparse_categorical_accuracy: 0.15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.791811943054199, 0.15399999916553497]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predict_retrained.evaluate(X_test,y_test)\n",
    "\n",
    "\n",
    "##% md\n",
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from ./ckpt/pred_ret\\val_acc=0.3273_acc=0.3436_ckpt=32\n"
     ]
    }
   ],
   "source": [
    "model_predict_retrained = make_or_restore_model_weights_only_uncompiled(get_uncompiled_model_predict, ckpt_dir_predict_retrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicts = model_predict_retrained.predict(X_test_full)\n",
    "predictions = [np.argmax(pred) for pred in predicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for i in range(len(predictions)):\n",
    "    indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.DataFrame(data={\"index\": indexes, \"label\": predictions})\n",
    "df.to_csv(\"./sub-2-class.csv\", sep=',', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}