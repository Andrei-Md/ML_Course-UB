{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'2.8.0'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some imports\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rc('font', size=12)\n",
    "plt.rc('figure', figsize=(12, 5))\n",
    "\n",
    "# Settings for the visualizations\n",
    "#import seaborn as sns\n",
    "#sns.set_style(\"whitegrid\")\n",
    "#sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 25)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# data\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Classifier\n",
    "import tqdm\n",
    "from sklearn import base\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chh\\AppData\\Local\\Temp/ipykernel_33788/1368645473.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test = y_test.to_numpy().astype(np.float)\n",
      "C:\\Users\\Chh\\AppData\\Local\\Temp/ipykernel_33788/1368645473.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train = y_train.to_numpy().astype(np.float)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:10000], y[:10000], test_size=0.2)  # 80% training and 20% test\n",
    "y_test = y_test.to_numpy().astype(np.float)\n",
    "y_train = y_train.to_numpy().astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Classifier\n",
    "based on SAMME algorithm - Stagewise Additive Modeling <br />\n",
    "Zhu, Ji & Rosset, Saharon & Zou, Hui & Hastie, Trevor. (2006). Multi-class AdaBoost. Statistics and its interface. 2. 10.4310/SII.2009.v2.n3.a8.\n",
    "\n",
    "<img src=\"./SAMME-alg.png\"\n",
    "     alt=\"SAMME algorithm\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Boosting:\n",
    "    '''\n",
    "    based on SAMME algorithm - Stagewise Additive Modeling\n",
    "    Zhu, Ji & Rosset, Saharon & Zou, Hui & Hastie, Trevor. (2006). Multi-class AdaBoost. Statistics and its interface. 2. 10.4310/SII.2009.v2.n3.a8.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.estimator_errors = None\n",
    "        self.models = None\n",
    "        self.alpha_m = None\n",
    "        self.labels = {}\n",
    "        self.labels2 = {}\n",
    "        self.classes_no = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        self.estimator_errors = []\n",
    "        self.alpha_m = []\n",
    "        n_samples = len(X)  # No of samples\n",
    "        weight = np.ones(n_samples, dtype=np.float64)  # sample weights - init to 1\n",
    "        weight /= weight.sum()\n",
    "\n",
    "        k = len(np.unique(y))  # no of classes\n",
    "\n",
    "        # create labels from classes type\n",
    "        self.classes_no = len(np.unique(y))\n",
    "        for i, cls in enumerate(np.unique(y)):\n",
    "            self.labels[cls] = i\n",
    "            self.labels2[i] = cls\n",
    "\n",
    "        for estimator_no in tqdm.tqdm(range(self.n_estimators)):\n",
    "            model = base.clone(self.base_estimator).fit(X, y, sample_weight=weight).predict\n",
    "            predictions = model(X)\n",
    "            predictions_truth = predictions != y\n",
    "\n",
    "            # calculate the weak estimator error\n",
    "            err_m = np.average(predictions_truth, weights=weight, axis=0)\n",
    "\n",
    "            #compute the wright of the currect classifier\n",
    "            alpha_m = (np.log((1 - err_m) / err_m) + np.log(k - 1))\n",
    "            # update the weights, if the prediction was wrong then increase it\n",
    "            weight = weight * np.exp(alpha_m * predictions_truth)\n",
    "            # normalize\n",
    "            # weight /= np.sum(weight)\n",
    "\n",
    "            # save data\n",
    "            self.estimator_errors.append(err_m)\n",
    "            self.models.append(model)\n",
    "            self.alpha_m.append(alpha_m)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        output is calculated by weighted vote\n",
    "        :param X: predict set\n",
    "        :return: predicted labels for each entry in the predict set\n",
    "        '''\n",
    "        y_pred = []\n",
    "        for alpha_m, model in tqdm.tqdm(zip(self.alpha_m, self.models)):\n",
    "            y_new = []\n",
    "            for predict in model(X):\n",
    "                pred = np.full(self.classes_no, fill_value=-1 / (self.classes_no - 1), dtype=np.float64)\n",
    "                pred[self.labels[predict]] = 1\n",
    "                y_new.append(pred)\n",
    "            y_pred.append([y * alpha_m for y in y_new])\n",
    "        y_pred = np.sum(y_pred, axis=0)\n",
    "        labels_no = [np.argmax(y) for y in y_pred]\n",
    "        return [self.labels2[i] for i in labels_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:47<00:00, 10.55it/s]\n",
      "500it [00:06, 78.23it/s]\n"
     ]
    }
   ],
   "source": [
    "clf = Boosting(n_estimators=500)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "F1 score: 0.79\n",
      "\n",
      "Confusion Matrix:\n",
      " [[146   0   2   1   0  54   1   0   3   0]\n",
      " [  0 198  12   0   1   1   0   0   4   0]\n",
      " [  2   3 142   6   4   6  22   1  18   0]\n",
      " [  2   3   4 146   0  17   2   4  11   3]\n",
      " [  2   0   2   1 183   3   1   2   3  14]\n",
      " [  3   5   1  15   3 132   3   1   9   4]\n",
      " [  4   0   6   0  16   8 182   0   4   0]\n",
      " [  0   3   6   0   3   2   0 163   2  37]\n",
      " [  0   4   3   5   0   7   0   1 139   7]\n",
      " [  1   3   1   7  18   0   0  10   3 149]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_pred, average='micro'))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", CM(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adaboost classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ab_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "                            algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "# Train the AdaBoost model\n",
    "ab_model = ab_clf.fit(X_train, y_train)\n",
    "#Predict the response for test dataset\n",
    "y_pred_ab = ab_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6455\n",
      "F1 score: 0.6455\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_ab))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_pred_ab, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9385\n",
      "F1 score: 0.9385\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_bag))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_pred_bag, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, min_samples_leaf=10, random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9375\n",
      "F1 score: 0.9375\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred_rf))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test, y_pred_rf, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MNIST Fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.8.0'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full_fashion, y_train_full_fashion), (X_test_fashion, y_test_fashion) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def flatten_vec(vectors):\n",
    "    vecs = []\n",
    "    for vec in vectors:\n",
    "        vecs.append(np.ndarray.flatten(vec))\n",
    "    return np.asarray(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train_full_fashion = flatten_vec(X_train_full_fashion)\n",
    "X_test_fashion = flatten_vec(X_test_fashion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [14:51<00:00,  1.78s/it]\n",
      "500it [00:22, 22.25it/s]\n"
     ]
    }
   ],
   "source": [
    "clf = Boosting(n_estimators=500)\n",
    "clf.fit(X_train_full_fashion, y_train_full_fashion)\n",
    "y_pred_boosting = clf.predict(X_test_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6055\n",
      "F1 score: 0.6055\n",
      "\n",
      "Confusion Matrix:\n",
      " [[340   9  64 329   4   1 236   0  17   0]\n",
      " [  3 703   7 267   7   0  12   0   1   0]\n",
      " [  4   0 738  25 146   0  79   0   8   0]\n",
      " [ 10  12  41 894  11   0  32   0   0   0]\n",
      " [  0   1 349 130 460   0  56   0   4   0]\n",
      " [  0   0   0   0   0 938   1  17  26  18]\n",
      " [ 56   6 277 177 197   0 252   0  35   0]\n",
      " [  0   0   0   0   0 734   0 237   0  29]\n",
      " [  0   1  22   6   1   8  25   4 933   0]\n",
      " [  0   0   0   0   0 406   0  26   8 560]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test_fashion, y_pred_boosting))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test_fashion, y_pred_boosting, average='micro'))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\\n\", CM(y_test_fashion, y_pred_boosting))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adaboost classifier\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "ab_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "                            algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "# Train the AdaBoost model\n",
    "ab_model = ab_clf.fit(X_train_full_fashion, y_train_full_fashion)\n",
    "#Predict the response for test dataset\n",
    "y_pred_ab_fashion = ab_model.predict(X_test_fashion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5814\n",
      "F1 score: 0.5814\n"
     ]
    }
   ],
   "source": [
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test_fashion, y_pred_ab_fashion))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test_fashion, y_pred_ab_fashion, average='micro'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bagging Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500,\n",
    "                            bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train_full_fashion, y_train_full_fashion)\n",
    "y_pred_bag_fashion = bag_clf.predict(X_test_fashion)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.872\n",
      "F1 score: 0.872\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test_fashion, y_pred_bag_fashion))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test_fashion, y_pred_bag_fashion, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=500, min_samples_leaf=10, random_state=42)\n",
    "rnd_clf.fit(X_train_full_fashion, y_train_full_fashion)\n",
    "y_pred_rf_fashion = rnd_clf.predict(X_test_fashion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n",
      "F1 score: 0.868\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test_fashion, y_pred_rf_fashion))\n",
    "print(\"F1 score:\", metrics.f1_score(y_test_fashion, y_pred_rf_fashion, average='micro'))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (ml_project)",
   "language": "python",
   "name": "pycharm-90f4dfab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}